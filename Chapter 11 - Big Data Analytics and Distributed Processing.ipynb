{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23673816",
   "metadata": {},
   "source": [
    "# Chapter 11: Big Data Analytics and Distributed Processing\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what \"big data\" means and when traditional tools are insufficient\n",
    "- Learn core distributed computing concepts (clusters, partitions, fault tolerance)\n",
    "- Get hands-on experience with Apache Spark and PySpark\n",
    "- Work with Spark DataFrames and understand lazy evaluation\n",
    "- Apply best practices for handling large datasets\n",
    "- Gain awareness of cloud-based analytics platforms\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python programming (Chapter 2)\n",
    "- Familiarity with Pandas DataFrames (Chapter 4)\n",
    "- Understanding of data manipulation concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [What is Big Data?](#11.1-What-is-Big-Data?)\n",
    "3. [Limitations of Traditional Tools](#11.2-Limitations-of-Traditional-Tools)\n",
    "4. [Distributed Computing Concepts](#11.3-Distributed-Computing-Concepts)\n",
    "5. [Introduction to Spark Architecture](#11.4-Introduction-to-Spark-Architecture)\n",
    "6. [Setup: Using PySpark](#11.5-Setup:-Using-PySpark-in-a-Notebook)\n",
    "7. [DataFrames and RDDs](#11.6-DataFrames-and-RDDs)\n",
    "8. [Parallel Computation in Spark](#11.7-Parallel-Computation-in-Spark)\n",
    "9. [Handling Large Datasets](#11.8-Handling-Large-Datasets-(Practical-Guidelines))\n",
    "10. [Cloud-Based Analytics Overview](#11.9-Cloud-Based-Analytics-Overview)\n",
    "11. [Exercises](#Exercises-(Practice))\n",
    "12. [Mini-Project](#Mini‚ÄëProject:-Clickstream-Summary-(Spark-DataFrames))\n",
    "13. [Summary](#Summary-/-Key-Takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e2a1a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Big data analytics is not about using \"fancy\" tools‚Äîit‚Äôs about **handling data that is too large, too fast, or too complex** for a single computer to process reliably and quickly.\n",
    "\n",
    "A common beginner misconception is: \"If I just buy a bigger laptop, I can handle big data.\" Sometimes that works for a while, but eventually you hit limits (memory, CPU, disk, time). Distributed systems solve this by using **multiple machines working together**.\n",
    "\n",
    "In this chapter we focus on Spark because it is widely used and approachable for Python users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc5820",
   "metadata": {},
   "source": [
    "## 11.1 What is Big Data?\n",
    "There isn‚Äôt a single cutoff like \"over 1GB is big data\". Big data is usually described using the **3 Vs** (sometimes 5 Vs):\n",
    "\n",
    "- **Volume**: too much data to store/process comfortably on one machine\n",
    "- **Velocity**: data arrives quickly (streams, logs, sensors, click events)\n",
    "- **Variety**: many formats (tables, JSON, images, text, audio)\n",
    "\n",
    "Additional Vs you may hear:\n",
    "- **Veracity**: data quality / uncertainty\n",
    "- **Value**: usefulness of the data\n",
    "\n",
    "### Examples\n",
    "- Web/app logs from millions of users\n",
    "- Transaction records from large e-commerce sites\n",
    "- IoT sensor readings every second\n",
    "- Social media text + images\n",
    "\n",
    "**Key idea:** Big data is often a *systems* problem (storage + compute + reliability), not just a programming problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f0532",
   "metadata": {},
   "source": [
    "## 11.2 Limitations of Traditional Tools\n",
    "Tools like Pandas are fantastic‚Äîbut they are mostly designed for **single-machine** workflows.\n",
    "\n",
    "### Where single-machine tools struggle\n",
    "| Limitation | Description | Example |\n",
    "|------------|-------------|---------|\n",
    "| **Memory limits** | DataFrame must largely fit into RAM | 50GB CSV on 16GB laptop |\n",
    "| **CPU limits** | One machine has limited cores | Complex transformations on millions of rows |\n",
    "| **I/O limits** | Reading huge files from disk is slow | Loading terabytes of log files |\n",
    "| **Long runtimes** | Jobs take hours/days; crashes lose progress | Overnight analytics job fails at 90% |\n",
    "\n",
    "### Visual comparison: Single vs Distributed Processing\n",
    "\n",
    "```\n",
    "Single Machine (Pandas):          Distributed System (Spark):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    Your Laptop      ‚îÇ           ‚îÇ Node 1  ‚îÇ ‚îÇ Node 2  ‚îÇ ‚îÇ Node 3  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ           ‚îÇ Part A  ‚îÇ ‚îÇ Part B  ‚îÇ ‚îÇ Part C  ‚îÇ\n",
    "‚îÇ  ‚îÇ  ALL DATA     ‚îÇ  ‚îÇ    vs     ‚îÇ of data ‚îÇ ‚îÇ of data ‚îÇ ‚îÇ of data ‚îÇ\n",
    "‚îÇ  ‚îÇ  (must fit!)  ‚îÇ  ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                ‚îÇ          ‚îÇ          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                            Combined Result\n",
    "```\n",
    "\n",
    "### Important nuance\n",
    "Not every dataset needs Spark. If your data fits in memory and runs fast enough, Pandas can be simpler and more productive.\n",
    "\n",
    "**Rule of thumb:** if your data is too big for memory or your processing takes too long, consider distributed tools.\n",
    "\n",
    "> üí° **Tip:** Start with Pandas. Only move to Spark when you hit real limitations. Premature optimization wastes time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f0259",
   "metadata": {},
   "source": [
    "## 11.3 Distributed Computing Concepts\n",
    "Distributed computing means splitting work across multiple computers (or processes).\n",
    "\n",
    "### Core terms explained\n",
    "\n",
    "| Term | What it means | Analogy |\n",
    "|------|--------------|---------|\n",
    "| **Cluster** | A group of machines working together | A team of workers |\n",
    "| **Node** | One machine in the cluster | One worker in the team |\n",
    "| **Driver** | The program that coordinates the job | The team leader |\n",
    "| **Worker/Executor** | Processes that do the actual computation | Team members doing tasks |\n",
    "| **Partition** | A chunk of data processed in parallel | Dividing work into portions |\n",
    "\n",
    "### Visual: How a Spark cluster works\n",
    "\n",
    "```\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Driver Program ‚îÇ  ‚Üê Your Python code runs here\n",
    "                    ‚îÇ  (Coordinator)  ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ              ‚îÇ              ‚îÇ\n",
    "              ‚ñº              ‚ñº              ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Executor ‚îÇ   ‚îÇ Executor ‚îÇ   ‚îÇ Executor ‚îÇ\n",
    "        ‚îÇ  Node 1  ‚îÇ   ‚îÇ  Node 2  ‚îÇ   ‚îÇ  Node 3  ‚îÇ\n",
    "        ‚îÇ          ‚îÇ   ‚îÇ          ‚îÇ   ‚îÇ          ‚îÇ\n",
    "        ‚îÇ [Part 1] ‚îÇ   ‚îÇ [Part 2] ‚îÇ   ‚îÇ [Part 3] ‚îÇ  ‚Üê Data partitions\n",
    "        ‚îÇ [Part 4] ‚îÇ   ‚îÇ [Part 5] ‚îÇ   ‚îÇ [Part 6] ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Why partitioning matters\n",
    "If you split your data into, say, 8 partitions, Spark can work on multiple partitions at the same time (depending on available cores).\n",
    "\n",
    "**Example:** 1 billion rows split into 100 partitions = each partition has ~10 million rows, processed in parallel.\n",
    "\n",
    "### Fault tolerance (beginner-friendly view)\n",
    "Distributed systems expect failures. What happens if a node crashes?\n",
    "\n",
    "**Spark's solution: Lineage**\n",
    "- Spark tracks how each partition was created (the \"recipe\")\n",
    "- If a partition is lost, Spark can recompute it from the original data\n",
    "- No need to restart the entire job!\n",
    "\n",
    "```\n",
    "Original Data ‚Üí filter() ‚Üí groupBy() ‚Üí Result\n",
    "                   ‚Üë\n",
    "            If Node 2 fails here, Spark can\n",
    "            recompute just the lost partitions\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Warning:** Fault tolerance adds overhead. For small, quick jobs on one machine, Pandas is often faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b7d44",
   "metadata": {},
   "source": [
    "## 11.4 Introduction to Spark Architecture\n",
    "Spark is a distributed compute engine designed for large-scale data processing.\n",
    "\n",
    "### Spark components (high level)\n",
    "\n",
    "| Component | Role | Description |\n",
    "|-----------|------|-------------|\n",
    "| **SparkSession** | Entry point | The main interface to Spark functionality |\n",
    "| **Driver program** | Coordinator | Runs your Python code, creates execution plan |\n",
    "| **Executors** | Workers | Run tasks on data partitions |\n",
    "| **Cluster manager** | Resource allocator | Manages cluster resources (local, YARN, K8s) |\n",
    "\n",
    "### Lazy evaluation (very important!)\n",
    "In Spark, many operations are **lazy**: Spark waits to execute until it must produce a result.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    TRANSFORMATIONS (Lazy)                    ‚îÇ\n",
    "‚îÇ  select() ‚Üí filter() ‚Üí withColumn() ‚Üí groupBy() ‚Üí ...      ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  ‚ö° Nothing actually happens yet! Spark builds a plan.       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      ACTIONS (Trigger!)                      ‚îÇ\n",
    "‚îÇ  count(), show(), collect(), write(), take()                ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  üöÄ NOW Spark executes the optimized plan!                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Why lazy evaluation?\n",
    "1. **Optimization**: Spark can reorder/merge operations for efficiency\n",
    "2. **Efficiency**: Skip unnecessary work (e.g., if you only need 10 rows)\n",
    "3. **Planning**: Spark sees the whole pipeline before executing\n",
    "\n",
    "### Common transformations vs actions\n",
    "\n",
    "| Transformations (lazy) | Actions (trigger execution) |\n",
    "|----------------------|---------------------------|\n",
    "| `select()` | `count()` |\n",
    "| `filter()` / `where()` | `show()` |\n",
    "| `withColumn()` | `collect()` |\n",
    "| `groupBy()` | `take(n)` |\n",
    "| `join()` | `write()` |\n",
    "| `orderBy()` | `first()` |\n",
    "\n",
    "> üí° **Tip:** If your Spark code seems to \"do nothing,\" you probably haven't called an action yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d58cc",
   "metadata": {},
   "source": [
    "## 11.5 Setup: Using PySpark in a Notebook\n",
    "You can run Spark in two main ways:\n",
    "- **Local mode** (recommended for learning): Spark runs on your machine using multiple threads\n",
    "- **Cluster mode**: Spark runs on many machines in a cluster\n",
    "\n",
    "### Installation options\n",
    "\n",
    "| Method | Command | Best for |\n",
    "|--------|---------|----------|\n",
    "| pip | `pip install pyspark` | Most users, learning |\n",
    "| conda | `conda install pyspark` | Anaconda users |\n",
    "| Databricks | Pre-installed | Production, collaboration |\n",
    "\n",
    "### Installing PySpark\n",
    "In a terminal (not in a notebook cell), run:\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "If `pyspark` is not installed, the code cells below will show a helpful message instead of crashing.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** PySpark requires Java (JDK 8 or 11). If you get Java errors, install Java first from https://adoptium.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b46b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark setup (works in local mode).\n",
    "# If pyspark isn't installed, this cell will explain what to do.\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql import types as T\n",
    "    PysparkAvailable = True\n",
    "except ImportError as e:\n",
    "    PysparkAvailable = False\n",
    "    print(\"PySpark is not installed in this environment.\")\n",
    "    print(\"Install it with: pip install pyspark\")\n",
    "    print(\"Then restart the notebook kernel and run this cell again.\")\n",
    "\n",
    "PysparkAvailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session (the entry point for Spark).\n",
    "# local[*] means: use all available CPU cores on your machine.\n",
    "\n",
    "if PysparkAvailable:\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName('Chapter11-BigData-Intro')\n",
    "        .master('local[*]')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark\n",
    "else:\n",
    "    spark = None\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652eee01",
   "metadata": {},
   "source": [
    "### Quick check: Spark version\n",
    "When learning, it helps to confirm Spark is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59738644",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark is not None:\n",
    "    print('Spark version:', spark.version)\n",
    "else:\n",
    "    print('Spark not available (install pyspark).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94cbde",
   "metadata": {},
   "source": [
    "## 11.6 DataFrames and RDDs\n",
    "Spark supports multiple APIs. The two most important are:\n",
    "\n",
    "### Comparison: DataFrames vs RDDs\n",
    "\n",
    "| Feature | DataFrames | RDDs |\n",
    "|---------|-----------|------|\n",
    "| **Abstraction** | Tabular (like Pandas) | Distributed collection |\n",
    "| **Schema** | Has schema (column names, types) | No schema |\n",
    "| **Optimization** | Catalyst optimizer (fast!) | No automatic optimization |\n",
    "| **Ease of use** | SQL-like, beginner-friendly | More verbose, lower-level |\n",
    "| **Best for** | Analytics, SQL queries | Custom transformations |\n",
    "\n",
    "### Which should you use?\n",
    "For most analytics tasks, start with **DataFrames**. They are easier and usually faster because Spark can optimize queries.\n",
    "\n",
    "```\n",
    "Your Analytics Code\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Spark DataFrame  ‚îÇ  ‚Üê Recommended for beginners!\n",
    "‚îÇ  (High-level API) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ (Spark optimizes automatically)\n",
    "          ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       RDD         ‚îÇ  ‚Üê Lower level, more control\n",
    "‚îÇ  (Low-level API)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "> üí° **Tip:** Think of DataFrames as \"Pandas for big data\" ‚Äî similar feel, but distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build a small example dataset.\n",
    "# Even though it's small, the same code patterns scale to bigger data.\n",
    "\n",
    "if spark is not None:\n",
    "    data = [\n",
    "        (1, 'A', 10.5),\n",
    "        (2, 'A', 20.0),\n",
    "        (3, 'B', 7.25),\n",
    "        (4, 'B', 12.0),\n",
    "        (5, 'C', 5.0),\n",
    "        (6, 'C', None),\n",
    "    ]\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField('id', T.IntegerType(), nullable=False),\n",
    "        T.StructField('group', T.StringType(), nullable=False),\n",
    "        T.StructField('value', T.DoubleType(), nullable=True),\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    df\n",
    "else:\n",
    "    df = None\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8741bd",
   "metadata": {},
   "source": [
    "### Inspecting data (schema + rows)\n",
    "In Spark, you often start by checking the schema and sampling rows.\n",
    "\n",
    "**Tip:** avoid `collect()` on big datasets; use `show()` or `limit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3561e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    df.printSchema()\n",
    "    df.show(truncate=False)\n",
    "else:\n",
    "    print('DataFrame not available (Spark not running).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d451f",
   "metadata": {},
   "source": [
    "### Transformations vs actions (hands-on)\n",
    "Below, we create a filtered DataFrame. This does **not** immediately compute results (lazy evaluation).\n",
    "\n",
    "Then we call an action (`count`) to actually run the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    filtered = df.filter(F.col('value').isNotNull())\n",
    "    # At this point Spark has a plan, but hasn't executed yet.\n",
    "    print('Rows with non-null value:', filtered.count())\n",
    "    filtered.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc389e",
   "metadata": {},
   "source": [
    "### Aggregation with groupBy\n",
    "A common analytics task is grouping and computing summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    summary = (\n",
    "        df.groupBy('group')\n",
    "          .agg(\n",
    "              F.count('*').alias('rows'),\n",
    "              F.count('value').alias('non_null_values'),\n",
    "              F.avg('value').alias('avg_value'),\n",
    "              F.min('value').alias('min_value'),\n",
    "              F.max('value').alias('max_value'),\n",
    "          )\n",
    "          .orderBy('group')\n",
    "    )\n",
    "    summary.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf027738",
   "metadata": {},
   "source": [
    "### Visual example: plot aggregated results\n",
    "Spark is not a plotting library. A typical workflow is:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Large Data    ‚îÇ    ‚îÇ  Small Summary  ‚îÇ    ‚îÇ   Visualization ‚îÇ\n",
    "‚îÇ   in Spark      ‚îÇ ‚Üí  ‚îÇ  (aggregated)   ‚îÇ ‚Üí  ‚îÇ   (Matplotlib)  ‚îÇ\n",
    "‚îÇ   (millions)    ‚îÇ    ‚îÇ  (few rows)     ‚îÇ    ‚îÇ                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                         .toPandas()\n",
    "                       (safe for small data)\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "1. Use Spark to compute a *small* aggregated result\n",
    "2. Convert that small result to Pandas with `.toPandas()`\n",
    "3. Plot with Matplotlib/Seaborn\n",
    "\n",
    "> ‚ö†Ô∏è **Warning:** Converting a large Spark DataFrame to Pandas can crash your machine. Only convert small, aggregated results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Collect only the small summary result\n",
    "    pdf = summary.toPandas()\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(pdf['group'], pdf['avg_value'])\n",
    "    plt.title('Average value per group')\n",
    "    plt.xlabel('group')\n",
    "    plt.ylabel('avg_value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b019e2c",
   "metadata": {},
   "source": [
    "### RDDs (brief introduction)\n",
    "RDDs are the older core abstraction in Spark. You may still see them in older tutorials or specialized tasks.\n",
    "\n",
    "RDDs feel more like working with distributed lists: `map`, `filter`, `reduce`.\n",
    "\n",
    "For analytics and SQL-like operations, prefer **DataFrames** unless you have a specific reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1311db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: a tiny RDD and a classic word count pattern.\n",
    "# This is for learning; DataFrames are usually preferred in real analytics work.\n",
    "\n",
    "if spark is not None:\n",
    "    sc = spark.sparkContext\n",
    "    lines = sc.parallelize([\n",
    "        'spark makes big data manageable',\n",
    "        'big data needs distributed computing',\n",
    "        'spark spark spark'\n",
    "    ])\n",
    "\n",
    "    words = lines.flatMap(lambda s: s.split())\n",
    "    counts = words.map(lambda w: (w.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "    print(counts.collect())\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8bf81",
   "metadata": {},
   "source": [
    "## 11.7 Parallel Computation in Spark\n",
    "Spark achieves parallelism mainly through **partitions**. Each partition is processed by a task, and tasks can run at the same time.\n",
    "\n",
    "### Why should beginners care?\n",
    "Even if your code is correct, a poor partition strategy can make jobs slow.\n",
    "\n",
    "### Useful tools\n",
    "- `df.rdd.getNumPartitions()` to check partitions\n",
    "- `repartition(n)` to increase (or change) partitioning (shuffle)\n",
    "- `coalesce(n)` to reduce partitions (often cheaper than repartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfa9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print('Default partitions:', df.rdd.getNumPartitions())\n",
    "    df2 = df.repartition(4)\n",
    "    print('After repartition(4):', df2.rdd.getNumPartitions())\n",
    "\n",
    "    df3 = df2.coalesce(2)\n",
    "    print('After coalesce(2):', df3.rdd.getNumPartitions())\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6084b71",
   "metadata": {},
   "source": [
    "### Common mistake: too many or too few partitions\n",
    "- **Too few** partitions ‚Üí not enough parallelism, slow jobs\n",
    "- **Too many** partitions ‚Üí overhead (scheduling, small tasks), also slow\n",
    "\n",
    "A practical approach is to start with defaults, measure, then tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98c938",
   "metadata": {},
   "source": [
    "## 11.8 Handling Large Datasets (Practical Guidelines)\n",
    "When datasets get large, small habits matter a lot.\n",
    "\n",
    "### ‚úÖ Do this (Best Practices)\n",
    "\n",
    "| Practice | Why it helps | Example |\n",
    "|----------|-------------|---------|\n",
    "| Use **Parquet** format | Columnar, compressed, fast scans | `df.write.parquet('data.parquet')` |\n",
    "| **Select** only needed columns | Less data to move | `df.select('col1', 'col2')` |\n",
    "| **Filter early** | Reduce data before processing | `df.filter(F.col('year') > 2020)` |\n",
    "| Use **groupBy + agg** | Summarize instead of collecting raw | `df.groupBy('cat').agg(F.count('*'))` |\n",
    "| **Cache** reused DataFrames | Avoid recomputation | `df.cache()` |\n",
    "| Use **built-in functions** | Optimized by Spark | `F.lower()`, `F.when()` |\n",
    "\n",
    "### ‚ùå Avoid this (Common Mistakes)\n",
    "\n",
    "| Mistake | Why it's bad | Alternative |\n",
    "|---------|-------------|-------------|\n",
    "| `collect()` on big data | Crashes driver (your notebook) | Use `show()` or `limit()` |\n",
    "| `toPandas()` on huge DataFrames | Memory errors | Aggregate first |\n",
    "| Python UDFs too early | Slower than built-ins | Use `F.` functions first |\n",
    "| Too few partitions | Poor parallelism | `repartition()` |\n",
    "| Too many partitions | Scheduling overhead | `coalesce()` |\n",
    "\n",
    "### Why built-in functions matter\n",
    "Spark can optimize built-in SQL functions (like `F.lower`, `F.when`, `F.regexp_extract`) better than custom Python functions.\n",
    "\n",
    "```python\n",
    "# ‚ùå Slow: Python UDF\n",
    "from pyspark.sql.functions import udf\n",
    "@udf\n",
    "def my_lower(s):\n",
    "    return s.lower() if s else None\n",
    "\n",
    "# ‚úÖ Fast: Built-in function\n",
    "from pyspark.sql import functions as F\n",
    "df.withColumn('name_lower', F.lower(F.col('name')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: cleaning + feature creation using built-in functions\n",
    "# We'll create a new column and handle missing values.\n",
    "\n",
    "if df is not None:\n",
    "    cleaned = (\n",
    "        df\n",
    "        .withColumn('value_filled', F.coalesce(F.col('value'), F.lit(0.0)))\n",
    "        .withColumn('is_missing', F.col('value').isNull().cast('int'))\n",
    "    )\n",
    "\n",
    "    cleaned.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f77922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: caching (only useful when reusing the same DataFrame multiple times)\n",
    "\n",
    "if df is not None:\n",
    "    cleaned_cached = cleaned.cache()\n",
    "    # First action materializes cache\n",
    "    print('Count:', cleaned_cached.count())\n",
    "    # Second action can reuse cached data\n",
    "    print('Distinct groups:', cleaned_cached.select('group').distinct().count())\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd39b4a",
   "metadata": {},
   "source": [
    "### Using SQL with Spark DataFrames\n",
    "One powerful feature of Spark is that you can use SQL queries directly on DataFrames. This is great if you already know SQL!\n",
    "\n",
    "**Workflow:**\n",
    "1. Register your DataFrame as a temporary view\n",
    "2. Write SQL queries against it\n",
    "3. Results are returned as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda65892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using SQL with Spark DataFrames\n",
    "# This is useful if you already know SQL!\n",
    "\n",
    "if df is not None:\n",
    "    # Step 1: Register DataFrame as a temporary SQL view\n",
    "    cleaned.createOrReplaceTempView('my_data')\n",
    "    \n",
    "    # Step 2: Write SQL query\n",
    "    sql_result = spark.sql('''\n",
    "        SELECT \n",
    "            group,\n",
    "            COUNT(*) as total_rows,\n",
    "            AVG(value_filled) as avg_value,\n",
    "            SUM(is_missing) as missing_count\n",
    "        FROM my_data\n",
    "        GROUP BY group\n",
    "        ORDER BY group\n",
    "    ''')\n",
    "    \n",
    "    # Step 3: Result is a DataFrame - show it\n",
    "    print(\"SQL Query Result:\")\n",
    "    sql_result.show()\n",
    "else:\n",
    "    print('Spark not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cb942",
   "metadata": {},
   "source": [
    "## 11.9 Cloud-Based Analytics Overview\n",
    "Big data systems are commonly run in the cloud because it's easier to scale up/down and integrate storage + compute.\n",
    "\n",
    "### Why cloud for big data?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Scalability** | Add/remove nodes as needed |\n",
    "| **Cost efficiency** | Pay only for what you use |\n",
    "| **Managed services** | Less infrastructure to maintain |\n",
    "| **Integration** | Easy connection to other cloud services |\n",
    "\n",
    "### Common cloud architecture pattern\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        CLOUD PLATFORM                        ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
    "‚îÇ   ‚îÇ  Object Storage ‚îÇ         ‚îÇ  Compute Cluster ‚îÇ           ‚îÇ\n",
    "‚îÇ   ‚îÇ  (S3/ADLS/GCS)  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  (Spark on EMR/  ‚îÇ           ‚îÇ\n",
    "‚îÇ   ‚îÇ                 ‚îÇ         ‚îÇ   Dataproc/etc.) ‚îÇ           ‚îÇ\n",
    "‚îÇ   ‚îÇ  üìÅ Raw Data    ‚îÇ         ‚îÇ                  ‚îÇ           ‚îÇ\n",
    "‚îÇ   ‚îÇ  üìÅ Processed   ‚îÇ         ‚îÇ  üî• Processing   ‚îÇ           ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
    "‚îÇ                                        ‚îÇ                     ‚îÇ\n",
    "‚îÇ                                        ‚ñº                     ‚îÇ\n",
    "‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ\n",
    "‚îÇ                              ‚îÇ   Dashboard/BI   ‚îÇ            ‚îÇ\n",
    "‚îÇ                              ‚îÇ   (Visualization)‚îÇ            ‚îÇ\n",
    "‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Popular Spark platforms\n",
    "\n",
    "| Platform | Provider | Key Features |\n",
    "|----------|----------|-------------|\n",
    "| **Databricks** | Multi-cloud | Spark-native, notebooks, MLflow |\n",
    "| **Amazon EMR** | AWS | Managed Hadoop/Spark clusters |\n",
    "| **Google Dataproc** | GCP | Fast cluster startup, GCS integration |\n",
    "| **Azure Synapse** | Azure | Analytics + data warehouse combo |\n",
    "| **Azure HDInsight** | Azure | Open-source analytics service |\n",
    "\n",
    "### Beginner tip: Separate storage from compute\n",
    "In many modern architectures:\n",
    "- **Data** lives in cheap object storage (S3, ADLS, GCS)\n",
    "- **Compute clusters** are created on-demand when needed\n",
    "- Clusters are shut down when done ‚Üí saves money!\n",
    "\n",
    "> üí° **Tip:** Start learning with local mode, then move to cloud when you need real scale or collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a60bb",
   "metadata": {},
   "source": [
    "## Exercises (Practice)\n",
    "Try these after the explanations. Keep datasets small while learning, but write code as if data could be large.\n",
    "\n",
    "### Exercise 1 ‚Äî Transformations vs actions\n",
    "1. Create a new DataFrame column `value_squared = value_filled ** 2`\n",
    "2. Filter to keep only rows where `value_squared > 100`\n",
    "3. Trigger an action to count the rows\n",
    "\n",
    "*Goal:* practice chaining transformations and finishing with an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30632f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution (starter template)\n",
    "\n",
    "if df is not None:\n",
    "    ex1 = (\n",
    "        cleaned\n",
    "        .withColumn('value_squared', F.col('value_filled') * F.col('value_filled'))\n",
    "        .filter(F.col('value_squared') > 100)\n",
    "    )\n",
    "    # Action\n",
    "    print('Rows with value_squared > 100:', ex1.count())\n",
    "    ex1.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21565d",
   "metadata": {},
   "source": [
    "### Exercise 2 ‚Äî GroupBy challenge\n",
    "Compute, for each `group`, the percentage of missing values in the original `value` column.\n",
    "\n",
    "Hints:\n",
    "- Use `is_missing` from the `cleaned` DataFrame\n",
    "- `percentage = 100 * missing / total`\n",
    "- Use `F.sum` and `F.count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    ex2 = (\n",
    "        cleaned\n",
    "        .groupBy('group')\n",
    "        .agg(\n",
    "            F.count('*').alias('total_rows'),\n",
    "            F.sum('is_missing').alias('missing_rows'),\n",
    "        )\n",
    "        .withColumn('missing_pct', F.round(F.col('missing_rows') / F.col('total_rows') * 100, 2))\n",
    "        .orderBy('group')\n",
    "    )\n",
    "    ex2.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d8ec2",
   "metadata": {},
   "source": [
    "### Exercise 3 ‚Äî RDD word count (concept check)\n",
    "1. Create an RDD from a list of sentences\n",
    "2. Compute word counts\n",
    "3. Show the top 5 words\n",
    "\n",
    "*Tip:* Use `takeOrdered(5, key=...)` or convert to a DataFrame after counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark is not None:\n",
    "    sc = spark.sparkContext\n",
    "    sentences = [\n",
    "        'data data data pipelines',\n",
    "        'pipelines run on spark',\n",
    "        'spark helps scale data analytics'\n",
    "    ]\n",
    "    rdd = sc.parallelize(sentences)\n",
    "    counts = (\n",
    "        rdd.flatMap(lambda s: s.split())\n",
    "           .map(lambda w: (w.lower(), 1))\n",
    "           .reduceByKey(lambda a, b: a + b)\n",
    "    )\n",
    "    top5 = counts.takeOrdered(5, key=lambda kv: -kv[1])\n",
    "    print(top5)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ef15b",
   "metadata": {},
   "source": [
    "### Exercise 4 ‚Äî Reading and Writing Parquet Files\n",
    "Parquet is the recommended format for Spark. Practice reading and writing data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write the `cleaned` DataFrame to a Parquet file\n",
    "2. Read it back and verify the data\n",
    "3. Compare with CSV (optional)\n",
    "\n",
    "> üí° **Note:** Parquet files are actually folders containing multiple partition files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca467b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Working with Parquet files\n",
    "# Parquet is columnar and compressed - ideal for big data\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "if df is not None:\n",
    "    # Create a temporary directory for our example\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    parquet_path = os.path.join(temp_dir, 'sample_data.parquet')\n",
    "    \n",
    "    # Write to Parquet\n",
    "    cleaned.write.mode('overwrite').parquet(parquet_path)\n",
    "    print(f\"‚úÖ Data written to: {parquet_path}\")\n",
    "    \n",
    "    # Read it back\n",
    "    df_from_parquet = spark.read.parquet(parquet_path)\n",
    "    print(\"\\nüìñ Data read back from Parquet:\")\n",
    "    df_from_parquet.show()\n",
    "    \n",
    "    # Show schema is preserved\n",
    "    print(\"Schema preserved:\")\n",
    "    df_from_parquet.printSchema()\n",
    "else:\n",
    "    print('Spark not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3565c",
   "metadata": {},
   "source": [
    "## Mini‚ÄëProject: Clickstream Summary (Spark DataFrames)\n",
    "In real analytics, a common big-data dataset is **clickstream / event logs**.\n",
    "\n",
    "You will simulate a small clickstream table and answer questions that scale to large logs.\n",
    "\n",
    "### Your tasks\n",
    "1. Create a DataFrame with columns: `user_id`, `event_type`, `ts` (timestamp string)\n",
    "2. Compute events per user\n",
    "3. Compute event counts per type\n",
    "4. Find the most active user\n",
    "5. (Optional) Extract the date from `ts` and compute events per day\n",
    "\n",
    "**Beginner warning:** do not use `collect()` on raw events; aggregate first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark is not None:\n",
    "    events = [\n",
    "        (101, 'view',  '2026-01-02 10:00:00'),\n",
    "        (101, 'click', '2026-01-02 10:01:00'),\n",
    "        (102, 'view',  '2026-01-02 11:00:00'),\n",
    "        (103, 'view',  '2026-01-03 09:00:00'),\n",
    "        (101, 'view',  '2026-01-03 09:05:00'),\n",
    "        (102, 'click', '2026-01-03 10:00:00'),\n",
    "        (102, 'click', '2026-01-03 10:02:00'),\n",
    "    ]\n",
    "\n",
    "    events_schema = T.StructType([\n",
    "        T.StructField('user_id', T.IntegerType(), nullable=False),\n",
    "        T.StructField('event_type', T.StringType(), nullable=False),\n",
    "        T.StructField('ts', T.StringType(), nullable=False),\n",
    "    ])\n",
    "\n",
    "    events_df = spark.createDataFrame(events, schema=events_schema)\n",
    "    events_df.show(truncate=False)\n",
    "else:\n",
    "    events_df = None\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if events_df is not None:\n",
    "    # 1) Events per user\n",
    "    per_user = (\n",
    "        events_df.groupBy('user_id')\n",
    "                .agg(F.count('*').alias('events'))\n",
    "                .orderBy(F.desc('events'))\n",
    "    )\n",
    "    per_user.show()\n",
    "\n",
    "    # 2) Events per type\n",
    "    per_type = (\n",
    "        events_df.groupBy('event_type')\n",
    "                .agg(F.count('*').alias('events'))\n",
    "                .orderBy(F.desc('events'))\n",
    "    )\n",
    "    per_type.show()\n",
    "\n",
    "    # 3) Most active user\n",
    "    most_active = per_user.limit(1)\n",
    "    most_active.show()\n",
    "\n",
    "    # 4) Optional: events per day\n",
    "    events_with_day = events_df.withColumn('day', F.to_date(F.col('ts')))\n",
    "    per_day = (\n",
    "        events_with_day.groupBy('day')\n",
    "                      .agg(F.count('*').alias('events'))\n",
    "                      .orderBy('day')\n",
    "    )\n",
    "    per_day.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443aca8",
   "metadata": {},
   "source": [
    "## Tips, Warnings, and Common Mistakes\n",
    "\n",
    "### ‚ùå Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | What happens | How to fix |\n",
    "|---------|-------------|-----------|\n",
    "| `df.collect()` on huge DataFrame | Driver crashes (out of memory) | Use `show()`, `limit()`, or aggregate first |\n",
    "| `df.toPandas()` on big data | Memory errors | Only convert small, aggregated results |\n",
    "| Writing everything as Python UDFs | Slow execution | Use built-in `F.` functions first |\n",
    "| Ignoring partitions | Poor performance | Check with `df.rdd.getNumPartitions()` |\n",
    "| Not filtering early | Processing unnecessary data | Add `filter()` as early as possible |\n",
    "\n",
    "### üí° Pro Tips\n",
    "\n",
    "1. **Use `df.explain()`** to see Spark's execution plan ‚Äî helps debug slow queries\n",
    "2. **Filter early, select only needed columns** ‚Äî less data = faster processing  \n",
    "3. **Keep raw data in Parquet** ‚Äî columnar format, much faster than CSV\n",
    "4. **Cache strategically** ‚Äî only cache DataFrames you reuse multiple times\n",
    "5. **Start small** ‚Äî develop with a sample, then run on full data\n",
    "\n",
    "### Debugging slow Spark jobs\n",
    "\n",
    "If something is slow, check these first:\n",
    "\n",
    "```\n",
    "1. Are you shuffling huge data?\n",
    "   ‚îî‚îÄ Look for: joins, groupBy, repartition\n",
    "   ‚îî‚îÄ Fix: filter before join, use broadcast for small tables\n",
    "\n",
    "2. Are you collecting too much data to the driver?\n",
    "   ‚îî‚îÄ Look for: collect(), toPandas() on big data\n",
    "   ‚îî‚îÄ Fix: aggregate first, use show() or limit()\n",
    "\n",
    "3. Are you using slow Python UDFs?\n",
    "   ‚îî‚îÄ Look for: @udf decorated functions\n",
    "   ‚îî‚îÄ Fix: replace with built-in F.functions\n",
    "```\n",
    "\n",
    "### Understanding Spark execution plans\n",
    "\n",
    "Use `explain()` to see how Spark will execute your query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a971fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: view the execution plan (useful when you start optimizing).\n",
    "\n",
    "if df is not None:\n",
    "    summary.explain(True)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a91d4",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- **Spark Official Docs**: https://spark.apache.org/docs/latest/\n",
    "- **PySpark SQL Guide**: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- **PySpark API Reference**: https://spark.apache.org/docs/latest/api/python/\n",
    "\n",
    "### Learning Platforms\n",
    "- **Databricks Free Community Edition**: https://community.cloud.databricks.com/ (free Spark environment)\n",
    "- **Databricks Learning Academy**: https://www.databricks.com/learn\n",
    "- **Spark: The Definitive Guide** (book by Chambers & Zaharia)\n",
    "\n",
    "### Key topics to explore next\n",
    "When you read docs, focus on:\n",
    "1. **DataFrames** ‚Äî the main API for analytics\n",
    "2. **Transformations vs Actions** ‚Äî understand lazy evaluation\n",
    "3. **Joins** ‚Äî combining data from multiple sources\n",
    "4. **Partitioning** ‚Äî key to performance\n",
    "5. **File formats** ‚Äî especially Parquet\n",
    "\n",
    "### Related chapters in this book\n",
    "- **Chapter 4**: Pandas fundamentals (transfer your skills to Spark)\n",
    "- **Chapter 9**: SQL for Data Analysis (SQL works in Spark too!)\n",
    "- **Chapter 12**: Automation and Reproducibility (scheduling Spark jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03447b68",
   "metadata": {},
   "source": [
    "## Summary / Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|-----------|\n",
    "| **Big Data** | Defined by practical constraints (volume/velocity/variety), not a fixed size |\n",
    "| **Distributed Computing** | Split work across multiple machines using partitions |\n",
    "| **Spark Architecture** | Driver coordinates, Executors process partitions in parallel |\n",
    "| **Lazy Evaluation** | Transformations build a plan; Actions trigger execution |\n",
    "| **Fault Tolerance** | Spark can recompute lost partitions using lineage |\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "‚úÖ **Do:**\n",
    "- Start with Pandas; move to Spark when you hit limits\n",
    "- Use Spark DataFrames (not RDDs) for analytics\n",
    "- Filter early, select only needed columns\n",
    "- Use built-in functions (`F.lower()`, `F.when()`, etc.)\n",
    "- Aggregate before converting to Pandas\n",
    "- Use Parquet format for large datasets\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Use `collect()` or `toPandas()` on large data\n",
    "- Write Python UDFs when built-in functions exist\n",
    "- Ignore partition count (too few = slow, too many = overhead)\n",
    "\n",
    "### When to use Spark vs Pandas\n",
    "\n",
    "```\n",
    "Use Pandas when:                    Use Spark when:\n",
    "‚îú‚îÄ Data fits in memory              ‚îú‚îÄ Data too large for one machine\n",
    "‚îú‚îÄ Quick, interactive analysis      ‚îú‚îÄ Processing takes too long\n",
    "‚îú‚îÄ Simple transformations           ‚îú‚îÄ Need fault tolerance\n",
    "‚îî‚îÄ Learning / prototyping           ‚îî‚îÄ Production pipelines at scale\n",
    "```\n",
    "\n",
    "### What's next?\n",
    "- **Practice** with the exercises in this chapter\n",
    "- **Try Databricks Community Edition** for a free Spark environment\n",
    "- **Explore Spark SQL** ‚Äî you can use SQL directly in Spark!\n",
    "- **Learn about joins and partitioning** for real-world data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand the fundamentals of big data analytics with Spark. These concepts will serve you well as data sizes grow in your analytics career."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12f33e",
   "metadata": {},
   "source": [
    "## Cleanup: Stop Spark Session\n",
    "Always stop your Spark session when done to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76927efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when you're done\n",
    "# This frees up resources (especially important in shared environments)\n",
    "\n",
    "if spark is not None:\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Spark session stopped successfully.\")\n",
    "else:\n",
    "    print(\"No Spark session to stop.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
