{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae41100",
   "metadata": {},
   "source": [
    "# Chapter 4: Data Manipulation and Analysis Using Pandas\n",
    "\n",
    "This chapter teaches you how to load, inspect, clean, transform, and analyze data using **Pandas** ‚Äî with hands-on examples, exercises, and a mini-project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50219d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Why Pandas is Essential for Data Analysts**\n",
    "\n",
    "In data analytics, most of your time is spent on things like:\n",
    "- üì• Importing data (CSV, Excel, databases)\n",
    "- üîç Understanding what the data looks like\n",
    "- üßπ Cleaning issues (missing values, wrong types, duplicates)\n",
    "- üîÑ Transforming it into a useful shape\n",
    "- üìä Summarizing it (grouping, aggregating)\n",
    "\n",
    "Pandas is the most common Python library for these jobs ‚Äî and mastering it will make you productive in nearly any data analytics role.\n",
    "\n",
    "**What you'll learn in this chapter:**\n",
    "- How to load data from files and databases\n",
    "- How to inspect, clean, and transform data\n",
    "- How to filter, sort, and aggregate data\n",
    "- How to merge multiple datasets\n",
    "- How to work with dates and time series\n",
    "- How to export your results\n",
    "\n",
    "**What you'll build:** A complete workflow that goes from \"raw data\" ‚Üí \"clean data\" ‚Üí \"summary + chart\" ‚Üí \"exported results\".\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge (Chapter 2) and familiarity with NumPy arrays (Chapter 3) will help, but are not strictly required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a6bea",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven‚Äôt installed Pandas yet, you can install it in your environment with:\n",
    "\n",
    "- `pip install pandas`\n",
    "\n",
    "We‚Äôll also use NumPy (for numeric helpers) and Matplotlib (for a few quick plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0726cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdf9c9",
   "metadata": {},
   "source": [
    "## 1) Pandas architecture: `Series` and `DataFrame`\n",
    "\n",
    "Think of Pandas like this:\n",
    "- A **Series** is a *single column* (1D) with an **index** (labels).\n",
    "- A **DataFrame** is a *table of columns* (2D). Every column is a Series, and rows are aligned by an index.\n",
    "\n",
    "Why this matters:\n",
    "- Most analysis is ‚Äúcolumn-based‚Äù: you compute new columns, filter rows, group, and summarize.\n",
    "- The **index** helps Pandas align data safely during operations (especially when merging or adding data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\n",
    "df = pd.DataFrame({\n",
    "    \"product\": [\"Pen\", \"Notebook\", \"Pencil\"],\n",
    "    \"price\": [1.50, 3.00, 0.75],\n",
    "    \"in_stock\": [True, True, False]\n",
    "})\n",
    "\n",
    "s, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56d5a4",
   "metadata": {},
   "source": [
    "> **Tip (Beginner-friendly rule):** When you see ‚Äúindex‚Äù in Pandas, think ‚Äúrow labels‚Äù.\n",
    ">\n",
    "> By default, Pandas uses `0, 1, 2, ...` but you can use something meaningful (like an `order_id`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bc1ec",
   "metadata": {},
   "source": [
    "## 2) Importing data from files and databases\n",
    "\n",
    "In the real world, you often start by importing data from:\n",
    "- CSV files (`pd.read_csv`)\n",
    "- Excel files (`pd.read_excel`)\n",
    "- JSON (`pd.read_json`)\n",
    "- databases (e.g., SQLite) using `pd.read_sql_query`\n",
    "\n",
    "We‚Äôll demo CSV and a small SQLite database **without downloading anything**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "csv_text = StringIO(\"\"\"order_id,customer,product,quantity,unit_price,order_date\n",
    "1001,Ana,Pen,2,1.50,2025-01-05\n",
    "1002,Ben,Notebook,1,3.00,2025-01-06\n",
    "1003,Ana,Pencil,5,0.75,2025-01-06\n",
    "1004,Chen,Notebook,2,3.00,2025-01-07\n",
    "\"\"\")\n",
    "\n",
    "orders = pd.read_csv(csv_text)\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "orders.to_sql(\"orders\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT customer, SUM(quantity * unit_price) AS revenue\n",
    "FROM orders\n",
    "GROUP BY customer\n",
    "ORDER BY revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "revenue_by_customer = pd.read_sql_query(query, conn)\n",
    "revenue_by_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340651f",
   "metadata": {},
   "source": [
    "> **Common mistake:** letting Pandas guess the wrong types.\n",
    ">\n",
    "> For example, dates usually load as plain text first. We‚Äôll fix that using `pd.to_datetime` later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12507b",
   "metadata": {},
   "source": [
    "## 3) Data inspection and summary methods\n",
    "\n",
    "Before you clean or analyze, do a quick inspection. Your goals are:\n",
    "- see a few rows\n",
    "- check column names\n",
    "- check data types\n",
    "- look for missing values\n",
    "- get quick summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c88bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d96a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336783dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4f478",
   "metadata": {},
   "source": [
    "### Exercise 1 ‚Äî Quick inspection\n",
    "1. How many rows and columns are in `orders`?\n",
    "2. What are the numeric columns?\n",
    "3. Which customers appear in the data, and how many orders does each have?\n",
    "\n",
    "*Try it yourself first, then run the solution cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape (rows, cols):\", orders.shape)\n",
    "\n",
    "numeric_cols = orders.select_dtypes(include=\"number\").columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "orders_per_customer = orders[\"customer\"].value_counts()\n",
    "orders_per_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21d4c4",
   "metadata": {},
   "source": [
    "## 4) Indexing and selection techniques\n",
    "\n",
    "Selecting the right rows/columns is a daily task. The two most important tools are:\n",
    "- `loc` for **label-based** selection\n",
    "- `iloc` for **position-based** selection\n",
    "\n",
    "You‚Äôll also use boolean filtering (conditions) a lot.\n",
    "\n",
    "> **Warning (very common bug):** avoid chained indexing like `df[df['x'] > 0]['y'] = 1`.\n",
    "> It can silently fail. Prefer `df.loc[mask, 'y'] = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection\n",
    "orders[[\"order_id\", \"customer\", \"product\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean filtering (rows)\n",
    "mask = (orders[\"product\"] == \"Notebook\") & (orders[\"quantity\"] >= 2)\n",
    "orders.loc[mask, [\"order_id\", \"customer\", \"quantity\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00872e",
   "metadata": {},
   "source": [
    "### Exercise 2 ‚Äî Filtering practice\n",
    "Create a filtered DataFrame containing only orders from customer `Ana`, and only show the columns `product`, `quantity`, and `unit_price`.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee86b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_orders = orders.loc[orders[\"customer\"] == \"Ana\", [\"product\", \"quantity\", \"unit_price\"]]\n",
    "ana_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce9943",
   "metadata": {},
   "source": [
    "## 5) Sorting and filtering data\n",
    "\n",
    "Once you can select rows, you‚Äôll often want to:\n",
    "- sort rows (e.g., biggest sales first)\n",
    "- filter using helper methods like `isin`, `between`, or `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df994d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders2 = orders.copy()\n",
    "orders2[\"total\"] = orders2[\"quantity\"] * orders2[\"unit_price\"]\n",
    "\n",
    "orders2.sort_values(by=\"total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only some products\n",
    "orders2[orders2[\"product\"].isin([\"Pen\", \"Notebook\"]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d29a5",
   "metadata": {},
   "source": [
    "## 6) Handling missing and duplicate data\n",
    "\n",
    "Real datasets often contain:\n",
    "- missing values (`NaN`)\n",
    "- duplicate rows\n",
    "\n",
    "A good cleaning approach is:\n",
    "1. *measure* the problem (how many missing/duplicates?)\n",
    "2. decide a strategy (drop, fill, or fix)\n",
    "3. apply the strategy and re-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aaf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty = orders2.copy()\n",
    "dirty.loc[1, \"unit_price\"] = np.nan  # introduce missing value\n",
    "dirty = pd.concat([dirty, dirty.iloc[[2]]], ignore_index=True)  # duplicate a row\n",
    "dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values per column:\")\n",
    "print(dirty.isna().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows (count):\", dirty.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy example: fill missing unit_price using the median price\n",
    "cleaned = dirty.copy()\n",
    "median_price = cleaned[\"unit_price\"].median()\n",
    "cleaned[\"unit_price\"] = cleaned[\"unit_price\"].fillna(median_price)\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "cleaned = cleaned.drop_duplicates()\n",
    "\n",
    "cleaned.isna().sum(), cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a806625",
   "metadata": {},
   "source": [
    "> **Tip:** Don‚Äôt automatically drop rows with missing values.\n",
    ">\n",
    "> Ask: *Is missingness meaningful?* For example, missing prices might mean ‚Äúprice not recorded‚Äù, which could be a data quality problem you need to report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a71a4",
   "metadata": {},
   "source": [
    "## 7) Data type conversion\n",
    "\n",
    "Data types matter because they control what you can do:\n",
    "- numeric columns can be summed/averaged\n",
    "- datetime columns can be sorted, resampled, and grouped by time\n",
    "\n",
    "Two very common conversions:\n",
    "- `pd.to_numeric(...)`\n",
    "- `pd.to_datetime(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f886b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "typed = cleaned.copy()\n",
    "typed[\"order_date\"] = pd.to_datetime(typed[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "typed.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d5b05",
   "metadata": {},
   "source": [
    "> **Warning:** `errors='coerce'` turns invalid values into `NaT` (missing datetime).\n",
    ">\n",
    "> That's often useful, but it can also hide a data quality problem. Always check how many values became missing after conversion.\n",
    "\n",
    "### Exercise 4 ‚Äî Data type conversion\n",
    "The `orders` DataFrame has `order_date` as a string. Convert it to datetime and extract the day of the week (Monday=0, Sunday=6) into a new column called `weekday`.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78874869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 4\n",
    "orders_with_date = orders.copy()\n",
    "orders_with_date[\"order_date\"] = pd.to_datetime(orders_with_date[\"order_date\"], errors=\"coerce\")\n",
    "orders_with_date[\"weekday\"] = orders_with_date[\"order_date\"].dt.dayofweek\n",
    "orders_with_date[[\"order_id\", \"order_date\", \"weekday\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "typed[\"order_date\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce329f",
   "metadata": {},
   "source": [
    "## 8) GroupBy and aggregation\n",
    "\n",
    "`groupby` answers questions like:\n",
    "- total revenue per customer\n",
    "- average quantity per product\n",
    "- number of orders per day\n",
    "\n",
    "A typical pattern is:\n",
    "1. create a useful measure (like `total = quantity * unit_price`)\n",
    "2. group by one or more columns\n",
    "3. aggregate (sum, mean, count, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = typed.copy()\n",
    "analysis[\"total\"] = analysis[\"quantity\"] * analysis[\"unit_price\"]\n",
    "\n",
    "by_customer = (\n",
    "    analysis.groupby(\"customer\", as_index=False)\n",
    "    .agg(orders=(\"order_id\", \"count\"), revenue=(\"total\", \"sum\"), avg_order_value=(\"total\", \"mean\"))\n",
    "    .sort_values(by=\"revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "by_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual example: revenue per customer\n",
    "ax = by_customer.set_index(\"customer\")[\"revenue\"].plot(kind=\"bar\", title=\"Revenue by Customer\")\n",
    "ax.set_xlabel(\"Customer\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2869aa1",
   "metadata": {},
   "source": [
    "### Exercise 3 ‚Äî GroupBy\n",
    "Compute total quantity sold per product and sort from highest to lowest.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_by_product = (\n",
    "    analysis.groupby(\"product\", as_index=False)\n",
    "    .agg(total_qty=(\"quantity\", \"sum\"))\n",
    "    .sort_values(by=\"total_qty\", ascending=False)\n",
    ")\n",
    "qty_by_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337530",
   "metadata": {},
   "source": [
    "## 9) Merging, joining, and concatenation\n",
    "\n",
    "Data often comes in multiple tables. You combine them using:\n",
    "- `pd.merge(...)` (SQL-style joins: inner, left, right, outer)\n",
    "- `df.join(...)` (join by index, or by a key column)\n",
    "- `pd.concat([...])` (stack tables vertically or place them side-by-side)\n",
    "\n",
    "We‚Äôll create a small product table and join it to orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a15316",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.DataFrame({\n",
    "    \"product\": [\"Pen\", \"Notebook\", \"Pencil\"],\n",
    "    \"category\": [\"Stationery\", \"Stationery\", \"Stationery\"],\n",
    "    \"reorder_level\": [20, 10, 30]\n",
    "})\n",
    "\n",
    "merged = pd.merge(analysis, products, on=\"product\", how=\"left\", validate=\"many_to_one\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation example (stack rows)\n",
    "more_orders = pd.DataFrame({\n",
    "    \"order_id\": [2001],\n",
    "    \"customer\": [\"Dana\"],\n",
    "    \"product\": [\"Pen\"],\n",
    "    \"quantity\": [3],\n",
    "    \"unit_price\": [1.50],\n",
    "    \"order_date\": [pd.Timestamp(\"2025-01-08\")],\n",
    "    \"total\": [4.50],\n",
    "})\n",
    "\n",
    "combined = pd.concat([analysis, more_orders], ignore_index=True)\n",
    "combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b18c0",
   "metadata": {},
   "source": [
    "> **Common mistakes when merging:**\n",
    "> - joining on columns with different spelling/case (`Product` vs `product`)\n",
    "> - joining on columns with different types (string vs integer)\n",
    "> - accidental \"many-to-many\" joins that duplicate rows\n",
    ">\n",
    "> Tip: `validate='many_to_one'` (as shown) can catch mistakes early.\n",
    "\n",
    "### Exercise 5 ‚Äî Merging practice\n",
    "Create a `customers` DataFrame with columns `customer` and `city` (make up 3-4 customers). Then merge it with the `analysis` DataFrame to add the city information to each order.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fe36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 5\n",
    "customers = pd.DataFrame({\n",
    "    \"customer\": [\"Ana\", \"Ben\", \"Chen\", \"Dana\"],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
    "})\n",
    "\n",
    "orders_with_city = pd.merge(analysis, customers, on=\"customer\", how=\"left\")\n",
    "orders_with_city[[\"order_id\", \"customer\", \"city\", \"product\", \"total\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f98d1",
   "metadata": {},
   "source": [
    "## 10) Reshaping data: `pivot` and `melt`\n",
    "\n",
    "Data can be **wide** or **long**:\n",
    "- **wide:** many columns (e.g., one column per month)\n",
    "- **long:** fewer columns, more rows (e.g., month stored as values)\n",
    "\n",
    "Why reshape?\n",
    "- many visualizations and groupby operations prefer long data\n",
    "- reporting tables often prefer wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_revenue = (\n",
    "    analysis.groupby([\"order_date\", \"product\"], as_index=False)\n",
    "    .agg(revenue=(\"total\", \"sum\"))\n",
    ")\n",
    "\n",
    "wide = daily_revenue.pivot(index=\"order_date\", columns=\"product\", values=\"revenue\").fillna(0)\n",
    "wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_again = wide.reset_index().melt(id_vars=\"order_date\", var_name=\"product\", value_name=\"revenue\")\n",
    "long_again.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62d946",
   "metadata": {},
   "source": [
    "> **Tip:** Use `pivot` when you want to reshape from long to wide format (e.g., for reporting tables).\n",
    "> Use `melt` when you want to go from wide to long format (e.g., for plotting or groupby operations).\n",
    "\n",
    "### Exercise 6 ‚Äî Reshaping practice\n",
    "Using the `wide` DataFrame created above, convert it back to long format using `melt`, then filter to show only rows where revenue > 0.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 6\n",
    "long_filtered = wide.reset_index().melt(id_vars=\"order_date\", var_name=\"product\", value_name=\"revenue\")\n",
    "long_filtered = long_filtered[long_filtered[\"revenue\"] > 0]\n",
    "long_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4acb56",
   "metadata": {},
   "source": [
    "## 11) Working with dates and time series\n",
    "\n",
    "When a column is a datetime type, Pandas gives you powerful tools:\n",
    "- `.dt` accessor (year, month, day, weekday, etc.)\n",
    "- sorting by time\n",
    "- resampling (daily ‚Üí weekly/monthly)\n",
    "- rolling averages\n",
    "\n",
    "We‚Äôll build a simple daily revenue time series and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (\n",
    "    analysis.groupby(\"order_date\", as_index=True)[\"total\"]\n",
    "    .sum()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f41962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ts.plot(marker=\"o\", title=\"Daily Revenue\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1904348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling average smooths short-term noise (window=2 days here)\n",
    "rolling = ts.rolling(window=2, min_periods=1).mean()\n",
    "\n",
    "ax = ts.plot(marker=\"o\", label=\"Daily\")\n",
    "rolling.plot(ax=ax, label=\"2-day rolling mean\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Daily Revenue with Rolling Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4744",
   "metadata": {},
   "source": [
    "> **Tip:** Rolling averages are useful for smoothing noisy data and identifying trends.\n",
    "> Common window sizes: 7 days (weekly), 30 days (monthly), depending on your data frequency.\n",
    "\n",
    "### Exercise 7 ‚Äî Time series practice\n",
    "Using the `ts` time series created above, compute a cumulative sum of daily revenue and display it.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e58a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 7\n",
    "cumulative_revenue = ts.cumsum()\n",
    "\n",
    "ax = cumulative_revenue.plot(marker=\"o\", title=\"Cumulative Daily Revenue\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Cumulative Revenue\")\n",
    "plt.show()\n",
    "\n",
    "cumulative_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d94839",
   "metadata": {},
   "source": [
    "## 12) Exporting processed data\n",
    "\n",
    "After cleaning/analysis, you often export results for:\n",
    "- reporting\n",
    "- dashboards\n",
    "- sharing with other teams\n",
    "\n",
    "Common exports:\n",
    "- CSV (`to_csv`)\n",
    "- Excel (`to_excel`)\n",
    "- Parquet (`to_parquet`) for efficient analytics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a summary table to CSV (writes to the current working directory)\n",
    "output_path = \"chapter04_revenue_by_customer.csv\"\n",
    "by_customer.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ec9bc",
   "metadata": {},
   "source": [
    "> **Warning:** Exporting overwrites files by default if the name already exists.\n",
    ">\n",
    "> If you're experimenting, use unique filenames or export into a dedicated folder.\n",
    "\n",
    "> **Tip:** For large datasets, consider using `to_parquet()` format instead of CSV. Parquet files are:\n",
    "> - Much smaller (compressed)\n",
    "> - Faster to read/write\n",
    "> - Preserve data types automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a39c67",
   "metadata": {},
   "source": [
    "## Mini‚ÄëProject: Clean and analyze a messy sales dataset\n",
    "\n",
    "In this mini‚Äëproject you will:\n",
    "1. create a small ‚Äúmessy‚Äù dataset (missing values, duplicates, wrong types)\n",
    "2. clean it (types, missing data, duplicates)\n",
    "3. compute revenue and summarize by product and by week\n",
    "4. plot a simple time trend\n",
    "5. export the cleaned dataset and a summary table\n",
    "\n",
    "This is a realistic end‚Äëto‚Äëend workflow for beginner analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449256e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.DataFrame({\n",
    "    \"order_id\": [3001, 3002, 3003, 3004, 3004],  # duplicate order_id + full duplicate row\n",
    "    \"customer\": [\"Ana\", \"Ben\", \"Ana\", \"Chen\", \"Chen\"],\n",
    "    \"product\": [\"Pen\", \"Notebook\", \"Pencil\", \"Notebook\", \"Notebook\"],\n",
    "    \"quantity\": [\"2\", \"1\", \"five\", \"2\", \"2\"],  # wrong types\n",
    "    \"unit_price\": [1.5, None, 0.75, 3.0, 3.0],  # missing value\n",
    "    \"order_date\": [\"2025-02-01\", \"2025-02-02\", \"2025-02-02\", \"2025-02-08\", \"2025-02-08\"]\n",
    "})\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = raw.copy()\n",
    "\n",
    "# 1) Convert types safely\n",
    "clean[\"order_date\"] = pd.to_datetime(clean[\"order_date\"], errors=\"coerce\")\n",
    "clean[\"quantity\"] = pd.to_numeric(clean[\"quantity\"], errors=\"coerce\")\n",
    "clean[\"unit_price\"] = pd.to_numeric(clean[\"unit_price\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Handle missing values\n",
    "clean[\"unit_price\"] = clean[\"unit_price\"].fillna(clean[\"unit_price\"].median())\n",
    "\n",
    "# If quantity couldn't be converted (e.g., 'five'), decide a strategy.\n",
    "# Here we'll drop those rows because we can't compute revenue reliably.\n",
    "clean = clean.dropna(subset=[\"quantity\"])\n",
    "\n",
    "# 3) Remove duplicates\n",
    "clean = clean.drop_duplicates()\n",
    "\n",
    "# 4) Add revenue\n",
    "clean[\"revenue\"] = clean[\"quantity\"] * clean[\"unit_price\"]\n",
    "\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34416774",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_product = (\n",
    "    clean.groupby(\"product\", as_index=False)\n",
    "    .agg(orders=(\"order_id\", \"count\"), revenue=(\"revenue\", \"sum\"))\n",
    "    .sort_values(by=\"revenue\", ascending=False)\n",
    ")\n",
    "summary_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86724cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly revenue (time series style)\n",
    "weekly = (\n",
    "    clean.set_index(\"order_date\")[\"revenue\"]\n",
    "    .resample(\"W\")\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "ax = weekly.plot(kind=\"bar\", title=\"Weekly Revenue\")\n",
    "ax.set_xlabel(\"Week\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "plt.show()\n",
    "\n",
    "weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.to_csv(\"chapter04_clean_sales.csv\", index=False)\n",
    "summary_product.to_csv(\"chapter04_sales_summary_by_product.csv\", index=False)\n",
    "\n",
    "[\"chapter04_clean_sales.csv\", \"chapter04_sales_summary_by_product.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d88368",
   "metadata": {},
   "source": [
    "## Optional resources\n",
    "- Pandas User Guide: https://pandas.pydata.org/docs/user_guide/\n",
    "- 10 minutes to pandas: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "- Pandas `merge` docs: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "- Time series / resampling: https://pandas.pydata.org/docs/user_guide/timeseries.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b7e0",
   "metadata": {},
   "source": [
    "## Summary / Key Takeaways\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Series** = one labeled column; **DataFrame** = a table of Series.\n",
    "- The **index** provides row labels and enables powerful alignment during operations.\n",
    "\n",
    "**Workflow Essentials:**\n",
    "- Start every analysis by inspecting: `head`, `info`, `describe`, missing counts.\n",
    "- Use `loc`/`iloc` + boolean masks for safe, readable selection.\n",
    "- Clean data intentionally: measure issues ‚Üí choose a strategy ‚Üí re-check.\n",
    "- Convert types early (especially dates) using `pd.to_datetime` / `pd.to_numeric`.\n",
    "\n",
    "**Analysis & Aggregation:**\n",
    "- Summarize with `groupby` + `agg`, and visualize quick insights with simple plots.\n",
    "- Combine tables with `merge`/`join`, stack with `concat`, reshape with `pivot`/`melt`.\n",
    "\n",
    "**Time Series & Export:**\n",
    "- Use `.dt` accessor for datetime operations; use `resample` for time-based aggregation.\n",
    "- Export results with `to_csv` (and other formats) to share or report.\n",
    "\n",
    "**Exercises Completed:**\n",
    "1. Quick inspection (shape, dtypes, value counts)\n",
    "2. Filtering practice (boolean masks with `loc`)\n",
    "3. GroupBy aggregation\n",
    "4. Data type conversion (datetime, weekday extraction)\n",
    "5. Merging DataFrames\n",
    "6. Reshaping with pivot/melt\n",
    "7. Time series cumulative sum\n",
    "\n",
    "**What's Next:** In Chapter 5, you'll learn how to create effective visualizations to communicate your data insights using Matplotlib and Seaborn."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
