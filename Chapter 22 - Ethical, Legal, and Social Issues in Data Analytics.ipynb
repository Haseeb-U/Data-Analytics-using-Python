{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54a386e",
   "metadata": {},
   "source": [
    "# Chapter 22: Ethical, Legal, and Social Issues in Data Analytics\n",
    "\n",
    "This chapter helps you build the habit of doing **responsible analytics**â€”not just accurate analytics.\n",
    "\n",
    "> Educational content only â€” not legal advice. For real projects, involve your legal/compliance team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498bf06",
   "metadata": {},
   "source": [
    "## Introduction: why this matters\n",
    "\n",
    "Data analytics can help organizations make better decisions. But it can also cause harm if we:\n",
    "- expose private information\n",
    "- treat groups unfairly\n",
    "- mislead people with overconfident conclusions\n",
    "- use data in ways people did not agree to.\n",
    "\n",
    "In this chapter you will learn practical, beginner-friendly techniques to:\n",
    "- apply privacy principles to datasets and reports\n",
    "- understand regulatory frameworks at a high level (GDPR and others)\n",
    "- measure simple fairness gaps in models\n",
    "- explain model behavior in plain language\n",
    "- use a lightweight ethical checklist before shipping work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439574f9",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "We will use common beginner-friendly libraries:\n",
    "- `pandas`, `numpy` for data work\n",
    "- `matplotlib` for simple plots\n",
    "- `scikit-learn` for a small fairness + explainability demo\n",
    "\n",
    "If you donâ€™t have scikit-learn installed, run: `pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda33b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    from sklearn.inspection import permutation_importance\n",
    "except ImportError:\n",
    "    print('Optional dependency missing: scikit-learn. Install with: pip install scikit-learn')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.rcParams['figure.figsize'] = (8, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda81ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.1 Data privacy principles\n",
    "\n",
    "Privacy is about **protecting people from harm** caused by misuse or exposure of data.\n",
    "\n",
    "### Key principles (plain language)\n",
    "- **Data minimization**: collect/use only what you truly need.\n",
    "- **Purpose limitation**: donâ€™t quietly reuse data for unrelated goals.\n",
    "- **Storage limitation**: donâ€™t keep raw extracts forever.\n",
    "- **Access control**: limit who can see raw data.\n",
    "- **Security**: protect data in storage and transit.\n",
    "\n",
    "### Tip / common mistake\n",
    "A common beginner mistake is sharing row-level data in slides or emails. Prefer aggregated results and suppress small groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27db541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic (fake) customer-style dataset for privacy demos\n",
    "rng = np.random.default_rng(42)\n",
    "n = 30\n",
    "\n",
    "first_names = np.array(['Ava', 'Noah', 'Mia', 'Liam', 'Emma', 'Olivia', 'Ethan', 'Sophia'])\n",
    "last_names = np.array(['Smith', 'Khan', 'Garcia', 'Chen', 'Patel', 'Brown', 'Jones', 'Martin'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': np.arange(1000, 1000 + n),\n",
    "    'name': rng.choice(first_names, n) + ' ' + rng.choice(last_names, n),\n",
    "    'email': [f'user{i}@example.com' for i in range(n)],\n",
    "    'age': rng.integers(18, 70, size=n),\n",
    "    'city': rng.choice(['Cairo', 'Lagos', 'Nairobi', 'Casablanca'], size=n, p=[0.4, 0.2, 0.2, 0.2]),\n",
    "    'annual_spend_usd': np.round(rng.normal(1200, 450, size=n).clip(200, 4000), 2),\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae256e2",
   "metadata": {},
   "source": [
    "### Step-by-step: make an analysis-ready table\n",
    "\n",
    "**Why this step?** Most analytics questions do not require names or emails. Removing direct identifiers lowers risk.\n",
    "\n",
    "We will:\n",
    "1. Drop direct identifiers (`name`, `email`).\n",
    "2. Pseudonymize IDs so we can join tables without exposing raw IDs.\n",
    "3. Generalize age into bins (age groups) to reduce uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "analysis_df = df.drop(columns=['name', 'email']).copy()\n",
    "\n",
    "def hash_id(value: int, salt: str = 'demo_salt') -> str:\n",
    "    # In real systems, store salts/keys securely (never hard-code secrets).\n",
    "    payload = f'{salt}:{value}'.encode('utf-8')\n",
    "    return hashlib.sha256(payload).hexdigest()[:12]\n",
    "\n",
    "privacy_df = analysis_df.copy()\n",
    "privacy_df['customer_pid'] = privacy_df['customer_id'].apply(hash_id)\n",
    "privacy_df = privacy_df.drop(columns=['customer_id'])\n",
    "\n",
    "privacy_df['age_group'] = pd.cut(\n",
    "    privacy_df['age'],\n",
    "    bins=[17, 24, 34, 44, 54, 64, 100],\n",
    "    labels=['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    " )\n",
    "privacy_df = privacy_df.drop(columns=['age'])\n",
    "\n",
    "privacy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edc167",
   "metadata": {},
   "source": [
    "### Visual check: are small groups â€œuniqueâ€? (k-anonymity intuition)\n",
    "\n",
    "A simple privacy intuition: if a combination like `(city, age_group)` appears only once, that row is easier to single out.\n",
    "\n",
    "This isnâ€™t a full privacy guarantee, but it helps you avoid accidentally revealing individuals in reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = (\n",
    "    privacy_df.groupby(['city', 'age_group'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count')\n",
    ")\n",
    "group_counts.head(10)\n",
    "\n",
    "plt.hist(group_counts['count'], bins=np.arange(1, group_counts['count'].max() + 2) - 0.5, edgecolor='black')\n",
    "plt.title('Counts per (city, age_group) combination')\n",
    "plt.xlabel('Records in a combination')\n",
    "plt.ylabel('Number of combinations')\n",
    "plt.xticks(sorted(group_counts['count'].unique()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987fe55",
   "metadata": {},
   "source": [
    "### Exercise (Privacy)\n",
    "\n",
    "1. Create `spend_band` by binning `annual_spend_usd` into: low, medium, high, very_high.\n",
    "2. Recompute uniqueness counts using `(city, age_group, spend_band)`.\n",
    "3. Question: Why can adding more columns increase re-identification risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df = privacy_df.copy()\n",
    "exercise_df['spend_band'] = pd.cut(\n",
    "    exercise_df['annual_spend_usd'],\n",
    "    bins=[0, 700, 1200, 2000, 10000],\n",
    "    labels=['low', 'medium', 'high', 'very_high']\n",
    " )\n",
    "\n",
    "risk_counts = (\n",
    "    exercise_df.groupby(['city', 'age_group', 'spend_band'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count')\n",
    ")\n",
    "risk_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ee857",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.2 Regulatory frameworks (GDPR, etc.) â€” high-level overview\n",
    "\n",
    "Regulations differ by country/region, but many share similar goals: protect peopleâ€™s rights and require responsible handling.\n",
    "\n",
    "### GDPR (EU) â€” key ideas (simplified)\n",
    "- **Lawful basis**: you need a valid reason to process personal data (consent, contract, legal obligation, legitimate interests, etc.).\n",
    "- **Rights of individuals**: access, correction, deletion, portability, objection, and more.\n",
    "- **Accountability**: you must be able to show what you did and why (documentation matters).\n",
    "- **Privacy by design**: think about privacy from the start, not after building the system.\n",
    "\n",
    "### Other examples (not exhaustive)\n",
    "- CCPA/CPRA (California)\n",
    "- HIPAA (US health data)\n",
    "- Country/sector-specific laws\n",
    "\n",
    "**Practical analyst takeaway:** minimize data, document purpose, and share aggregated results when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_checklist = {\n",
    "    'project_name': 'Customer spend exploration',\n",
    "    'purpose': 'Understand spend patterns to support planning',\n",
    "    'fields_used': ['age_group', 'city', 'annual_spend_usd'],\n",
    "    'contains_direct_identifiers': False,\n",
    "    'contains_sensitive_data': False,\n",
    "    'sharing_plan': 'Aggregated charts only (no row-level export)',\n",
    "    'retention_plan': 'Delete raw extracts after 30 days; keep report 12 months',\n",
    "    'risks_and_mitigations': [\n",
    "        {'risk': 'Small groups can be unique', 'mitigation': 'Suppress small counts in reporting'}\n",
    "    ],\n",
    "}\n",
    "\n",
    "pd.json_normalize(analysis_checklist, sep='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bc9b4",
   "metadata": {},
   "source": [
    "### Exercise (Regulation-aware thinking)\n",
    "Answer briefly:\n",
    "1. What is a lawful basis (GDPR)?\n",
    "2. Name two individual rights.\n",
    "3. One action an analyst can take to reduce privacy risk.\n",
    "\n",
    "Then extend `analysis_checklist` with `contact_person` and `review_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8138c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_checklist2 = dict(analysis_checklist)\n",
    "analysis_checklist2['contact_person'] = 'Compliance / DPO (example)'\n",
    "analysis_checklist2['review_date'] = '2026-01-03'\n",
    "\n",
    "pd.json_normalize(analysis_checklist2, sep='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109856b7",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.3 Bias and fairness in analytics\n",
    "\n",
    "Bias can enter analytics through history, sampling, measurement, and modeling choices.\n",
    "\n",
    "A beginner-friendly fairness habit: **always check metrics by group** instead of relying on one overall number.\n",
    "\n",
    "Weâ€™ll create a small synthetic dataset with a `group` attribute (for teaching) and measure:\n",
    "- selection rate (how often the model predicts positive)\n",
    "- TPR (true positive rate / recall)\n",
    "- two simple gaps: demographic parity difference and equal opportunity difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "n = 2000\n",
    "\n",
    "group = rng.choice(['A', 'B'], size=n, p=[0.6, 0.4])\n",
    "income = rng.normal(50000, 15000, size=n).clip(10000, 150000)\n",
    "tenure_months = rng.integers(0, 60, size=n)\n",
    "\n",
    "# Create an outcome with an unfair shift between groups (for demonstration only)\n",
    "base_score = (income / 60000) + (tenure_months / 40)\n",
    "group_shift = np.where(group == 'A', 0.15, -0.05)\n",
    "prob = 1 / (1 + np.exp(-(base_score + group_shift - 1.6)))\n",
    "approved = rng.binomial(1, prob)\n",
    "\n",
    "fair_df = pd.DataFrame({\n",
    "    'group': group,\n",
    "    'income': income,\n",
    "    'tenure_months': tenure_months,\n",
    "    'approved': approved,\n",
    "})\n",
    "\n",
    "fair_df.groupby('group')['approved'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fair_df[['group', 'income', 'tenure_months']]\n",
    "y = fair_df['approved']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['group']),\n",
    "        ('num', 'passthrough', ['income', 'tenure_months']),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline([('preprocess', preprocess), ('clf', LogisticRegression(max_iter=1000))])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print('Overall accuracy:', round(accuracy_score(y_test, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d717ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = X_test.copy()\n",
    "results['y_true'] = y_test.to_numpy()\n",
    "results['y_pred'] = y_pred\n",
    "\n",
    "def group_metrics(df_group: pd.DataFrame) -> dict:\n",
    "    y_true = df_group['y_true'].to_numpy()\n",
    "    y_hat = df_group['y_pred'].to_numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    selection_rate = float(y_hat.mean())\n",
    "    tpr = float(tp / (tp + fn)) if (tp + fn) else np.nan\n",
    "    return {\n",
    "        'n': int(len(df_group)),\n",
    "        'accuracy': float(accuracy_score(y_true, y_hat)),\n",
    "        'selection_rate': selection_rate,\n",
    "        'TPR': float(tpr),\n",
    "    }\n",
    "\n",
    "metric_table = results.groupby('group').apply(lambda g: pd.Series(group_metrics(g))).reset_index()\n",
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a77c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = metric_table.set_index('group')\n",
    "dp_diff = float(mt.loc['A', 'selection_rate'] - mt.loc['B', 'selection_rate'])\n",
    "eo_diff = float(mt.loc['A', 'TPR'] - mt.loc['B', 'TPR'])\n",
    "\n",
    "print('Demographic parity difference (A - B):', round(dp_diff, 3))\n",
    "print('Equal opportunity difference (TPR A - TPR B):', round(eo_diff, 3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].bar(metric_table['group'], metric_table['selection_rate'])\n",
    "axes[0].set_title('Selection rate by group')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].bar(metric_table['group'], metric_table['TPR'])\n",
    "axes[1].set_title('TPR by group')\n",
    "axes[1].set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae21fe",
   "metadata": {},
   "source": [
    "### Exercise (Fairness)\n",
    "1. Change the threshold to `0.4` and `0.6`.\n",
    "2. Recompute demographic parity and equal opportunity differences.\n",
    "3. What changed: accuracy, selection rates, or TPRs? (Write 2â€“3 observations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(threshold: float) -> pd.DataFrame:\n",
    "    y_hat = (proba >= threshold).astype(int)\n",
    "    tmp = X_test.copy()\n",
    "    tmp['y_true'] = y_test.to_numpy()\n",
    "    tmp['y_pred'] = y_hat\n",
    "    mt_ = tmp.groupby('group').apply(lambda g: pd.Series(group_metrics(g))).reset_index()\n",
    "    mt_['threshold'] = threshold\n",
    "    mt_['overall_accuracy'] = float(accuracy_score(y_test, y_hat))\n",
    "    return mt_\n",
    "\n",
    "eval_table = pd.concat([evaluate_threshold(t) for t in [0.4, 0.5, 0.6]], ignore_index=True)\n",
    "eval_table[['threshold', 'group', 'selection_rate', 'TPR', 'accuracy', 'overall_accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248b380",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.4 Transparency and explainability\n",
    "\n",
    "> **Transparency** means being clear about what you did, what data you used, and the limits of your results.\n",
    "\n",
    "> **Explainability** means giving understandable reasons for outputs (especially for models).\n",
    "\n",
    "### Why it matters\n",
    "- People may be affected by decisions based on analytics (e.g., approvals, hiring, pricing).\n",
    "- Stakeholders need to trust and challenge results.\n",
    "- Regulations and internal policies often require documentation and explanations.\n",
    "\n",
    "### Important warning\n",
    "Explainability is **not** the same as fairness. A model can be easy to explain and still be unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce07858",
   "metadata": {},
   "source": [
    "### A practical, beginner-friendly method: permutation importance\n",
    "\n",
    "Permutation importance answers: **â€œIf I randomly shuffle this feature, how much does model performance drop?â€**\n",
    "\n",
    "- If performance drops a lot, the feature was important.\n",
    "- If performance barely changes, the feature mattered less (at least for this model and dataset).\n",
    "\n",
    "**Tip:** This is a *global* explanation (overall behavior). It does not explain one single row perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d55cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance on the trained pipeline model (from the fairness section)\n",
    "# Note: this requires scikit-learn. If sklearn imports failed earlier, install it and rerun Setup.\n",
    "\n",
    "base_acc = accuracy_score(y_test, y_pred)\n",
    "print('Baseline accuracy:', round(float(base_acc), 3))\n",
    "\n",
    "perm = permutation_importance(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=0,\n",
    "    scoring='accuracy',\n",
    " )\n",
    "\n",
    "# Try to get readable feature names (depends on sklearn version)\n",
    "try:\n",
    "    feature_names = model.named_steps['preprocess'].get_feature_names_out()\n",
    "except Exception:\n",
    "    feature_names = np.array([f'feature_{i}' for i in range(len(perm.importances_mean))])\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance_mean': perm.importances_mean,\n",
    "    'importance_std': perm.importances_std,\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "imp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae490fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top importances\n",
    "top_k = 10\n",
    "top = imp.head(top_k).iloc[::-1]  # reverse for horizontal bar plot\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.barh(top['feature'].astype(str), top['importance_mean'])\n",
    "plt.title('Permutation importance (top features)')\n",
    "plt.xlabel('Mean decrease in accuracy after shuffling')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plain-language interpretation helper\n",
    "print('Plain-language interpretation:')\n",
    "for _, row in imp.head(5).iterrows():\n",
    "    print(f\"- Shuffling '{row['feature']}' reduces accuracy by ~{row['importance_mean']:.3f} on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf5ad8",
   "metadata": {},
   "source": [
    "### Exercise (Explainability)\n",
    "\n",
    "1. Looking at the permutation importance results above, which feature is most important for the model's predictions?\n",
    "2. Write a 2-sentence plain-language summary explaining to a non-technical stakeholder what drives the model's decisions.\n",
    "3. What is one limitation of permutation importance as an explanation method?\n",
    "\n",
    "**Hint:** Think about whether it explains global vs local behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56221c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solution space\n",
    "# 1. Most important feature (fill in based on your results):\n",
    "most_important_feature = imp.iloc[0]['feature']\n",
    "print(f\"Most important feature: {most_important_feature}\")\n",
    "\n",
    "# 2. Plain-language summary (example):\n",
    "plain_summary = \"\"\"\n",
    "The model primarily uses [feature name] to make approval decisions.\n",
    "When this information is removed, the model's accuracy drops significantly,\n",
    "showing it relies heavily on this factor.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Limitation:\n",
    "# Permutation importance shows global/average importance, not why a specific\n",
    "# individual got a particular prediction. For individual explanations,\n",
    "# techniques like SHAP or LIME are more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdc14c",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.5 Responsible data usage\n",
    "\n",
    "Responsible data usage means treating dataâ€”and the people it representsâ€”with care throughout your analysis.\n",
    "\n",
    "### Key principles\n",
    "\n",
    "1. **Consent and awareness**: People should know how their data is being used. Even if data is anonymized, consider whether the original collection was transparent.\n",
    "\n",
    "2. **Proportionality**: Use data in ways that are proportionate to the benefit. Don't build invasive systems for trivial gains.\n",
    "\n",
    "3. **Accuracy and quality**: Using inaccurate or outdated data can lead to wrong conclusions that harm people or organizations.\n",
    "\n",
    "4. **Secondary use caution**: Data collected for one purpose may not be appropriate for another. Always ask: \"Would the data subjects expect this use?\"\n",
    "\n",
    "5. **Data lifecycle management**: Plan for the entire lifecycleâ€”collection, storage, use, sharing, archival, and deletion.\n",
    "\n",
    "### Common mistakes to avoid\n",
    "\n",
    "| Mistake | Why it matters | Better approach |\n",
    "|---------|----------------|-----------------|\n",
    "| Keeping data \"just in case\" | Increases breach risk and storage costs | Define retention periods upfront |\n",
    "| Sharing raw data in emails | Easy to forward, hard to recall | Share aggregated results or secure links |\n",
    "| Using production data for testing | Exposes real people's information | Create synthetic or anonymized test data |\n",
    "| Ignoring data quality issues | Garbage in, garbage out | Validate and clean before analysis |\n",
    "\n",
    "### Tip\n",
    "Before starting any analysis, ask yourself: **\"If the people in this dataset knew exactly how I'm using their data, would they be comfortable with it?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple data usage assessment template\n",
    "# Use this before starting any analysis project\n",
    "\n",
    "data_usage_assessment = {\n",
    "    'project_name': 'Example Project',\n",
    "    'data_source': 'Internal CRM system',\n",
    "    'original_collection_purpose': 'Customer service and order fulfillment',\n",
    "    'proposed_use': 'Customer segmentation for marketing',\n",
    "    'is_use_aligned_with_original_purpose': True,  # If False, extra review needed\n",
    "    'data_subjects_informed': True,\n",
    "    'contains_sensitive_categories': False,  # e.g., health, religion, ethnicity\n",
    "    'data_quality_verified': True,\n",
    "    'retention_period_days': 90,\n",
    "    'access_restricted_to': ['Analytics team', 'Marketing manager'],\n",
    "    'deletion_plan': 'Automated deletion after retention period',\n",
    "}\n",
    "\n",
    "# Display as a readable table\n",
    "assessment_df = pd.DataFrame([\n",
    "    {'Question': k.replace('_', ' ').title(), 'Answer': str(v)}\n",
    "    for k, v in data_usage_assessment.items()\n",
    "])\n",
    "assessment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000924e9",
   "metadata": {},
   "source": [
    "### Exercise (Responsible Data Usage)\n",
    "\n",
    "Scenario: Your company wants to use customer purchase history (originally collected for order fulfillment) to predict which customers might leave (churn prediction), and then share this with a third-party marketing agency.\n",
    "\n",
    "Answer these questions:\n",
    "1. Is this a secondary use of data? Why or why not?\n",
    "2. What consent or transparency issues might arise?\n",
    "3. What risks come from sharing with a third party?\n",
    "4. Suggest two safeguards you would recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solution space\n",
    "# Write your answers here:\n",
    "\n",
    "exercise_answers = {\n",
    "    '1_secondary_use': \"\"\"\n",
    "    Yes, this is secondary use. The data was collected for order fulfillment,\n",
    "    not for predicting churn or marketing purposes.\n",
    "    \"\"\",\n",
    "    '2_consent_issues': \"\"\"\n",
    "    Customers may not have been informed their purchase data would be used\n",
    "    for predictive modeling or shared externally. This could violate their\n",
    "    expectations and potentially regulations like GDPR.\n",
    "    \"\"\",\n",
    "    '3_third_party_risks': \"\"\"\n",
    "    - Loss of control over how data is used\n",
    "    - Data could be combined with other sources to re-identify individuals\n",
    "    - Third party may have different security standards\n",
    "    - Breach at third party exposes your customers\n",
    "    \"\"\",\n",
    "    '4_safeguards': \"\"\"\n",
    "    1. Aggregate the data before sharing (no individual-level records)\n",
    "    2. Require a data processing agreement with the third party\n",
    "    3. Anonymize customer identifiers\n",
    "    4. Limit data fields to only what's strictly necessary\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for q, a in exercise_answers.items():\n",
    "    print(f\"{q}:\\n{a.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13159f0f",
   "metadata": {},
   "source": [
    "---\n",
    "## 22.6 Ethical decision-making\n",
    "\n",
    "Ethics in data analytics is not just about following rulesâ€”it's about making thoughtful decisions when the rules are unclear or when multiple values conflict.\n",
    "\n",
    "### A simple ethical decision-making framework\n",
    "\n",
    "When facing an ethical dilemma in your analytics work, ask yourself these questions:\n",
    "\n",
    "1. **Stakeholder impact**: Who could be affected by this analysis? (customers, employees, communities, the organization)\n",
    "\n",
    "2. **Harm assessment**: Could this cause harm? What kind? (privacy violation, discrimination, financial loss, reputational damage)\n",
    "\n",
    "3. **Reversibility**: Can the effects be undone if something goes wrong?\n",
    "\n",
    "4. **Transparency test**: Would I be comfortable if this analysis and how it was done were made public?\n",
    "\n",
    "5. **Golden rule**: Would I want this done to me or my family?\n",
    "\n",
    "6. **Legal and policy check**: Does this comply with laws, regulations, and organizational policies?\n",
    "\n",
    "### Common ethical dilemmas in analytics\n",
    "\n",
    "| Dilemma | Considerations |\n",
    "|---------|----------------|\n",
    "| Pressure to show favorable results | Integrity matters more than pleasing stakeholders. Report honestly. |\n",
    "| Using data that might be biased | Acknowledge limitations. Test for fairness. Recommend caution. |\n",
    "| Automating decisions that affect people | Ensure human oversight. Provide appeal mechanisms. |\n",
    "| Discovering sensitive patterns | Just because you *can* find something doesn't mean you *should* use it. |\n",
    "| Tight deadlines vs. proper validation | Communicate risks. Don't ship analyses you haven't validated. |\n",
    "\n",
    "### Warning\n",
    "**\"Everyone does it\"** is not an ethical justification. As a data professional, you have a responsibility to advocate for ethical practices, even when it's inconvenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785aa605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ethics checklist you can use before publishing any analysis\n",
    "# This is a practical tool to pause and reflect\n",
    "\n",
    "ethics_checklist = {\n",
    "    'project_name': '',\n",
    "    'analyst': '',\n",
    "    'date': '',\n",
    "    'checks': [\n",
    "        {'item': 'Data was collected with appropriate consent', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Analysis purpose aligns with original data collection purpose', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Sensitive/protected attributes handled appropriately', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Checked for potential bias or unfair outcomes by group', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Results are explainable to affected stakeholders', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Limitations and uncertainties are documented', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Complies with relevant regulations (GDPR, etc.)', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Reviewed by another team member', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "        {'item': 'Would be comfortable if this were made public', 'status': 'Yes/No/NA', 'notes': ''},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the checklist as a table\n",
    "checklist_df = pd.DataFrame(ethics_checklist['checks'])\n",
    "print(\"Pre-Publication Ethics Checklist\")\n",
    "print(\"=\" * 50)\n",
    "checklist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edecf58",
   "metadata": {},
   "source": [
    "### Case study: The ethical dilemma\n",
    "\n",
    "Read the scenario below and work through the ethical decision-making framework.\n",
    "\n",
    "**Scenario:** You work as a data analyst at a retail company. Your manager asks you to build a model that predicts which employees are likely to quit in the next 6 months. The HR department wants to use this to \"proactively engage\" these employees. You have access to:\n",
    "- Employee performance reviews\n",
    "- Attendance records\n",
    "- Salary information\n",
    "- Personal details (age, marital status, number of children)\n",
    "- Internal chat message sentiment scores\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Who are the stakeholders and how might each be affected?\n",
    "2. What are the potential harms?\n",
    "3. Which data fields raise ethical concerns?\n",
    "4. Would employees expect their chat messages to be analyzed this way?\n",
    "5. What safeguards would you recommend?\n",
    "6. Would you proceed with this project? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Apply the ethical framework to the case study\n",
    "# Fill in your analysis below\n",
    "\n",
    "case_study_analysis = {\n",
    "    'stakeholders': [\n",
    "        'Employees (directly affected by predictions)',\n",
    "        'HR department (will act on predictions)',\n",
    "        'Managers (may treat employees differently)',\n",
    "        'Company (reputation and legal risk)',\n",
    "    ],\n",
    "    'potential_harms': [\n",
    "        'Self-fulfilling prophecy: flagged employees treated differently, then quit',\n",
    "        'Discrimination if model uses protected characteristics',\n",
    "        'Invasion of privacy (chat sentiment analysis)',\n",
    "        'Erosion of trust in the workplace',\n",
    "    ],\n",
    "    'concerning_fields': [\n",
    "        'Age, marital status, children - protected characteristics',\n",
    "        'Chat sentiment - employees likely did not consent to this use',\n",
    "    ],\n",
    "    'recommendations': [\n",
    "        'Exclude protected characteristics and chat data',\n",
    "        'Ensure transparency - inform employees about the program',\n",
    "        'Use for aggregate insights, not individual targeting',\n",
    "        'Involve ethics/legal review before deployment',\n",
    "        'Consider whether problem could be solved without prediction (e.g., exit surveys)',\n",
    "    ],\n",
    "    'proceed_decision': \"\"\"\n",
    "    Proceed with caution and modifications:\n",
    "    - Remove sensitive fields\n",
    "    - Focus on organizational patterns, not individual predictions\n",
    "    - Be transparent with employees\n",
    "    - Or recommend alternative approaches (surveys, focus groups)\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"Case Study Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in case_study_analysis.items():\n",
    "    print(f\"\\n{key.upper().replace('_', ' ')}:\")\n",
    "    if isinstance(value, list):\n",
    "        for item in value:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "    else:\n",
    "        print(f\"  {value.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7c626",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "This chapter covered the ethical, legal, and social responsibilities that come with data analytics work. Here's what you should remember:\n",
    "\n",
    "### ðŸ”’ Data Privacy (Section 22.1)\n",
    "- **Minimize data**: Only collect and use what you need\n",
    "- **Remove identifiers**: Drop names, emails, and other direct identifiers for analysis\n",
    "- **Generalize**: Use age groups instead of exact ages, regions instead of addresses\n",
    "- **Check for uniqueness**: Small groups can be re-identified even without names\n",
    "\n",
    "### âš–ï¸ Regulatory Frameworks (Section 22.2)\n",
    "- Laws like GDPR require lawful basis, transparency, and accountability\n",
    "- Document your analysis purpose, data sources, and retention plans\n",
    "- When in doubt, consult your legal or compliance team\n",
    "\n",
    "### ðŸŽ¯ Bias and Fairness (Section 22.3)\n",
    "- Always check metrics **by group**, not just overall\n",
    "- Understand key fairness concepts: demographic parity, equal opportunity\n",
    "- Different thresholds can shift fairnessâ€”there's no \"neutral\" choice\n",
    "- Fairness is a design decision, not just a technical measurement\n",
    "\n",
    "### ðŸ” Transparency and Explainability (Section 22.4)\n",
    "- Be clear about what you did, what data you used, and limitations\n",
    "- Use techniques like permutation importance to explain model behavior\n",
    "- Remember: explainable â‰  fair\n",
    "\n",
    "### ðŸ“‹ Responsible Data Usage (Section 22.5)\n",
    "- Consider whether data subjects would expect this use\n",
    "- Plan for the full data lifecycle (collection â†’ deletion)\n",
    "- Be cautious with secondary uses and third-party sharing\n",
    "\n",
    "### ðŸ§­ Ethical Decision-Making (Section 22.6)\n",
    "- Use frameworks: stakeholder impact, harm assessment, transparency test\n",
    "- Ethics is about thoughtful decisions, not just rule-following\n",
    "- You have a professional responsibility to advocate for ethical practices\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Final Thought\n",
    "\n",
    "> **Good analytics is not just about being *right*â€”it's about being *responsible*.**\n",
    "\n",
    "As you build your career in data analytics, remember that the data you work with represents real people. The analyses you produce can affect real lives. Take that responsibility seriously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dac13",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Resources\n",
    "\n",
    "### Privacy and Regulations\n",
    "- [GDPR Official Text](https://gdpr-info.eu/) - The full text of the General Data Protection Regulation\n",
    "- [ICO Guide to Data Protection](https://ico.org.uk/for-organisations/guide-to-data-protection/) - UK's Information Commissioner's Office guide\n",
    "- [NIST Privacy Framework](https://www.nist.gov/privacy-framework) - A tool for improving privacy through enterprise risk management\n",
    "\n",
    "### Fairness in Machine Learning\n",
    "- [Google's Responsible AI Practices](https://ai.google/responsibility/responsible-ai-practices/) - Practical guidance on AI fairness\n",
    "- [Fairlearn Documentation](https://fairlearn.org/) - Open-source toolkit for assessing and improving ML fairness\n",
    "- [IBM AI Fairness 360](https://aif360.mybluemix.net/) - Comprehensive toolkit for fairness metrics and algorithms\n",
    "\n",
    "### Explainability\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/) - SHapley Additive exPlanations for model interpretability\n",
    "- [Interpretable Machine Learning Book](https://christophm.github.io/interpretable-ml-book/) - Free online book by Christoph Molnar\n",
    "- [LIME Documentation](https://lime-ml.readthedocs.io/) - Local Interpretable Model-agnostic Explanations\n",
    "\n",
    "### Ethics in Data Science\n",
    "- [Data Science Ethics Resources](https://datascienceethics.com/) - Collection of ethics resources for data professionals\n",
    "- [ACM Code of Ethics](https://www.acm.org/code-of-ethics) - Professional ethics guidelines\n",
    "- [ODI Data Ethics Canvas](https://theodi.org/insights/tools/the-data-ethics-canvas/) - Framework for ethical data projects\n",
    "\n",
    "### Books (Recommended Reading)\n",
    "- *Weapons of Math Destruction* by Cathy O'Neil - How algorithms can increase inequality\n",
    "- *Invisible Women* by Caroline Criado Perez - Data bias and its real-world effects\n",
    "- *The Alignment Problem* by Brian Christian - How AI learns human values (and doesn't)\n",
    "\n",
    "> **Note:** Links are provided for reference. Always verify that resources are current and appropriate for your jurisdiction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
