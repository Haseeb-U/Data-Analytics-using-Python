{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a2bc12",
   "metadata": {},
   "source": [
    "# Chapter 8: Statistical Methods for Data Analytics\n",
    "\n",
    "This chapter gives you the **statistics toolkit** you’ll use constantly as a data analyst: probability, sampling, hypothesis testing, confidence intervals, correlation vs causation, regression, and A/B testing.\n",
    "\n",
    "**Goal:** understand what statistical results mean, when to use each method, and how to avoid common mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2265b20",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Statistics is the backbone of data analytics. While you can collect and visualize data without statistics, you cannot make **reliable conclusions** or **informed decisions** without it.\n",
    "\n",
    "Think of statistics as your toolkit for answering questions like:\n",
    "- \"Is this difference real, or just random noise?\"\n",
    "- \"How confident can I be in this result?\"\n",
    "- \"What's likely to happen next based on past data?\"\n",
    "\n",
    "### What You'll Learn in This Chapter\n",
    "\n",
    "| Section | Topic | Why It Matters |\n",
    "|---------|-------|----------------|\n",
    "| 8.1 | Role of Statistics | Understand why statistics is essential |\n",
    "| 8.2 | Probability & Distributions | Model uncertainty and randomness |\n",
    "| 8.3 | Sampling Techniques | Collect data that represents reality |\n",
    "| 8.4 | Hypothesis Testing | Make data-driven decisions |\n",
    "| 8.5 | Parametric vs Non-parametric | Choose the right test for your data |\n",
    "| 8.6 | Confidence Intervals | Quantify uncertainty in estimates |\n",
    "| 8.7 | Correlation vs Causation | Avoid misleading conclusions |\n",
    "| 8.8 | Regression Analysis | Model relationships between variables |\n",
    "| 8.9 | A/B Testing | Run controlled experiments |\n",
    "| 8.10 | Assumptions & Limitations | Know when statistics can fail |\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python (Chapter 2)\n",
    "- NumPy and Pandas basics (Chapters 3-4)\n",
    "- Data visualization (Chapter 5)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953a6b8",
   "metadata": {},
   "source": [
    "## 8.0 Setup (Imports + Reproducibility)\n",
    "\n",
    "We’ll use standard data analytics libraries. If a library is missing, install it (in a terminal) with: `pip install scipy statsmodels seaborn`.\n",
    "\n",
    "We also set a random seed so examples are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc56ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "def t_confidence_interval(sample, confidence=0.95):\n",
    "    \"\"\"Return a t-based confidence interval for the mean.\n",
    "\n",
    "    Uses SciPy's newer API when available, otherwise falls back to the older one.\n",
    "    \"\"\"\n",
    "    sample = np.asarray(sample)\n",
    "    mean = sample.mean()\n",
    "    sem = stats.sem(sample)\n",
    "    df = len(sample) - 1\n",
    "    try:\n",
    "        # SciPy >= 1.11 uses 'confidence='\n",
    "        return stats.t.interval(confidence=confidence, df=df, loc=mean, scale=sem)\n",
    "    except TypeError:\n",
    "        # Older SciPy uses 'alpha='\n",
    "        return stats.t.interval(alpha=confidence, df=df, loc=mean, scale=sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68040490",
   "metadata": {},
   "source": [
    "## 8.1 Role of Statistics in Analytics\n",
    "\n",
    "Statistics helps you move from **observations** (what happened in your data) to **inference** (what is likely true in the real world).\n",
    "\n",
    "### Two big ideas\n",
    "- **Descriptive statistics:** summarize what you observed (mean, median, charts).\n",
    "- **Inferential statistics:** make a careful guess about a larger population using a sample (confidence intervals, hypothesis tests).\n",
    "\n",
    "> **Common mistake:** treating a sample result as a guaranteed truth about the population. Statistics always includes uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab56707",
   "metadata": {},
   "source": [
    "## 8.2 Probability Concepts and Distributions\n",
    "\n",
    "### Probability (the intuition)\n",
    "Probability is a number from 0 to 1 that represents how likely an event is.\n",
    "\n",
    "Key terms:\n",
    "- **Experiment:** a process that produces an outcome (e.g., a customer visit).\n",
    "- **Outcome:** one result (e.g., purchase vs no purchase).\n",
    "- **Event:** a set of outcomes (e.g., purchase).\n",
    "\n",
    "### Random variables\n",
    "A **random variable** turns outcomes into numbers.\n",
    "- **Discrete:** counts (number of purchases)\n",
    "- **Continuous:** measurements (time on page)\n",
    "\n",
    "### Probability distributions\n",
    "A distribution describes how likely different values are.\n",
    "- **PMF** (discrete) or **PDF** (continuous)\n",
    "- **CDF:** probability a value is *≤ x*\n",
    "\n",
    "> **Tip:** In analytics, you rarely need to memorize formulas. Focus on what the distribution *models* and when it’s a reasonable approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete distribution example: Binomial (purchases out of n visitors)\n",
    "n = 50      # trials (visitors)\n",
    "p = 0.10    # probability of purchase\n",
    "\n",
    "k = np.arange(0, n + 1)\n",
    "pmf = stats.binom.pmf(k, n=n, p=p)\n",
    "\n",
    "plt.bar(k, pmf)\n",
    "plt.title('Binomial: Purchases out of 50 visitors (p=0.10)')\n",
    "plt.xlabel('Number of purchases')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f796036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous distribution example: Standard normal\n",
    "x = np.linspace(-4, 4, 400)\n",
    "pdf = stats.norm.pdf(x, loc=0, scale=1)\n",
    "\n",
    "plt.plot(x, pdf)\n",
    "plt.title('Standard Normal Distribution (mean=0, std=1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc0916",
   "metadata": {},
   "source": [
    "### Common distributions in analytics\n",
    "- **Bernoulli:** one yes/no trial (purchase or not).\n",
    "- **Binomial:** number of successes in *n* Bernoulli trials.\n",
    "- **Poisson:** counts in a fixed period/space (e.g., tickets per hour).\n",
    "- **Normal:** many natural measurements; often appears via the Central Limit Theorem.\n",
    "- **Exponential:** time between events (e.g., time between arrivals).\n",
    "\n",
    "> **Warning:** Not everything is normal. Always look at the data (histogram/box plot) before assuming a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson example: number of support tickets per hour\n",
    "lam = 4  # average tickets per hour\n",
    "k = np.arange(0, 16)\n",
    "pmf = stats.poisson.pmf(k, mu=lam)\n",
    "\n",
    "plt.stem(k, pmf)\n",
    "plt.title('Poisson: Tickets per hour (lambda=4)')\n",
    "plt.xlabel('Tickets per hour')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c550173",
   "metadata": {},
   "source": [
    "## 8.3 Sampling Techniques\n",
    "\n",
    "In real analytics, you rarely measure the whole population. You sample.\n",
    "\n",
    "### Population vs sample\n",
    "- **Population:** everyone/everything you care about (all customers).\n",
    "- **Sample:** the subset you observed.\n",
    "\n",
    "### Common sampling methods\n",
    "- **Simple random sampling:** everyone has equal chance.\n",
    "- **Stratified sampling:** sample within groups (e.g., regions) to ensure representation.\n",
    "- **Cluster sampling:** sample groups (clusters) then include all/part within clusters.\n",
    "- **Systematic sampling:** take every k-th item (be careful: can introduce patterns).\n",
    "\n",
    "> **Common mistake:** sampling only what’s convenient (convenience sampling). This can produce biased conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7feaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic population dataset\n",
    "N = 5000\n",
    "population = pd.DataFrame({\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], size=N, p=[0.25, 0.35, 0.20, 0.20]),\n",
    "    'spend': np.random.gamma(shape=2.0, scale=30.0, size=N)  # positive, skewed\n",
    "})\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6714b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple random sample\n",
    "sample_srs = population.sample(n=300, random_state=42)\n",
    "\n",
    "# Stratified sample: equal n from each region\n",
    "sample_strat = (\n",
    "    population.groupby('region', group_keys=False)\n",
    "    .apply(lambda g: g.sample(n=75, random_state=42))\n",
    ")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'population_mean_spend': [population['spend'].mean()],\n",
    "    'srs_mean_spend': [sample_srs['spend'].mean()],\n",
    "    'strat_mean_spend': [sample_strat['spend'].mean()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ce589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare region proportions in population vs samples\n",
    "compare = pd.DataFrame({\n",
    "    'population': population['region'].value_counts(normalize=True),\n",
    "    'srs': sample_srs['region'].value_counts(normalize=True),\n",
    "    'strat': sample_strat['region'].value_counts(normalize=True),\n",
    "}).fillna(0)\n",
    "\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121d9bd",
   "metadata": {},
   "source": [
    "### Why sampling method matters\n",
    "If some groups are under-represented, your estimates (like mean spend) can shift.\n",
    "\n",
    "> **Tip:** Use stratified sampling when you *know* some groups matter and you want stable comparisons across them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e8775",
   "metadata": {},
   "source": [
    "## Exercise 8.2 (Sampling)\n",
    "\n",
    "Using the `population` DataFrame created above:\n",
    "\n",
    "1. Take a **systematic sample** of 300 records (every k-th record)\n",
    "2. Calculate the mean spend from your systematic sample\n",
    "3. Compare it to the population mean\n",
    "4. Discuss: What could go wrong with systematic sampling if the data has a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de385e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Systematic sampling - take every k-th record\n",
    "k = len(population) // 300  # Calculate step size\n",
    "systematic_sample = population.iloc[::k].head(300)  # Take every k-th row\n",
    "\n",
    "# Step 2: Calculate mean spend\n",
    "systematic_mean = systematic_sample['spend'].mean()\n",
    "\n",
    "# Step 3: Compare to population mean\n",
    "population_mean = population['spend'].mean()\n",
    "\n",
    "print(f\"Population mean spend: ${population_mean:.2f}\")\n",
    "print(f\"Systematic sample mean spend: ${systematic_mean:.2f}\")\n",
    "print(f\"Difference: ${abs(systematic_mean - population_mean):.2f}\")\n",
    "\n",
    "# Discussion: Systematic sampling works well when data is randomly ordered.\n",
    "# However, if there's a pattern (e.g., data sorted by region or time), \n",
    "# the sample might over-represent or under-represent certain groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722e69b",
   "metadata": {},
   "source": [
    "## 8.4 Hypothesis Testing Framework\n",
    "\n",
    "A hypothesis test is a structured way to decide whether the data is inconsistent with a default assumption.\n",
    "\n",
    "### The core steps\n",
    "1. State hypotheses\n",
    "   - **Null ($H_0$):** ‘no effect’ / ‘no difference’\n",
    "   - **Alternative ($H_1$):** the effect/difference you suspect\n",
    "2. Choose a test statistic\n",
    "3. Compute a p-value: how surprising your data would be if $H_0$ were true\n",
    "4. Compare p-value to alpha (common: 0.05)\n",
    "\n",
    "### Important interpretation\n",
    "- Small p-value ≠ proof that $H_1$ is true.\n",
    "- Large p-value ≠ proof that $H_0$ is true.\n",
    "\n",
    "> **Warning:** Statistical significance is not the same as practical significance. Always look at effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: does a new subject line increase click-through rate (CTR)?\n",
    "n_control = 1000\n",
    "n_variant = 1000\n",
    "\n",
    "p_control = 0.08\n",
    "p_variant = 0.095\n",
    "\n",
    "control_clicks = np.random.binomial(1, p_control, size=n_control)\n",
    "variant_clicks = np.random.binomial(1, p_variant, size=n_variant)\n",
    "\n",
    "ctr_control = control_clicks.mean()\n",
    "ctr_variant = variant_clicks.mean()\n",
    "ctr_control, ctr_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "count = np.array([control_clicks.sum(), variant_clicks.sum()])\n",
    "nobs = np.array([n_control, n_variant])\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'group': ['control', 'variant'],\n",
    "    'n': [n_control, n_variant],\n",
    "    'clicks': [int(count[0]), int(count[1])],\n",
    "    'ctr': [ctr_control, ctr_variant],\n",
    "})\n",
    "\n",
    "summary, z_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02341a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff = ctr_variant - ctr_control\n",
    "rel_lift = abs_diff / ctr_control if ctr_control > 0 else np.nan\n",
    "abs_diff, rel_lift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c12f4",
   "metadata": {},
   "source": [
    "## Exercise 8.1 (Hypothesis testing)\n",
    "\n",
    "Simulate another experiment where:\n",
    "- control CTR = 0.12\n",
    "- variant CTR = 0.125\n",
    "- sample size = 500 per group\n",
    "\n",
    "Tasks:\n",
    "1. Compute both CTRs\n",
    "2. Run `proportions_ztest`\n",
    "3. Report p-value and absolute difference\n",
    "4. Write one sentence: significant at 0.05? practically meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aaf752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "n = 500\n",
    "p_c = 0.12\n",
    "p_v = 0.125\n",
    "\n",
    "control = np.random.binomial(1, p_c, size=n)\n",
    "variant = np.random.binomial(1, p_v, size=n)\n",
    "\n",
    "ctr_c = control.mean()\n",
    "ctr_v = variant.mean()\n",
    "\n",
    "count = np.array([control.sum(), variant.sum()])\n",
    "nobs = np.array([n, n])\n",
    "z, p = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "\n",
    "abs_diff = ctr_v - ctr_c\n",
    "ctr_c, ctr_v, z, p, abs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfbf67",
   "metadata": {},
   "source": [
    "## 8.5 Parametric and Non-parametric Tests\n",
    "\n",
    "### Parametric tests\n",
    "Parametric tests assume a particular distribution (often normal) and/or assumptions about variance.\n",
    "- Example: **t-test** compares means\n",
    "\n",
    "### Non-parametric tests\n",
    "Non-parametric tests make fewer distribution assumptions (often use ranks).\n",
    "- Example: **Mann–Whitney U** compares central tendency without assuming normality\n",
    "\n",
    "When to consider non-parametric tests:\n",
    "- data is heavily skewed\n",
    "- many outliers\n",
    "- small sample sizes\n",
    "\n",
    "> **Tip:** Non-parametric does not mean ‘assumption-free’. It means ‘fewer/simpler assumptions’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate skewed spending data for two customer segments\n",
    "n = 200\n",
    "spend_A = np.random.gamma(shape=2.0, scale=25.0, size=n)\n",
    "spend_B = np.random.gamma(shape=2.2, scale=25.0, size=n)\n",
    "\n",
    "df_spend = pd.DataFrame({\n",
    "    'segment': ['A'] * n + ['B'] * n,\n",
    "    'spend': np.concatenate([spend_A, spend_B]),\n",
    "})\n",
    "\n",
    "sns.boxplot(data=df_spend, x='segment', y='spend')\n",
    "plt.title('Spending by Segment (skewed data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric test: Welch's t-test (does not assume equal variances)\n",
    "t_stat, p_t = stats.ttest_ind(spend_A, spend_B, equal_var=False)\n",
    "\n",
    "# Non-parametric test: Mann-Whitney U (two-sided)\n",
    "u_stat, p_u = stats.mannwhitneyu(spend_A, spend_B, alternative='two-sided', method='auto')\n",
    "\n",
    "t_stat, p_t, u_stat, p_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc319767",
   "metadata": {},
   "source": [
    "## 8.6 Confidence Intervals\n",
    "\n",
    "A **confidence interval (CI)** gives a range of plausible values for an unknown population parameter (like the true mean).\n",
    "\n",
    "### How to interpret a 95% CI\n",
    "If we repeated the same sampling method many times, **about 95% of the constructed intervals would contain the true value**.\n",
    "\n",
    "> **Warning:** It does *not* mean ‘there is a 95% probability the true value is in this specific interval’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d657ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.normal(loc=50, scale=10, size=30)\n",
    "mean = sample.mean()\n",
    "ci_low, ci_high = t_confidence_interval(sample, confidence=0.95)\n",
    "mean, (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample, bins=12, kde=True)\n",
    "plt.axvline(mean, color='black', linestyle='--', label=f'Mean = {mean:.2f}')\n",
    "plt.axvspan(ci_low, ci_high, alpha=0.2, label='95% CI for mean')\n",
    "plt.title('Sample Distribution + 95% CI for the Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22281df",
   "metadata": {},
   "source": [
    "### Bootstrap confidence interval (practical and flexible)\n",
    "Bootstrapping uses repeated resampling (with replacement) to approximate the sampling distribution. This is useful when the data is skewed or the statistic is complex.\n",
    "\n",
    "We’ll bootstrap the mean spend from a skewed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_sample = np.random.gamma(shape=2.0, scale=30.0, size=200)\n",
    "\n",
    "B = 3000\n",
    "boot_means = np.array([np.random.choice(skewed_sample, size=len(skewed_sample), replace=True).mean() for _ in range(B)])\n",
    "\n",
    "ci_low, ci_high = np.percentile(boot_means, [2.5, 97.5])\n",
    "skewed_sample.mean(), (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9584f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(boot_means, bins=40, kde=True)\n",
    "plt.axvline(ci_low, color='red', linestyle='--', label='2.5%')\n",
    "plt.axvline(ci_high, color='red', linestyle='--', label='97.5%')\n",
    "plt.title('Bootstrap Sampling Distribution of the Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29997a1b",
   "metadata": {},
   "source": [
    "## Exercise 8.3 (Confidence Intervals)\n",
    "\n",
    "A company wants to estimate the average time customers spend on their website.\n",
    "\n",
    "Tasks:\n",
    "1. Generate a random sample of 50 session durations (use `np.random.exponential(scale=5, size=50)` for realistic right-skewed data)\n",
    "2. Calculate a 95% t-based confidence interval using the helper function\n",
    "3. Calculate a 95% bootstrap confidence interval (use 2000 iterations)\n",
    "4. Compare the two intervals - which one is wider? Why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Generate session duration data (exponential is right-skewed)\n",
    "session_times = np.random.exponential(scale=5, size=50)\n",
    "\n",
    "print(f\"Sample mean: {session_times.mean():.2f} minutes\")\n",
    "print(f\"Sample median: {np.median(session_times):.2f} minutes\")\n",
    "\n",
    "# Step 2: T-based confidence interval\n",
    "t_ci = t_confidence_interval(session_times, confidence=0.95)\n",
    "print(f\"\\n95% T-based CI: ({t_ci[0]:.2f}, {t_ci[1]:.2f})\")\n",
    "print(f\"T-based CI width: {t_ci[1] - t_ci[0]:.2f}\")\n",
    "\n",
    "# Step 3: Bootstrap confidence interval\n",
    "B = 2000\n",
    "boot_means = np.array([\n",
    "    np.random.choice(session_times, size=len(session_times), replace=True).mean() \n",
    "    for _ in range(B)\n",
    "])\n",
    "boot_ci = np.percentile(boot_means, [2.5, 97.5])\n",
    "print(f\"\\n95% Bootstrap CI: ({boot_ci[0]:.2f}, {boot_ci[1]:.2f})\")\n",
    "print(f\"Bootstrap CI width: {boot_ci[1] - boot_ci[0]:.2f}\")\n",
    "\n",
    "# Step 4: Comparison\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"For skewed data, bootstrap CI may be more accurate because it doesn't\")\n",
    "print(\"assume normality. The t-based CI assumes the sampling distribution of\")\n",
    "print(\"the mean is approximately normal, which may not hold for small, skewed samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845fa40",
   "metadata": {},
   "source": [
    "## 8.7 Correlation and Causation\n",
    "\n",
    "### Correlation\n",
    "Correlation measures how strongly two variables move together.\n",
    "- **Pearson correlation:** linear relationship\n",
    "- **Spearman correlation:** rank-based, more robust to outliers\n",
    "\n",
    "### Causation\n",
    "Causation means changing X *causes* a change in Y. Correlation alone does not prove causation.\n",
    "\n",
    "Why correlation can be misleading:\n",
    "- **Confounders:** a third variable affects both X and Y\n",
    "- **Reverse causality:** Y causes X\n",
    "- **Coincidence:** especially with small samples\n",
    "\n",
    "> **Tip:** Strong evidence for causation often comes from randomized experiments (like A/B tests), not observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confounder example: temperature influences two variables\n",
    "n = 800\n",
    "temperature = np.random.normal(loc=25, scale=5, size=n)\n",
    "ice_cream_sales = 50 + 5 * temperature + np.random.normal(0, 20, size=n)\n",
    "drownings = 2 + 0.3 * temperature + np.random.normal(0, 2, size=n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'temperature': temperature,\n",
    "    'ice_cream_sales': ice_cream_sales,\n",
    "    'drownings': drownings,\n",
    "})\n",
    "\n",
    "df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ab030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='ice_cream_sales', y='drownings', alpha=0.6)\n",
    "plt.title('Correlation example: Ice cream sales vs drownings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860929f",
   "metadata": {},
   "source": [
    "## Exercise 8.4 (Correlation vs Causation)\n",
    "\n",
    "Analyze the relationship between study hours and exam scores:\n",
    "\n",
    "1. Generate synthetic data where study hours genuinely affects exam scores\n",
    "2. Calculate both Pearson and Spearman correlations\n",
    "3. Create a scatter plot with correlation values displayed\n",
    "4. Think: Can you conclude that studying *causes* better scores from this data alone? What other factors might be involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695353c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Generate data where study hours affects scores\n",
    "n = 100\n",
    "study_hours = np.random.uniform(1, 10, size=n)\n",
    "# Scores increase with study hours, but with some noise\n",
    "exam_scores = 40 + 5 * study_hours + np.random.normal(0, 8, size=n)\n",
    "exam_scores = np.clip(exam_scores, 0, 100)  # Keep scores in valid range\n",
    "\n",
    "df_study = pd.DataFrame({'study_hours': study_hours, 'exam_scores': exam_scores})\n",
    "\n",
    "# Step 2: Calculate correlations\n",
    "pearson_r = df_study['study_hours'].corr(df_study['exam_scores'], method='pearson')\n",
    "spearman_r = df_study['study_hours'].corr(df_study['exam_scores'], method='spearman')\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_r:.3f}\")\n",
    "print(f\"Spearman correlation: {spearman_r:.3f}\")\n",
    "\n",
    "# Step 3: Scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=df_study, x='study_hours', y='exam_scores', alpha=0.7)\n",
    "plt.title(f'Study Hours vs Exam Scores\\nPearson r = {pearson_r:.3f}, Spearman ρ = {spearman_r:.3f}')\n",
    "plt.xlabel('Study Hours')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Discussion\n",
    "print(\"\\n--- Causation Discussion ---\")\n",
    "print(\"Even with strong correlation, we cannot definitively conclude causation because:\")\n",
    "print(\"• Confounders: Students who study more might also sleep better, attend class more, etc.\")\n",
    "print(\"• Self-selection: Motivated students both study more AND perform better\")\n",
    "print(\"• To establish causation, we'd need a randomized experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd28d47",
   "metadata": {},
   "source": [
    "## 8.8 Simple and Multiple Regression\n",
    "\n",
    "Regression models how an outcome changes with one or more predictors.\n",
    "\n",
    "### Simple linear regression\n",
    "One predictor: $y = b_0 + b_1 x + \"noise\"$\n",
    "\n",
    "### Multiple regression\n",
    "Multiple predictors can ‘control for’ confounders and improve predictions.\n",
    "\n",
    "What to look at:\n",
    "- **Coefficients:** direction and size (within the model)\n",
    "- **p-values:** evidence against coefficient being 0 (interpret carefully)\n",
    "- **$R^2$:** variance explained (higher is not always better)\n",
    "\n",
    "> **Warning:** Regression often shows association, not automatic causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: sales depends on ad_spend and price\n",
    "n = 300\n",
    "ad_spend = np.random.uniform(0, 100, size=n)\n",
    "price = np.random.uniform(10, 30, size=n)\n",
    "sales = 200 + 3.5 * ad_spend - 5.0 * price + np.random.normal(0, 30, size=n)\n",
    "\n",
    "df_reg = pd.DataFrame({'sales': sales, 'ad_spend': ad_spend, 'price': price})\n",
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c77c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_simple = smf.ols('sales ~ ad_spend', data=df_reg).fit()\n",
    "m_simple.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_multi = smf.ols('sales ~ ad_spend + price', data=df_reg).fit()\n",
    "m_multi.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc135cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df_reg, x='ad_spend', y='sales', scatter_kws={'alpha': 0.5})\n",
    "plt.title('Sales vs Ad Spend (simple regression line)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82be580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_reg.copy()\n",
    "df_reg['residuals'] = m_multi.resid\n",
    "df_reg['fitted'] = m_multi.fittedvalues\n",
    "\n",
    "sns.scatterplot(data=df_reg, x='fitted', y='residuals', alpha=0.6)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Residuals vs Fitted (look for random scatter around 0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17e34c",
   "metadata": {},
   "source": [
    "## Exercise 8.5 (Regression)\n",
    "\n",
    "A retail company wants to predict monthly sales based on marketing spend and store size.\n",
    "\n",
    "Tasks:\n",
    "1. Create a synthetic dataset with 200 stores including: `marketing_spend` (0-50), `store_size` (500-5000 sq ft), and `monthly_sales`\n",
    "2. Build a simple regression model with only `marketing_spend`\n",
    "3. Build a multiple regression model with both predictors\n",
    "4. Compare the R² values - how much does adding `store_size` improve the model?\n",
    "5. Interpret the coefficients: what does each one mean in plain English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ad80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create synthetic retail data\n",
    "n = 200\n",
    "marketing_spend = np.random.uniform(0, 50, size=n)\n",
    "store_size = np.random.uniform(500, 5000, size=n)\n",
    "\n",
    "# Sales depends on both, with some noise\n",
    "monthly_sales = (10000 + \n",
    "                 200 * marketing_spend + \n",
    "                 5 * store_size + \n",
    "                 np.random.normal(0, 3000, size=n))\n",
    "\n",
    "df_retail = pd.DataFrame({\n",
    "    'marketing_spend': marketing_spend,\n",
    "    'store_size': store_size,\n",
    "    'monthly_sales': monthly_sales\n",
    "})\n",
    "\n",
    "print(\"Data preview:\")\n",
    "print(df_retail.head())\n",
    "\n",
    "# Step 2: Simple regression (marketing only)\n",
    "model_simple = smf.ols('monthly_sales ~ marketing_spend', data=df_retail).fit()\n",
    "print(f\"\\n--- Simple Model (marketing only) ---\")\n",
    "print(f\"R² = {model_simple.rsquared:.4f}\")\n",
    "\n",
    "# Step 3: Multiple regression (both predictors)\n",
    "model_multi = smf.ols('monthly_sales ~ marketing_spend + store_size', data=df_retail).fit()\n",
    "print(f\"\\n--- Multiple Model (marketing + store size) ---\")\n",
    "print(f\"R² = {model_multi.rsquared:.4f}\")\n",
    "\n",
    "# Step 4: Compare R²\n",
    "r2_improvement = model_multi.rsquared - model_simple.rsquared\n",
    "print(f\"\\nR² improvement by adding store_size: {r2_improvement:.4f} ({r2_improvement*100:.1f}% more variance explained)\")\n",
    "\n",
    "# Step 5: Interpret coefficients\n",
    "print(\"\\n--- Coefficient Interpretation ---\")\n",
    "print(model_multi.summary().tables[1])\n",
    "print(\"\\nIn plain English:\")\n",
    "print(f\"• For every $1 increase in marketing spend, sales increase by ~${model_multi.params['marketing_spend']:.0f}\")\n",
    "print(f\"• For every 1 sq ft increase in store size, sales increase by ~${model_multi.params['store_size']:.0f}\")\n",
    "print(f\"• Base sales (no marketing, zero size) would be ~${model_multi.params['Intercept']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054499d",
   "metadata": {},
   "source": [
    "## 8.9 A/B Testing Methodology\n",
    "\n",
    "A/B testing is a controlled experiment where users are randomly assigned to:\n",
    "- **A (control):** current version\n",
    "- **B (variant):** new change\n",
    "\n",
    "### Typical workflow\n",
    "1. Define the goal metric (CTR, conversion, revenue per user)\n",
    "2. Define hypotheses and alpha\n",
    "3. Randomize and run the experiment\n",
    "4. Analyze results (p-value + effect size + confidence interval)\n",
    "5. Decide considering business impact and risks\n",
    "\n",
    "### Common pitfalls\n",
    "- Stopping early when p-value looks good (peeking)\n",
    "- Running too many metrics and picking the best one\n",
    "- Not checking sample ratio mismatch\n",
    "- Ignoring practical impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project: analyze an A/B test end-to-end (simulated)\n",
    "nA = 3000\n",
    "nB = 3000\n",
    "pA = 0.060\n",
    "pB = 0.066  # small lift\n",
    "\n",
    "A = np.random.binomial(1, pA, size=nA)\n",
    "B = np.random.binomial(1, pB, size=nB)\n",
    "\n",
    "ctrA = A.mean()\n",
    "ctrB = B.mean()\n",
    "diff = ctrB - ctrA\n",
    "\n",
    "count = np.array([A.sum(), B.sum()])\n",
    "nobs = np.array([nA, nB])\n",
    "z, p = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'group': ['A', 'B'],\n",
    "    'n': [nA, nB],\n",
    "    'clicks': [int(count[0]), int(count[1])],\n",
    "    'ctr': [ctrA, ctrB],\n",
    "})\n",
    "\n",
    "results, diff, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43beb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CI for the CTR difference (B - A)\n",
    "B_iter = 2000\n",
    "boot_diffs = np.array([\n",
    "    np.random.choice(B, size=len(B), replace=True).mean()\n",
    "    - np.random.choice(A, size=len(A), replace=True).mean()\n",
    "    for _ in range(B_iter)\n",
    "])\n",
    "\n",
    "ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n",
    "diff, (ci_low, ci_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(boot_diffs, bins=40, kde=True)\n",
    "plt.axvline(ci_low, color='red', linestyle='--', label='2.5%')\n",
    "plt.axvline(ci_high, color='red', linestyle='--', label='97.5%')\n",
    "plt.axvline(diff, color='black', linestyle='--', label='Observed diff')\n",
    "plt.title('Bootstrap CI for CTR Difference (B - A)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0ae06",
   "metadata": {},
   "source": [
    "## Exercise 8.6 (A/B Testing - Mini Project)\n",
    "\n",
    "You're a data analyst at an e-commerce company. The product team wants to test whether a new checkout button color (green instead of blue) increases the conversion rate.\n",
    "\n",
    "**Scenario:**\n",
    "- Control (blue button): 5,000 visitors, 3.2% conversion rate\n",
    "- Variant (green button): 5,000 visitors, 3.6% conversion rate\n",
    "\n",
    "**Tasks:**\n",
    "1. Simulate the experiment data\n",
    "2. Perform a two-proportion z-test\n",
    "3. Calculate the absolute and relative lift\n",
    "4. Compute a 95% bootstrap confidence interval for the difference\n",
    "5. Write a short recommendation: Should the company adopt the green button? Consider both statistical significance and practical impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77165e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - A/B Testing Mini Project\n",
    "\n",
    "# Step 1: Simulate experiment data\n",
    "np.random.seed(123)  # For reproducibility\n",
    "n_control = 5000\n",
    "n_variant = 5000\n",
    "p_control = 0.032  # 3.2% conversion\n",
    "p_variant = 0.036  # 3.6% conversion\n",
    "\n",
    "control_conversions = np.random.binomial(1, p_control, size=n_control)\n",
    "variant_conversions = np.random.binomial(1, p_variant, size=n_variant)\n",
    "\n",
    "observed_control = control_conversions.mean()\n",
    "observed_variant = variant_conversions.mean()\n",
    "\n",
    "print(\"=== A/B Test Results ===\")\n",
    "print(f\"\\nControl (Blue): {observed_control*100:.2f}% conversion ({control_conversions.sum()} / {n_control})\")\n",
    "print(f\"Variant (Green): {observed_variant*100:.2f}% conversion ({variant_conversions.sum()} / {n_variant})\")\n",
    "\n",
    "# Step 2: Two-proportion z-test\n",
    "count = np.array([control_conversions.sum(), variant_conversions.sum()])\n",
    "nobs = np.array([n_control, n_variant])\n",
    "z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "\n",
    "print(f\"\\n--- Statistical Test ---\")\n",
    "print(f\"Z-statistic: {z_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05? {'Yes ✓' if p_value < 0.05 else 'No ✗'}\")\n",
    "\n",
    "# Step 3: Calculate lift\n",
    "abs_lift = observed_variant - observed_control\n",
    "rel_lift = abs_lift / observed_control if observed_control > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Effect Size ---\")\n",
    "print(f\"Absolute lift: {abs_lift*100:.2f} percentage points\")\n",
    "print(f\"Relative lift: {rel_lift*100:.1f}%\")\n",
    "\n",
    "# Step 4: Bootstrap CI for difference\n",
    "B = 2000\n",
    "boot_diffs = np.array([\n",
    "    np.random.choice(variant_conversions, size=n_variant, replace=True).mean() -\n",
    "    np.random.choice(control_conversions, size=n_control, replace=True).mean()\n",
    "    for _ in range(B)\n",
    "])\n",
    "ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\n--- 95% Bootstrap CI for Difference ---\")\n",
    "print(f\"CI: ({ci_low*100:.2f}%, {ci_high*100:.2f}%)\")\n",
    "print(f\"Does CI include 0? {'Yes (uncertain)' if ci_low <= 0 <= ci_high else 'No (significant)'}\")\n",
    "\n",
    "# Step 5: Recommendation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*50)\n",
    "if p_value < 0.05 and ci_low > 0:\n",
    "    print(\"✓ The green button shows a statistically significant improvement.\")\n",
    "    print(f\"  Expected additional conversions per 10,000 visitors: ~{abs_lift * 10000:.0f}\")\n",
    "    print(\"  Recommendation: ADOPT the green button.\")\n",
    "else:\n",
    "    print(\"✗ Results are not statistically significant at α=0.05.\")\n",
    "    print(\"  Recommendation: Consider running the test longer or with more traffic.\")\n",
    "    print(\"  The observed difference could be due to random chance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d6465",
   "metadata": {},
   "source": [
    "## 8.10 Statistical Assumptions and Limitations\n",
    "\n",
    "### Common assumptions to check\n",
    "- Random sampling / random assignment\n",
    "- Independence\n",
    "- Distribution assumptions (for parametric tests)\n",
    "- Sufficient sample size\n",
    "\n",
    "### Common limitations\n",
    "- Multiple comparisons (false positives)\n",
    "- Selection bias\n",
    "- Measurement error\n",
    "- Simpson’s paradox\n",
    "\n",
    "> **Tip:** Always pair statistics with domain knowledge, good data collection, and clear metric definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b4e9c",
   "metadata": {},
   "source": [
    "## 8.10 Statistical Assumptions and Limitations\n",
    "\n",
    "Understanding when statistical methods work (and when they fail) is as important as knowing how to use them.\n",
    "\n",
    "### Common assumptions to check\n",
    "\n",
    "| Test/Method | Key Assumptions |\n",
    "|-------------|-----------------|\n",
    "| T-test | Data is roughly normal (or large sample), independent observations |\n",
    "| Chi-square test | Expected frequencies ≥ 5, independent observations |\n",
    "| Linear regression | Linear relationship, normal residuals, homoscedasticity, independent errors |\n",
    "| Correlation | Linear relationship (for Pearson), no extreme outliers |\n",
    "| A/B test | Random assignment, independent users, no interference between groups |\n",
    "\n",
    "### How to check assumptions\n",
    "\n",
    "1. **Normality:** Histogram, Q-Q plot, Shapiro-Wilk test\n",
    "2. **Independence:** Study design (was randomization proper?)\n",
    "3. **Homoscedasticity:** Residual plot (look for constant spread)\n",
    "4. **Linearity:** Scatter plot, residual plot\n",
    "\n",
    "### Common limitations and pitfalls\n",
    "\n",
    "| Problem | What Goes Wrong | How to Avoid |\n",
    "|---------|-----------------|--------------|\n",
    "| Multiple comparisons | Running many tests inflates false positives | Use Bonferroni correction or control FDR |\n",
    "| Selection bias | Sample doesn't represent population | Use proper sampling methods |\n",
    "| Survivorship bias | Only seeing \"survivors\" | Think about what's missing from data |\n",
    "| Simpson's paradox | Aggregate trend reverses in subgroups | Always check grouped data |\n",
    "| P-hacking | Trying analyses until p < 0.05 | Pre-register hypotheses, be honest |\n",
    "| Overfitting | Model fits noise, not signal | Use holdout validation |\n",
    "\n",
    "> **Tip:** Always pair statistics with domain knowledge, good data collection, and clear metric definitions. Statistics is a tool, not a replacement for thinking.\n",
    "\n",
    "> **Warning:** A statistically significant result is not automatically a correct or important result. Context matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e767df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking normality assumption\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Generate two datasets: one normal, one skewed\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=100)\n",
    "skewed_data = np.random.exponential(scale=10, size=100)\n",
    "\n",
    "# Shapiro-Wilk test (null hypothesis: data is normally distributed)\n",
    "stat_normal, p_normal = shapiro(normal_data)\n",
    "stat_skewed, p_skewed = shapiro(skewed_data)\n",
    "\n",
    "print(\"Shapiro-Wilk Normality Test\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Normal data: W={stat_normal:.4f}, p={p_normal:.4f}\")\n",
    "print(f\"  → {'Likely normal' if p_normal > 0.05 else 'Not normal'}\")\n",
    "print(f\"\\nSkewed data: W={stat_skewed:.4f}, p={p_skewed:.4f}\")\n",
    "print(f\"  → {'Likely normal' if p_skewed > 0.05 else 'Not normal'}\")\n",
    "\n",
    "# Visual check with histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(normal_data, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'Normal Data\\n(Shapiro p={p_normal:.3f})')\n",
    "axes[0].set_xlabel('Value')\n",
    "\n",
    "axes[1].hist(skewed_data, bins=15, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title(f'Skewed Data\\n(Shapiro p={p_skewed:.3f})')\n",
    "axes[1].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e044c",
   "metadata": {},
   "source": [
    "## Exercise 8.7 (Chapter Review - Comprehensive Mini Project)\n",
    "\n",
    "**Scenario:** You're analyzing customer data for a subscription service.\n",
    "\n",
    "Given the following synthetic dataset, complete the analysis:\n",
    "\n",
    "1. **Descriptive Stats:** Calculate mean, median, and std of `monthly_spend`\n",
    "2. **Confidence Interval:** Calculate a 95% CI for mean monthly spend\n",
    "3. **Hypothesis Test:** Test if premium users spend more than basic users (use t-test)\n",
    "4. **Correlation:** Calculate correlation between `tenure_months` and `monthly_spend`\n",
    "5. **Regression:** Build a model predicting spend from tenure and user type\n",
    "6. **Interpret:** Write 2-3 sentences summarizing your findings for a non-technical stakeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the customer dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 400\n",
    "\n",
    "user_type = np.random.choice(['basic', 'premium'], size=n_customers, p=[0.7, 0.3])\n",
    "tenure_months = np.random.exponential(scale=12, size=n_customers) + 1\n",
    "\n",
    "# Premium users spend more on average\n",
    "base_spend = np.where(user_type == 'premium', 45, 25)\n",
    "monthly_spend = base_spend + 0.5 * tenure_months + np.random.normal(0, 8, size=n_customers)\n",
    "monthly_spend = np.maximum(monthly_spend, 5)  # Minimum spend\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'user_type': user_type,\n",
    "    'tenure_months': tenure_months,\n",
    "    'monthly_spend': monthly_spend\n",
    "})\n",
    "\n",
    "print(\"Customer Data Sample:\")\n",
    "print(customers.head(10))\n",
    "print(f\"\\nDataset shape: {customers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd60412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - Complete the analysis\n",
    "\n",
    "# 1. Descriptive Statistics\n",
    "print(\"=== 1. Descriptive Statistics ===\")\n",
    "print(f\"Mean monthly spend: ${customers['monthly_spend'].mean():.2f}\")\n",
    "print(f\"Median monthly spend: ${customers['monthly_spend'].median():.2f}\")\n",
    "print(f\"Std Dev: ${customers['monthly_spend'].std():.2f}\")\n",
    "\n",
    "# 2. Confidence Interval\n",
    "print(\"\\n=== 2. 95% Confidence Interval ===\")\n",
    "ci = t_confidence_interval(customers['monthly_spend'], confidence=0.95)\n",
    "print(f\"95% CI for mean spend: (${ci[0]:.2f}, ${ci[1]:.2f})\")\n",
    "\n",
    "# 3. Hypothesis Test (Premium vs Basic)\n",
    "print(\"\\n=== 3. Hypothesis Test ===\")\n",
    "premium_spend = customers[customers['user_type'] == 'premium']['monthly_spend']\n",
    "basic_spend = customers[customers['user_type'] == 'basic']['monthly_spend']\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(premium_spend, basic_spend, alternative='greater')\n",
    "print(f\"Premium mean: ${premium_spend.mean():.2f}\")\n",
    "print(f\"Basic mean: ${basic_spend.mean():.2f}\")\n",
    "print(f\"T-statistic: {t_stat:.3f}, P-value: {p_val:.6f}\")\n",
    "print(f\"Conclusion: Premium users {'DO' if p_val < 0.05 else 'DO NOT'} spend significantly more (α=0.05)\")\n",
    "\n",
    "# 4. Correlation\n",
    "print(\"\\n=== 4. Correlation Analysis ===\")\n",
    "pearson_r = customers['tenure_months'].corr(customers['monthly_spend'])\n",
    "print(f\"Pearson correlation (tenure vs spend): {pearson_r:.3f}\")\n",
    "\n",
    "# 5. Regression\n",
    "print(\"\\n=== 5. Regression Model ===\")\n",
    "# Create dummy variable for user type\n",
    "customers['is_premium'] = (customers['user_type'] == 'premium').astype(int)\n",
    "model = smf.ols('monthly_spend ~ tenure_months + is_premium', data=customers).fit()\n",
    "print(model.summary().tables[1])\n",
    "\n",
    "# 6. Interpretation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STAKEHOLDER SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Our analysis of 400 customers reveals:\n",
    "\n",
    "1. The average customer spends about ${:.0f}/month (95% confident it's between \n",
    "   ${:.0f} and ${:.0f}).\n",
    "\n",
    "2. Premium users spend significantly more than basic users - about ${:.0f} more \n",
    "   per month on average.\n",
    "\n",
    "3. Longer-tenured customers tend to spend more. For each additional month of \n",
    "   tenure, customers spend about ${:.2f} more.\n",
    "\n",
    "Recommendation: Focus retention efforts on both converting basic users to premium \n",
    "and keeping customers engaged long-term.\n",
    "\"\"\".format(\n",
    "    customers['monthly_spend'].mean(),\n",
    "    ci[0], ci[1],\n",
    "    premium_spend.mean() - basic_spend.mean(),\n",
    "    model.params['tenure_months']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18190c2a",
   "metadata": {},
   "source": [
    "## Additional Resources (Optional)\n",
    "\n",
    "### Documentation\n",
    "- **SciPy stats reference:** https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "- **Statsmodels documentation:** https://www.statsmodels.org/\n",
    "\n",
    "### Free Learning Resources\n",
    "- **Khan Academy (Statistics & Probability):** https://www.khanacademy.org/math/statistics-probability\n",
    "- **OpenIntro Statistics (free textbook):** https://www.openintro.org/book/os/\n",
    "- **Seeing Theory (visual probability):** https://seeing-theory.brown.edu/\n",
    "\n",
    "### Recommended Reading\n",
    "- *Naked Statistics* by Charles Wheelan - Excellent for building intuition\n",
    "- *Statistics Done Wrong* by Alex Reinhart - Learn from common mistakes\n",
    "- *The Art of Statistics* by David Spiegelhalter - Modern statistical thinking\n",
    "\n",
    "### Tools & Calculators\n",
    "- **Sample size calculators:** https://www.evanmiller.org/ab-testing/sample-size.html\n",
    "- **Power analysis:** Use `statsmodels.stats.power` module in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3ad02",
   "metadata": {},
   "source": [
    "## Summary / Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "- **Statistics** helps you quantify uncertainty when learning from samples.\n",
    "- **Descriptive statistics** summarize what you observed; **inferential statistics** make careful guesses about populations.\n",
    "- **Distributions** (binomial, Poisson, normal, etc.) model how data can behave.\n",
    "\n",
    "### Methods Learned\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| Sampling techniques | Collect representative data |\n",
    "| Hypothesis testing | Make yes/no decisions with controlled error |\n",
    "| Confidence intervals | Quantify uncertainty in estimates |\n",
    "| Correlation analysis | Measure relationships between variables |\n",
    "| Linear regression | Model and predict outcomes |\n",
    "| A/B testing | Run controlled experiments |\n",
    "\n",
    "### Critical Thinking Points\n",
    "1. **Correlation ≠ Causation:** Always consider confounders and alternative explanations\n",
    "2. **Statistical significance ≠ Practical significance:** A tiny effect can be \"significant\" with large samples\n",
    "3. **P-values are not probabilities of truth:** They measure surprise, not certainty\n",
    "4. **Check assumptions:** Every statistical method has assumptions; violating them can invalidate results\n",
    "5. **Context matters:** Domain knowledge should guide statistical analysis, not the other way around\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "- ❌ Stopping an experiment early because p < 0.05 (peeking problem)\n",
    "- ❌ Running many tests and picking the best result (multiple comparisons)\n",
    "- ❌ Confusing correlation with causation\n",
    "- ❌ Ignoring effect size and only reporting p-values\n",
    "- ❌ Using parametric tests on heavily skewed data without checking assumptions\n",
    "\n",
    "### What's Next?\n",
    "In Chapter 9, you'll learn about **Database Systems and SQL**, which will help you extract and query data that you'll analyze using these statistical methods."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
