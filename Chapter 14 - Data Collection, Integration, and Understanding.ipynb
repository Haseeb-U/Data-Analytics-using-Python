{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b18b0a",
   "metadata": {},
   "source": [
    "# Chapter 14: Data Collection, Integration, and Understanding\n",
    "\n",
    "Welcome to **Chapter 14**! This chapter is a crucial step in your data analytics journey. Before you can analyze data, you need to **find it**, **bring it together**, and **understand its quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "In this chapter, you will learn how to:\n",
    "\n",
    "1. **Identify data sources** — Know where to find the data you need (files, databases, APIs, etc.)\n",
    "2. **Distinguish structured vs unstructured data** — Understand different data formats and how to work with them\n",
    "3. **Combine internal and external data** — Enrich your analysis with data from multiple sources\n",
    "4. **Apply data integration techniques** — Use merging, joining, and concatenation to combine datasets\n",
    "5. **Assess data quality dimensions** — Check for completeness, validity, uniqueness, consistency, and timeliness\n",
    "6. **Document your data** — Create metadata and data dictionaries for clarity and reproducibility\n",
    "7. **Perform initial data assessment** — Run quick checks before diving into deep analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Chapter Matters\n",
    "\n",
    "> \"Garbage in, garbage out.\"\n",
    "\n",
    "No matter how sophisticated your analysis or model is, if the underlying data is incomplete, incorrect, or poorly understood, your results will be unreliable. This chapter teaches you to **ask the right questions about your data** before you start analyzing it.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this chapter, you should be comfortable with:\n",
    "- Basic Python syntax (Chapter 2)\n",
    "- Pandas DataFrames and basic operations (Chapter 4)\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c564a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.0 Setup: Import Libraries and Create Example Data\n",
    "\n",
    "Before we explore data collection and integration concepts, let's set up our environment and create some realistic example datasets.\n",
    "\n",
    "We'll use:\n",
    "- **pandas** — for working with tabular data\n",
    "- **numpy** — for numeric operations\n",
    "- **matplotlib** — for quick visualizations\n",
    "\n",
    "> **Tip:** If you see `ModuleNotFoundError`, install the required packages with:\n",
    "> ```\n",
    "> pip install pandas numpy matplotlib\n",
    "> ```\n",
    "> (or use Anaconda, which includes these by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d15435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two small, realistic datasets: customers and orders\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103, 104, 105, 106],\n",
    "    'name': ['Asha', 'Bilal', 'Chen', 'Dina', 'Evan', 'Fatima'],\n",
    "    'email': ['asha@example.com', 'bilal@example.com', None, 'dina@example.com', 'evan@example.com', 'fatima@example.com'],\n",
    "    'country': ['PK', 'PK', 'CN', 'US', 'US', 'PK'],\n",
    "    'signup_date': ['2025-10-01', '2025-10-05', '2025-10-05', '2025-10-12', '2025-10-20', '2025-11-02']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [5001, 5002, 5003, 5004, 5005, 5006, 5007],\n",
    "    'customer_id': [101, 102, 102, 104, 999, 105, 106],  # note: 999 doesn't exist in customers\n",
    "    'order_date': ['2025-10-03', '2025-10-06', '2025-10-06', '2025-10-19', '2025-10-21', '2025-10-25', '2025-11-05'],\n",
    "    'amount': [120.50, 49.99, 49.99, 220.00, 15.00, -10.00, 80.00],  # note: negative amount is suspicious\n",
    "    'channel': ['web', 'mobile', 'mobile', 'web', 'web', 'web', 'mobile']\n",
    "})\n",
    "\n",
    "# Convert date columns to real datetime types (important for time logic!)\n",
    "customers['signup_date'] = pd.to_datetime(customers['signup_date'])\n",
    "orders['order_date'] = pd.to_datetime(orders['order_date'])\n",
    "\n",
    "customers, orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410adb15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.1 Identifying Data Sources\n",
    "\n",
    "Before you analyze anything, you need to answer: **Where will the data come from?**\n",
    "\n",
    "### Common Data Sources for Analytics Projects\n",
    "\n",
    "| Source Type | Examples | Typical Format |\n",
    "|-------------|----------|----------------|\n",
    "| **Files** | CSV, Excel, JSON, Parquet | Structured/Semi-structured |\n",
    "| **Databases** | SQLite, PostgreSQL, MySQL, SQL Server | Structured (SQL) |\n",
    "| **APIs** | REST APIs, web services | JSON, XML |\n",
    "| **Logs/Events** | Application logs, clickstream | Semi-structured |\n",
    "| **Manual inputs** | Surveys, forms, spreadsheets | Structured |\n",
    "\n",
    "### How to Choose a Data Source (Beginner Checklist)\n",
    "\n",
    "Ask these questions before selecting a data source:\n",
    "\n",
    "1. **Does it contain the fields you need?** (e.g., `customer_id`, `date`, `amount`)\n",
    "2. **How often is it updated?** (daily, real-time, monthly)\n",
    "3. **Is it trustworthy?** (authoritative system vs. unofficial copy)\n",
    "4. **Can you access it legally and safely?** (permissions, privacy)\n",
    "5. **How much data is it?** (small file vs. billions of rows)\n",
    "\n",
    "> **Tip:** In real projects, it's common to use *multiple* sources (e.g., CRM + web analytics + sales DB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05813f2d",
   "metadata": {},
   "source": [
    "### Example: loading data from common file formats\n",
    "In this chapter we created DataFrames directly, but in real work you usually load from files.\n",
    "\n",
    "Below are examples you can adapt. (They won’t run unless the files exist on your machine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples (uncomment and edit paths to run)\n",
    "# df_csv = pd.read_csv('data/customers.csv')\n",
    "# df_excel = pd.read_excel('data/customers.xlsx', sheet_name='Sheet1')\n",
    "# df_json = pd.read_json('data/customers.json')\n",
    "\n",
    "# Best practice: always inspect after loading\n",
    "# print(df_csv.head())\n",
    "# print(df_csv.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd85948",
   "metadata": {},
   "source": [
    "#### Common beginner mistakes (data sources)\n",
    "- **Assuming** the export file is the “truth” (it might be outdated).\n",
    "- Not checking **encoding** (text files can break on special characters).\n",
    "- Not checking **types** (dates imported as strings, numbers as text).\n",
    "- Loading the *wrong* sheet/tab from Excel.\n",
    "\n",
    "Exercise: Write down 3 possible data sources for a problem you care about (e.g., sales analysis, student performance, social media). For each source, note one risk (missing data, outdated, access restrictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72452a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.2 Structured vs Unstructured Data\n",
    "\n",
    "Understanding the **shape** of data helps you choose the right tools and methods.\n",
    "\n",
    "### Types of Data Structure\n",
    "\n",
    "| Type | Description | Examples | Tools |\n",
    "|------|-------------|----------|-------|\n",
    "| **Structured** | Organized in rows/columns (tables) | CSV, Excel, SQL tables | pandas, SQL |\n",
    "| **Semi-structured** | Has structure, but not fixed columns | JSON, XML, logs | `pd.json_normalize()`, parsers |\n",
    "| **Unstructured** | No consistent table-like structure | Emails, PDFs, images, audio | NLP, OCR, specialized libraries |\n",
    "\n",
    "In beginner analytics, you'll spend most time with **structured** or **semi-structured** data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32b3ad",
   "metadata": {},
   "source": [
    "### Example: semi-structured JSON → structured table\n",
    "Imagine you receive API data like this (nested JSON). We can normalize it into a DataFrame.\n",
    "\n",
    "Why this matters: most analysis needs tabular columns like `customer_id`, `order_id`, `amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_like_json = [\n",
    "    {\n",
    "        'customer': {'customer_id': 101, 'country': 'PK'},\n",
    "        'order': {'order_id': 7001, 'amount': 35.5, 'channel': 'web'},\n",
    "        'tags': ['promo', 'new_user']\n",
    "    },\n",
    "    {\n",
    "        'customer': {'customer_id': 102, 'country': 'PK'},\n",
    "        'order': {'order_id': 7002, 'amount': 120.0, 'channel': 'mobile'},\n",
    "        'tags': []\n",
    "    }\n",
    "]\n",
    "\n",
    "# pd.json_normalize flattens nested dictionaries into columns\n",
    "normalized = pd.json_normalize(api_like_json)\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccc244",
   "metadata": {},
   "source": [
    "Tip: Arrays/lists inside JSON (like `tags`) are not automatically “tabular”.\n",
    "- If each row can have multiple tags, you may need a **separate table** (one row per tag).\n",
    "\n",
    "Exercise: Create a JSON-like list with 3 records. Include a nested object and convert it using `pd.json_normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396994e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter: edit this JSON and normalize it\n",
    "exercise_json = [\n",
    "    {'user': {'id': 1, 'name': 'Sam'}, 'event': {'type': 'click', 'value': 10}},\n",
    "    {'user': {'id': 2, 'name': 'Rita'}, 'event': {'type': 'purchase', 'value': 99.99}},\n",
    "]\n",
    "\n",
    "pd.json_normalize(exercise_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6071ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.3 Internal vs External Data\n",
    "\n",
    "Data can come from **inside your organization** or **outside**. Both can be valuable.\n",
    "\n",
    "### Internal Data\n",
    "- **Examples:** Sales transactions, customer profiles, support tickets, app logs\n",
    "- **Pros:** Usually detailed, aligned to your business\n",
    "- **Cons:** May have missing fields, messy historical changes, siloed systems\n",
    "\n",
    "### External Data\n",
    "- **Examples:** Census data, market prices, weather, competitor data, public APIs\n",
    "- **Pros:** Adds context and comparability\n",
    "- **Cons:** Different definitions, update schedules, licensing restrictions\n",
    "\n",
    "> ⚠️ **Warning:** External data can be **legally restricted**. Always check terms of use and privacy rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333fedd",
   "metadata": {},
   "source": [
    "### Example: enriching internal data with external mapping\n",
    "Suppose our internal data has `country` codes, and we want readable country names (external reference table).\n",
    "\n",
    "Why we do this: readable labels help analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_lookup = pd.DataFrame({\n",
    "    'country': ['PK', 'US', 'CN'],\n",
    "    'country_name': ['Pakistan', 'United States', 'China'],\n",
    "    'region': ['South Asia', 'North America', 'East Asia']\n",
    "})\n",
    "\n",
    "customers_enriched = customers.merge(country_lookup, on='country', how='left')\n",
    "customers_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53a4fc",
   "metadata": {},
   "source": [
    "Common mistake: If you join and see **missing `country_name`**, it usually means:\n",
    "- The key values don’t match (e.g., `pk` vs `PK`, extra spaces)\n",
    "- The lookup table is incomplete\n",
    "\n",
    "Exercise: Intentionally break a key (change `PK` to `pk`) and see what happens. Then fix it using `.str.upper()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f90744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: break and fix join keys\n",
    "customers_bad = customers.copy()\n",
    "customers_bad.loc[customers_bad['country'] == 'PK', 'country'] = 'pk'\n",
    "\n",
    "broken = customers_bad.merge(country_lookup, on='country', how='left')\n",
    "print('Broken join (notice missing country_name):')\n",
    "display(broken)\n",
    "\n",
    "# Fix by standardizing keys\n",
    "customers_fixed = customers_bad.copy()\n",
    "customers_fixed['country'] = customers_fixed['country'].str.upper().str.strip()\n",
    "fixed = customers_fixed.merge(country_lookup, on='country', how='left')\n",
    "print('Fixed join:')\n",
    "fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf6962",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.4 Data Integration Techniques\n",
    "\n",
    "**Data integration** means combining data from multiple sources into a form you can analyze.\n",
    "\n",
    "### Common Integration Patterns\n",
    "\n",
    "| Pattern | When to Use | pandas Method |\n",
    "|---------|-------------|---------------|\n",
    "| **Merge/Join** | Combine columns using a key (e.g., `customer_id`) | `pd.merge()` |\n",
    "| **Append/Concatenate** | Stack rows of similar tables | `pd.concat()` |\n",
    "| **Union with schema alignment** | Combine tables after matching column names/types | `pd.concat()` after alignment |\n",
    "| **Deduplication** | Remove repeated records after combining | `.drop_duplicates()` |\n",
    "| **Mapping/Standardization** | Make categories consistent | `.map()`, `.replace()` |\n",
    "\n",
    "> **Key Concept:** A join is only as good as its **join key quality**. If keys are missing, duplicated, or inconsistent, results can be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f76d83",
   "metadata": {},
   "source": [
    "### 14.4.1 Merge (join) basics\n",
    "We’ll join `orders` with `customers` using `customer_id`.\n",
    "\n",
    "Why: Orders alone tell us *what was bought*, but customers tell us *who bought it* (country, signup date, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_customers = orders.merge(customers, on='customer_id', how='left', indicator=True)\n",
    "orders_with_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611e059",
   "metadata": {},
   "source": [
    "Notice the `_merge` column: it tells us whether the join found a match.\n",
    "- `both` means matching `customer_id` existed in both tables\n",
    "- `left_only` means the order had a `customer_id` not found in customers\n",
    "\n",
    "This is an **early warning** sign of integration issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which orders didn't match a customer?\n",
    "unmatched = orders_with_customers[orders_with_customers['_merge'] != 'both']\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c012a",
   "metadata": {},
   "source": [
    "#### Common join mistakes\n",
    "- Joining on the wrong key (e.g., name instead of id)\n",
    "- Forgetting that keys can have different formats (strings vs integers)\n",
    "- **Many-to-many joins** causing duplicated rows\n",
    "\n",
    "Tip: Always check row counts before and after merge and validate with a quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows in orders:', len(orders))\n",
    "print('Rows after merge:', len(orders_with_customers))\n",
    "\n",
    "# Sanity: order_id should still be unique if each order is one row\n",
    "print('Unique order_id:', orders_with_customers['order_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638be5e3",
   "metadata": {},
   "source": [
    "### 14.4.2 Append/Concatenate basics\n",
    "Concatenation is used when you have **the same kind of table** split into multiple parts.\n",
    "Example: `orders_october.csv` + `orders_november.csv` → one `orders` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_part_1 = orders.iloc[:4].copy()\n",
    "orders_part_2 = orders.iloc[4:].copy()\n",
    "\n",
    "combined_orders = pd.concat([orders_part_1, orders_part_2], ignore_index=True)\n",
    "combined_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdd80f",
   "metadata": {},
   "source": [
    "Warning: If columns don’t match, `concat` will create missing values.\n",
    "This is good (it prevents silent data loss), but it means you must align schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a02cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of mismatched columns\n",
    "orders_part_2_mismatch = orders_part_2.drop(columns=['channel']).copy()\n",
    "mismatch_concat = pd.concat([orders_part_1, orders_part_2_mismatch], ignore_index=True)\n",
    "mismatch_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60c7ac",
   "metadata": {},
   "source": [
    "Exercise: Create two small DataFrames with mostly the same columns, then concatenate.\n",
    "- Identify which column becomes missing\n",
    "- Decide whether to fill missing values or fix the schema before concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf53d2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.5 Data Quality Dimensions\n",
    "\n",
    "Data quality answers: **Can we trust this data enough to use it?**\n",
    "\n",
    "### The Six Dimensions of Data Quality\n",
    "\n",
    "| Dimension | Question | Example Issue |\n",
    "|-----------|----------|---------------|\n",
    "| **Completeness** | Are required fields present? | Missing email addresses |\n",
    "| **Validity** | Do values follow rules? | Negative amounts, future dates |\n",
    "| **Accuracy** | Are values correct in reality? | Wrong customer address |\n",
    "| **Consistency** | Are values the same across systems? | `PK` vs `pk` vs `Pakistan` |\n",
    "| **Uniqueness** | Are there duplicates? | Same order recorded twice |\n",
    "| **Timeliness** | Is the data up to date? | Data from 6 months ago |\n",
    "\n",
    "> **Important:** You can't always fix quality issues immediately, but you must at least **detect and document** them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b145a5",
   "metadata": {},
   "source": [
    "### 14.5.1 Completeness: missing values\n",
    "We start with missing values because they are common and easy to measure.\n",
    "Why: Missing values can break calculations or bias results (e.g., if only some customers have emails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    missing_count = df.isna().sum()\n",
    "    missing_pct = (missing_count / len(df) * 100).round(1)\n",
    "    report = pd.DataFrame({\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct\n",
    "    }).sort_values('missing_pct', ascending=False)\n",
    "    return report\n",
    "\n",
    "missingness_report(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visual: missing percentage by column\n",
    "report = missingness_report(customers)\n",
    "ax = report['missing_pct'].plot(kind='bar', title='Missing % by Column (customers)', ylabel='Missing %')\n",
    "ax.set_ylim(0, max(5, report['missing_pct'].max() + 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815df231",
   "metadata": {},
   "source": [
    "Tip: For critical columns (like keys), missing values are often **not acceptable**.\n",
    "\n",
    "Exercise: Check missingness for `orders`.\n",
    "- Which columns have missing values?\n",
    "- If `amount` were missing, what would you do (drop, fill, investigate)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_report(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8bfb3",
   "metadata": {},
   "source": [
    "### 14.5.2 Validity: rule checks\n",
    "Validity means values follow **business rules** or **logical rules**.\n",
    "Examples:\n",
    "- `amount` should be ≥ 0\n",
    "- `order_date` should not be before `signup_date`\n",
    "- `country` should be one of allowed codes\n",
    "\n",
    "We’ll implement simple rule checks and flag violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a80d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join first so we can compare order_date vs signup_date\n",
    "joined = orders.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "invalid_amount = joined[joined['amount'] < 0]\n",
    "invalid_customer = joined[joined['name'].isna()]  # customer missing\n",
    "invalid_date_logic = joined[(~joined['signup_date'].isna()) & (joined['order_date'] < joined['signup_date'])]\n",
    "\n",
    "print('Invalid amounts (amount < 0):')\n",
    "display(invalid_amount)\n",
    "print('Orders with missing customer record:')\n",
    "display(invalid_customer)\n",
    "print('Orders before signup_date (date logic issue):')\n",
    "display(invalid_date_logic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff917a",
   "metadata": {},
   "source": [
    "Warning: A rule violation doesn’t automatically mean “delete the row”.\n",
    "It means you must **investigate**:\n",
    "- Is it a refund (negative amount)?\n",
    "- Is the customer table incomplete?\n",
    "- Was the signup date recorded incorrectly?\n",
    "\n",
    "Exercise: Add a new rule check: channel must be either `web` or `mobile`. Then intentionally insert a bad value and see if your check catches it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter\n",
    "orders_test = orders.copy()\n",
    "orders_test.loc[0, 'channel'] = 'phone'  # invalid\n",
    "\n",
    "allowed_channels = {'web', 'mobile'}\n",
    "invalid_channel = orders_test[~orders_test['channel'].isin(allowed_channels)]\n",
    "invalid_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8c362",
   "metadata": {},
   "source": [
    "### 14.5.3 Uniqueness and duplicates\n",
    "Duplicates can happen when you:\n",
    "- Import the same file twice\n",
    "- Merge incorrectly\n",
    "- Get repeated events from logs\n",
    "\n",
    "We’ll check uniqueness for `order_id` and also check for duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('order_id unique?', orders['order_id'].is_unique)\n",
    "print('customer_id unique in customers?', customers['customer_id'].is_unique)\n",
    "\n",
    "# Find duplicate rows (exact duplicates)\n",
    "duplicate_rows = orders[orders.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d059d",
   "metadata": {},
   "source": [
    "Exercise: Create a duplicate order row (append it), then use `.duplicated()` to find it.\n",
    "Tip: Use `keep=False` to mark *all* duplicates, not just later copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_dup = pd.concat([orders, orders.iloc[[1]]], ignore_index=True)\n",
    "dups_all = orders_dup[orders_dup.duplicated(keep=False)]\n",
    "dups_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833ceb7",
   "metadata": {},
   "source": [
    "### 14.5.4 Consistency and standardization\n",
    "Consistency means the same concept is recorded the same way across datasets.\n",
    "Examples:\n",
    "- Country codes are always uppercase (`PK`, not `pk`)\n",
    "- Date formats are consistent\n",
    "- Categories are standardized (`mobile` vs `Mobile`)\n",
    "\n",
    "A simple habit: standardize text columns early using `.str.strip()` and consistent casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_messy = orders.copy()\n",
    "orders_messy.loc[2, 'channel'] = ' Mobile '  # messy value\n",
    "\n",
    "print('Before standardization:', orders_messy['channel'].unique())\n",
    "\n",
    "orders_clean = orders_messy.copy()\n",
    "orders_clean['channel'] = orders_clean['channel'].str.strip().str.lower()\n",
    "\n",
    "print('After standardization:', orders_clean['channel'].unique())\n",
    "orders_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac646661",
   "metadata": {},
   "source": [
    "### 14.5.5 Timeliness (freshness)\n",
    "Timeliness asks: **Is the data recent enough for the decision?**\n",
    "Example: If you’re monitoring daily sales, data from 30 days ago might be too old.\n",
    "\n",
    "A simple freshness check is to look at the latest date in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_order_date = orders['order_date'].max()\n",
    "earliest_order_date = orders['order_date'].min()\n",
    "\n",
    "print('Earliest order:', earliest_order_date)\n",
    "print('Latest order:', latest_order_date)\n",
    "\n",
    "# Example: define \n",
    "today = pd.Timestamp('2025-11-10')  # pretend \n",
    " for demonstration\n",
    "freshness_days = (today - latest_order_date).days\n",
    "print('Freshness (days since latest):', freshness_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ed37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.6 Data Documentation and Metadata\n",
    "\n",
    "Professional analytics work is not just code—it's also **communication**.\n",
    "\n",
    "### What is Metadata?\n",
    "\n",
    "**Metadata** is \"data about data\". It answers questions like:\n",
    "- What does each column mean?\n",
    "- What are the allowed values?\n",
    "- Where did the data come from and when was it extracted?\n",
    "- Who owns the dataset?\n",
    "\n",
    "### The Data Dictionary\n",
    "\n",
    "A **data dictionary** is a table describing each column in your dataset. It's one of the most beginner-friendly documentation tools.\n",
    "\n",
    "| Column | Data Type | Description | Allowed Values/Rules |\n",
    "|--------|-----------|-------------|----------------------|\n",
    "| `order_id` | int | Unique identifier for each order | Must be unique |\n",
    "| `amount` | float | Order total in USD | `>= 0` (or negative for refunds) |\n",
    "\n",
    "> **Tip:** When you return to a project later (or share it with a teammate), documentation saves hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cf56a",
   "metadata": {},
   "source": [
    "### Example: building a simple data dictionary\n",
    "We’ll generate a starting data dictionary automatically, then you can fill in descriptions and rules.\n",
    "\n",
    "Why: When you return to a project later (or share it with a teammate), documentation saves hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a871329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dictionary(df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        'table': table_name,\n",
    "        'column': df.columns,\n",
    "        'dtype': [str(t) for t in df.dtypes],\n",
    "        'example_value': [df[c].dropna().iloc[0] if df[c].notna().any() else None for c in df.columns],\n",
    "        'description': ['' for _ in df.columns],\n",
    "        'allowed_values_or_rules': ['' for _ in df.columns]\n",
    "    })\n",
    "\n",
    "dd_customers = make_data_dictionary(customers, 'customers')\n",
    "dd_orders = make_data_dictionary(orders, 'orders')\n",
    "\n",
    "pd.concat([dd_customers, dd_orders], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c3f38",
   "metadata": {},
   "source": [
    "Tip: Keep your data dictionary near your analysis (in the notebook or as a CSV/Markdown file).\n",
    "\n",
    "Exercise: Fill in descriptions for 3 columns in `dd_orders` and add at least one rule (e.g., `amount >= 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41695458",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_orders_filled = dd_orders.copy()\n",
    "\n",
    "dd_orders_filled.loc[dd_orders_filled['column'] == 'order_id', 'description'] = 'Unique identifier for each order'\n",
    "dd_orders_filled.loc[dd_orders_filled['column'] == 'customer_id', 'description'] = 'ID of the customer who placed the order'\n",
    "dd_orders_filled.loc[dd_orders_filled['column'] == 'amount', 'description'] = 'Order total amount in USD'\n",
    "\n",
    "dd_orders_filled.loc[dd_orders_filled['column'] == 'amount', 'allowed_values_or_rules'] = 'amount >= 0 (unless refunds are represented as negatives)'\n",
    "dd_orders_filled.loc[dd_orders_filled['column'] == 'channel', 'allowed_values_or_rules'] = \"Must be one of: 'web', 'mobile'\"\n",
    "\n",
    "dd_orders_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc2f2d",
   "metadata": {},
   "source": [
    "## Chapter 14.7: Initial Data Assessment\n",
    "An **initial data assessment** is a quick, structured review before doing deep analysis.\n",
    "\n",
    "Goal: Find obvious problems early so you don’t build a full analysis on broken assumptions.\n",
    "\n",
    "A practical first-pass checklist:\n",
    "1. **Shape**: How many rows/columns?\n",
    "2. **Types**: Are dates really dates, numbers really numbers?\n",
    "3. **Missing values**: Where are the gaps?\n",
    "4. **Duplicates**: Are keys unique?\n",
    "5. **Ranges**: Are values in reasonable ranges?\n",
    "6. **Join coverage**: Do keys match across tables?\n",
    "7. **Basic distributions**: Quick histograms / value counts\n",
    "\n",
    "We’ll implement a simple assessment report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## 14.7 Initial Data Assessment\n",
    "\n",
    "An **initial data assessment** is a quick, structured review before doing deep analysis.\n",
    "\n",
    "### Goal\n",
    "Find obvious problems early so you don't build a full analysis on broken assumptions.\n",
    "\n",
    "### First-Pass Checklist\n",
    "\n",
    "| Check | What to Look For | pandas Method |\n",
    "|-------|------------------|---------------|\n",
    "| **Shape** | How many rows/columns? | `.shape` |\n",
    "| **Types** | Are dates really dates? | `.dtypes`, `.info()` |\n",
    "| **Missing values** | Where are the gaps? | `.isna().sum()` |\n",
    "| **Duplicates** | Are keys unique? | `.duplicated()`, `.is_unique` |\n",
    "| **Ranges** | Are values in reasonable ranges? | `.describe()`, `.min()`, `.max()` |\n",
    "| **Join coverage** | Do keys match across tables? | `merge(..., indicator=True)` |\n",
    "| **Distributions** | Quick histograms / value counts | `.value_counts()`, `.hist()` |\n",
    "\n",
    "Let's implement a simple assessment report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12aedd",
   "metadata": {},
   "source": [
    "### A quick numeric summary and distribution check\n",
    "Why: Sometimes you catch errors instantly (negative amounts, impossible ages, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[['amount']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of amounts\n",
    "orders['amount'].plot(kind='hist', bins=10, title='Order Amount Distribution', xlabel='amount')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91449e",
   "metadata": {},
   "source": [
    "Exercise: Run `.value_counts()` on `channel` and `country`.\n",
    "- Which category is most common?\n",
    "- If you saw an unexpected category, what would you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f003249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Order channels:')\n",
    "display(orders['channel'].value_counts())\n",
    "\n",
    "print('Customer countries:')\n",
    "display(customers['country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbf983",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14.8 Mini-Project: Build an Integrated, Quality-Checked Dataset\n",
    "\n",
    "In this mini-project you will practice everything from this chapter:\n",
    "\n",
    "1. **Integrate** orders + customers using a merge\n",
    "2. **Flag quality issues** (missing customers, invalid amounts)\n",
    "3. **Produce a clean dataset** for analysis\n",
    "4. **Create a data dictionary** for the final table\n",
    "\n",
    "> **The goal is not \"perfect data\"** — it's building a **repeatable workflow** that you can apply to any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba31803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: integrate\n",
    "integrated = orders.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "# Step 2: add quality flags\n",
    "integrated['flag_missing_customer'] = integrated['name'].isna()\n",
    "integrated['flag_invalid_amount'] = integrated['amount'] < 0\n",
    "\n",
    "# A simple 'status' column can help summarize issues\n",
    "conditions = [\n",
    "    integrated['flag_missing_customer'],\n",
    "    integrated['flag_invalid_amount']\n",
    "]\n",
    "choices = [\n",
    "    'missing_customer',\n",
    "    'invalid_amount'\n",
    "]\n",
    "integrated['quality_status'] = np.select(conditions, choices, default='ok')\n",
    "\n",
    "integrated.sort_values(['quality_status', 'order_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17d152",
   "metadata": {},
   "source": [
    "### Decide what “clean” means\n",
    "Cleaning is a decision based on your context. Here’s a simple approach for beginners:\n",
    "- Exclude rows where `customer_id` is unknown (cannot analyze customer behavior)\n",
    "- Keep negative amounts but treat them separately as potential refunds (do not hide them!)\n",
    "\n",
    "We’ll build a dataset that is usable for typical revenue analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_for_revenue = integrated[~integrated['flag_missing_customer']].copy()\n",
    "\n",
    "# For revenue calculations, you might exclude negative amounts (or handle as refunds)\n",
    "clean_for_revenue['amount_nonnegative'] = clean_for_revenue['amount'].clip(lower=0)\n",
    "\n",
    "clean_for_revenue[['order_id', 'customer_id', 'name', 'order_date', 'amount', 'amount_nonnegative', 'quality_status']].sort_values('order_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64b74f",
   "metadata": {},
   "source": [
    "### Quick analysis check (sanity)\n",
    "Let’s do one small analysis to confirm the integrated dataset behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26950fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_by_country = (\n",
    "    clean_for_revenue\n",
    "    .groupby('country', dropna=False)['amount_nonnegative']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "revenue_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad778cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_by_country.plot(kind='bar', title='Revenue by Country (non-negative amounts)')\n",
    "plt.ylabel('Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1f78a",
   "metadata": {},
   "source": [
    "### Document the final dataset\n",
    "We’ll generate a data dictionary for the `clean_for_revenue` table.\n",
    "Then you can extend it with business meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958378e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_final = make_data_dictionary(clean_for_revenue, 'clean_for_revenue')\n",
    "dd_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad8ae2",
   "metadata": {},
   "source": [
    "Mini-project exercises (recommended)\n",
    "1. Add a new flag: `flag_missing_email` (customer email missing).\n",
    "2. Create a small report table counting rows by `quality_status`.\n",
    "3. Decide a policy for negative amounts (refunds) and implement it.\n",
    "4. Add 3 meaningful descriptions to the final data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mini-Project Exercises\n",
    "\n",
    "Try these exercises to reinforce what you've learned:\n",
    "\n",
    "1. **Add a new flag:** `flag_missing_email` (customer email is missing)\n",
    "2. **Create a summary report:** Count rows by `quality_status`\n",
    "3. **Decide a policy:** How will you handle negative amounts (refunds)?\n",
    "4. **Document your work:** Add 3 meaningful descriptions to the final data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198758b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources (Optional Reading)\n",
    "\n",
    "Expand your knowledge with these resources:\n",
    "\n",
    "- **pandas Merge/Join Guide:** https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "- **pandas Missing Data Guide:** https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "- **JSON Normalization:** https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html\n",
    "- **Intro to Data Quality (Wikipedia):** https://en.wikipedia.org/wiki/Data_quality\n",
    "\n",
    "> **Tip:** When you start working with real APIs and databases, you'll also want to learn about authentication, rate limits, and SQL joins (covered in Chapters 9 and 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b269c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary / Key Takeaways\n",
    "\n",
    "Here's what you learned in this chapter:\n",
    "\n",
    "| Topic | Key Insight |\n",
    "|-------|-------------|\n",
    "| **Data Sources** | Know where your data comes from and verify it's trustworthy and accessible |\n",
    "| **Structured vs Unstructured** | Most analytics uses structured (tables) or semi-structured (JSON) data |\n",
    "| **Internal vs External** | Combine internal data with external sources for richer insights |\n",
    "| **Data Integration** | Use `merge()` for joins and `concat()` for appending — watch your keys! |\n",
    "| **Data Quality** | Check completeness, validity, uniqueness, consistency, and timeliness |\n",
    "| **Documentation** | Create data dictionaries to make your work reusable and trustworthy |\n",
    "| **Initial Assessment** | Always do a quick assessment before advanced analysis |\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Chapter 15: Data Cleaning, Transformation, and Preprocessing**, you'll learn how to:\n",
    "- Handle missing data\n",
    "- Detect and treat outliers\n",
    "- Normalize and scale data\n",
    "- Encode categorical variables\n",
    "- Create new features\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have the foundation to collect, integrate, and understand data before diving into analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
