{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6915430d",
   "metadata": {},
   "source": [
    "# Chapter 23: Applied Data Analytics Case Studies\n",
    "\n",
    "In the previous chapters, you learned individual skills — cleaning data, exploring patterns, building visualizations, and applying statistical methods. Now it's time to put everything together.\n",
    "\n",
    "This chapter presents **four complete case studies** that walk you through real-world analytics scenarios from start to finish. Each case study follows a consistent workflow, demonstrating how professional analysts approach problems in business, marketing, finance, and operations.\n",
    "\n",
    "**Why case studies matter:**\n",
    "- They show how different skills connect in practice\n",
    "- They expose you to realistic data issues and trade-offs\n",
    "- They help you build intuition for what questions to ask\n",
    "- They give you templates you can adapt for your own projects\n",
    "\n",
    "**What you'll work with:**\n",
    "| Case Study | Domain | Key Question |\n",
    "|------------|--------|--------------|\n",
    "| 1 | Retail/Business | Which product categories drive revenue and profit? |\n",
    "| 2 | Marketing | Which channels and messages drive conversions efficiently? |\n",
    "| 3 | Finance | Which loan applicants are higher risk? |\n",
    "| 4 | Operations | Which products are likely to run out of stock? |\n",
    "\n",
    "Each case study uses synthetic data so you can experiment freely without privacy concerns. The patterns and challenges, however, mirror what you'll encounter with real data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c6c19",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- **Follow a repeatable analytics workflow** from problem definition to recommendation\n",
    "- **Use Pandas** to clean, join, and summarize data across different domains\n",
    "- **Create clear visualizations** that support conclusions and tell a story\n",
    "- **Build simple baselines** and explain results in plain, non-technical language\n",
    "- **Recognize common mistakes** (data leakage, small sample traps, metric confusion) and avoid them\n",
    "- **Translate analytical findings into business recommendations**\n",
    "\n",
    "> **Note:** This chapter assumes you're comfortable with the basics of Pandas, Matplotlib, and NumPy covered in earlier chapters. If you need a refresher, refer back to Chapters 3–5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for this chapter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Reproducibility: set a random seed so your results are repeatable\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85e7b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.1 End-to-end analytics workflow (a reusable template)\n",
    "In real projects, the hardest part is often **not** writing code — it's keeping the work structured and explainable.\n",
    "\n",
    "Here is a simple template you can reuse:\n",
    "\n",
    "1) **Problem statement**: What decision will this analysis inform?\n",
    "2) **Success metric**: How will we measure improvement?\n",
    "3) **Data**: What tables/files do we have? What is missing?\n",
    "4) **Data cleaning**: Fix missing values, types, duplicates, outliers\n",
    "5) **EDA**: Summaries + visuals to understand patterns\n",
    "6) **Analysis/model**: Start with a baseline; keep it interpretable\n",
    "7) **Communication**: Insights + recommended actions + limitations\n",
    "\n",
    "### Tip\n",
    "A good beginner habit is writing down your assumptions before you code. It prevents you from accidentally answering a different question than the one you meant to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c8c02",
   "metadata": {},
   "source": [
    "We’ll use the same helper functions across case studies:\n",
    "- `quick_check(df)`: quick data quality scan\n",
    "- `plot_hist(df, col)`: quick distribution check\n",
    "- `train_test_split_time(df, time_col, split)`: time-based split (avoids leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8626bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_check(df: pd.DataFrame, name: str = 'df') -> pd.DataFrame:\n",
    "    \"\"\"A quick, beginner-friendly quality report.\"\"\"\n",
    "    report = pd.DataFrame({\n",
    "        'dtype': df.dtypes.astype(str),\n",
    "        'missing': df.isna().sum(),\n",
    "        'missing_%': (df.isna().mean() * 100).round(2),\n",
    "        'n_unique': df.nunique(dropna=True)\n",
    "    })\n",
    "    print(f'[{name}] shape = {df.shape}')\n",
    "    display(df.head(5))\n",
    "    return report.sort_values(by=['missing', 'n_unique'], ascending=False)\n",
    "\n",
    "def plot_hist(df: pd.DataFrame, col: str, bins: int = 30, title: str | None = None):\n",
    "    data = df[col].dropna()\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.hist(data, bins=bins)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('count')\n",
    "    plt.title(title or f'Distribution of {col}')\n",
    "    plt.show()\n",
    "\n",
    "def train_test_split_time(df: pd.DataFrame, time_col: str, split: float = 0.8):\n",
    "    \"\"\"Time-based split: train is earlier, test is later.\"\"\"\n",
    "    df_sorted = df.sort_values(time_col).reset_index(drop=True)\n",
    "    cut = int(len(df_sorted) * split)\n",
    "    return df_sorted.iloc[:cut].copy(), df_sorted.iloc[cut:].copy()\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366e133",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.2 Case study 1 — Business analytics (retail orders)\n",
    "### Scenario\n",
    "A small online retailer asks: **Which product categories drive revenue and profit?**\n",
    "\n",
    "### What we will do\n",
    "- Generate an orders table (synthetic)\n",
    "- Clean data types and handle missing values\n",
    "- Compute revenue and profit\n",
    "- Summarize by category and visualize top drivers\n",
    "\n",
    "### Common beginner mistake\n",
    "Mixing up **revenue** (money coming in) and **profit** (revenue minus cost). They can tell very different stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea64bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic retail dataset\n",
    "n = 2000\n",
    "categories = ['Electronics', 'Home', 'Clothing', 'Beauty', 'Sports']\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': np.arange(1, n + 1),\n",
    "    'order_date': pd.to_datetime('2025-01-01') + pd.to_timedelta(RNG.integers(0, 365, size=n), unit='D'),\n",
    "    'category': RNG.choice(categories, size=n, p=[0.18, 0.27, 0.25, 0.15, 0.15]),\n",
    "    'units': RNG.integers(1, 6, size=n),\n",
    "    # base_price simulates different typical prices by category\n",
    "    'unit_price': 0.0,\n",
    "    'unit_cost': 0.0,\n",
    "    'returned': RNG.choice([0, 1], size=n, p=[0.92, 0.08])\n",
    "})\n",
    "\n",
    "price_map = {\n",
    "    'Electronics': (220, 0.72),\n",
    "    'Home': (60, 0.65),\n",
    "    'Clothing': (35, 0.55),\n",
    "    'Beauty': (25, 0.50),\n",
    "    'Sports': (55, 0.62),\n",
    "}\n",
    "\n",
    "# Generate price/cost with noise\n",
    "for cat, (base_price, cost_ratio) in price_map.items():\n",
    "    mask = orders['category'] == cat\n",
    "    unit_price = np.maximum(5, RNG.normal(base_price, base_price * 0.15, size=mask.sum()))\n",
    "    unit_cost = unit_price * cost_ratio * RNG.normal(1.0, 0.05, size=mask.sum())\n",
    "    orders.loc[mask, 'unit_price'] = unit_price\n",
    "    orders.loc[mask, 'unit_cost'] = unit_cost\n",
    "\n",
    "# Inject some realistic data issues\n",
    "bad_idx = RNG.choice(orders.index, size=25, replace=False)\n",
    "orders.loc[bad_idx, 'unit_price'] = np.nan\n",
    "orders.loc[RNG.choice(orders.index, size=10, replace=False), 'category'] = None\n",
    "\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7c1ca",
   "metadata": {},
   "source": [
    "### Step 1: Quick data check\n",
    "Before analysis, look for problems: missing values, wrong types, unrealistic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = quick_check(orders, 'orders')\n",
    "quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22324a",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning decisions (and why)\n",
    "We need consistent data to compute revenue/profit. We'll do simple, explainable fixes:\n",
    "- Missing `category`: drop those rows (we can’t group them)\n",
    "- Missing `unit_price`: fill with the **median price of that category** (robust to outliers)\n",
    "\n",
    "**Warning:** Filling missing values can bias results. Always report what you filled and how many rows were affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_clean = orders.dropna(subset=['category']).copy()\n",
    "\n",
    "# Fill missing unit_price using median per category\n",
    "orders_clean['unit_price'] = orders_clean['unit_price'].fillna(\n",
    "    orders_clean.groupby('category')['unit_price'].transform('median')\n",
    ")\n",
    "\n",
    "# Basic sanity checks\n",
    "orders_clean = orders_clean[(orders_clean['units'] > 0) & (orders_clean['unit_price'] > 0) & (orders_clean['unit_cost'] > 0)]\n",
    "\n",
    "quick_check(orders_clean, 'orders_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaee2b9",
   "metadata": {},
   "source": [
    "### Step 3: Feature engineering (revenue & profit)\n",
    "We create new columns that represent business concepts:\n",
    "- `revenue = units × unit_price`\n",
    "- `cost = units × unit_cost`\n",
    "- `profit = revenue − cost`\n",
    "\n",
    "We also handle returns: in a real system, returns often reduce revenue and profit. We'll treat returned orders as **zero revenue and zero profit** for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92154a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_clean['revenue'] = orders_clean['units'] * orders_clean['unit_price']\n",
    "orders_clean['cost'] = orders_clean['units'] * orders_clean['unit_cost']\n",
    "orders_clean['profit'] = orders_clean['revenue'] - orders_clean['cost']\n",
    "\n",
    "orders_clean.loc[orders_clean['returned'] == 1, ['revenue', 'cost', 'profit']] = 0\n",
    "\n",
    "orders_clean[['order_id', 'category', 'units', 'unit_price', 'unit_cost', 'returned', 'revenue', 'profit']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6619a",
   "metadata": {},
   "source": [
    "### Step 4: Summarize and visualize\n",
    "We aggregate by category to find what drives the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cat = (\n",
    "    orders_clean.groupby('category')\n",
    "    .agg(orders=('order_id', 'count'),\n",
    "         units=('units', 'sum'),\n",
    "         revenue=('revenue', 'sum'),\n",
    "         profit=('profit', 'sum'),\n",
    "         return_rate=('returned', 'mean'))\n",
    "    .sort_values('revenue', ascending=False)\n",
    ")\n",
    "\n",
    "summary_cat['profit_margin_%'] = (summary_cat['profit'] / summary_cat['revenue'].replace(0, np.nan) * 100).round(2)\n",
    "summary_cat['return_rate_%'] = (summary_cat['return_rate'] * 100).round(2)\n",
    "summary_cat.drop(columns=['return_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aadbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: revenue vs profit by category\n",
    "plot_df = summary_cat.sort_values('revenue', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.barh(plot_df.index, plot_df['revenue'], label='Revenue')\n",
    "ax.barh(plot_df.index, plot_df['profit'], label='Profit')\n",
    "ax.set_title('Revenue and Profit by Category')\n",
    "ax.set_xlabel('Amount')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53f0df",
   "metadata": {},
   "source": [
    "### Interpretation (example)\n",
    "When you interpret results, try to write in **business language**:\n",
    "- Which categories bring the most revenue?\n",
    "- Which categories bring the most profit (not always the same)?\n",
    "- Do returns look unusually high in any category?\n",
    "\n",
    "**Tip:** Always check profit margin. A category can have high revenue but low profit if costs are high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64711b1",
   "metadata": {},
   "source": [
    "### Exercise 1 (practice)\n",
    "1) Find the top 2 categories by **profit margin** (not by total profit).\n",
    "2) Add a column `avg_order_value` = revenue / orders.\n",
    "3) Create a bar chart of `return_rate_%` by category.\n",
    "\n",
    "Try to answer: *If you could improve only one thing, would you focus on returns or margin?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: profit_margin_% already exists in summary_cat\n",
    "exercise1 = summary_cat.copy()\n",
    "exercise1['avg_order_value'] = (exercise1['revenue'] / exercise1['orders']).round(2)\n",
    "\n",
    "top2_margin = exercise1.sort_values('profit_margin_%', ascending=False).head(2)\n",
    "display(top2_margin[['revenue', 'profit', 'profit_margin_%', 'avg_order_value', 'return_rate_%']])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(exercise1.index, exercise1['return_rate_%'])\n",
    "plt.title('Return Rate by Category')\n",
    "plt.ylabel('Return rate (%)')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698cc46",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.3 Case study 2 — Marketing analytics (campaign performance)\n",
    "### Scenario\n",
    "A marketing team ran several campaigns and asks: **Which channels and messages drive conversions efficiently?**\n",
    "\n",
    "### Key metrics\n",
    "- **CTR** (click-through rate): clicks / impressions\n",
    "- **Conversion rate**: conversions / clicks\n",
    "- **CPA** (cost per acquisition): spend / conversions\n",
    "\n",
    "### Common beginner mistake\n",
    "Comparing conversion rates without checking sample size. A channel with 2 clicks and 1 conversion has 50% conversion rate — but that’s not reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84920e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic marketing campaign dataset\n",
    "days = pd.date_range('2025-07-01', periods=120, freq='D')\n",
    "channels = ['Search', 'Social', 'Email', 'Display']\n",
    "messages = ['Discount', 'Free Shipping', 'New Arrival']\n",
    "\n",
    "rows = []\n",
    "for d in days:\n",
    "    for ch in channels:\n",
    "        for msg in messages:\n",
    "            impressions = int(np.maximum(50, RNG.normal(800 if ch in ['Search', 'Social'] else 500, 180)))\n",
    "            base_ctr = {'Search': 0.035, 'Social': 0.025, 'Email': 0.06, 'Display': 0.012}[ch]\n",
    "            msg_boost = {'Discount': 1.15, 'Free Shipping': 1.05, 'New Arrival': 0.95}[msg]\n",
    "            ctr = np.clip(base_ctr * msg_boost * RNG.normal(1.0, 0.12), 0.001, 0.2)\n",
    "            clicks = RNG.binomial(impressions, ctr)\n",
    "\n",
    "            # conversion probability given a click\n",
    "            base_cvr = {'Search': 0.06, 'Social': 0.035, 'Email': 0.07, 'Display': 0.02}[ch]\n",
    "            cvr = np.clip(base_cvr * msg_boost * RNG.normal(1.0, 0.12), 0.001, 0.4)\n",
    "            conversions = RNG.binomial(clicks, cvr)\n",
    "\n",
    "            # spend: depends on channel and volume\n",
    "            cpc = {'Search': 1.8, 'Social': 1.1, 'Email': 0.2, 'Display': 0.6}[ch]\n",
    "            spend = clicks * cpc * float(np.clip(RNG.normal(1.0, 0.08), 0.7, 1.3))\n",
    "\n",
    "            rows.append((d, ch, msg, impressions, clicks, conversions, spend))\n",
    "\n",
    "marketing = pd.DataFrame(rows, columns=['date', 'channel', 'message', 'impressions', 'clicks', 'conversions', 'spend'])\n",
    "\n",
    "# Inject a common data issue: missing spend\n",
    "marketing.loc[RNG.choice(marketing.index, size=20, replace=False), 'spend'] = np.nan\n",
    "\n",
    "marketing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5f573",
   "metadata": {},
   "source": [
    "### Step 1: Clean and create metrics\n",
    "We compute metrics carefully to avoid division-by-zero. We'll fill missing spend using the median spend for the same channel (simple and explainable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_clean = marketing.copy()\n",
    "marketing_clean['spend'] = marketing_clean['spend'].fillna(\n",
    "    marketing_clean.groupby('channel')['spend'].transform('median')\n",
    ")\n",
    "\n",
    "# Safe divisions: replace 0 with NaN to avoid errors, then fill where appropriate\n",
    "marketing_clean['ctr'] = marketing_clean['clicks'] / marketing_clean['impressions'].replace(0, np.nan)\n",
    "marketing_clean['conversion_rate'] = marketing_clean['conversions'] / marketing_clean['clicks'].replace(0, np.nan)\n",
    "marketing_clean['cpa'] = marketing_clean['spend'] / marketing_clean['conversions'].replace(0, np.nan)\n",
    "\n",
    "quick_check(marketing_clean, 'marketing_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cce7b",
   "metadata": {},
   "source": [
    "### Step 2: Summarize performance\n",
    "We group by `channel` and `message`, then look for:\n",
    "- High conversions\n",
    "- Good conversion rate\n",
    "- Low CPA\n",
    "\n",
    "**Warning:** CPA can be misleading when conversions are very low. Always check conversion counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f93fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = (\n",
    "    marketing_clean\n",
    "    .groupby(['channel', 'message'])\n",
    "    .agg(impressions=('impressions', 'sum'),\n",
    "         clicks=('clicks', 'sum'),\n",
    "         conversions=('conversions', 'sum'),\n",
    "         spend=('spend', 'sum'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "perf['ctr'] = perf['clicks'] / perf['impressions'].replace(0, np.nan)\n",
    "perf['conversion_rate'] = perf['conversions'] / perf['clicks'].replace(0, np.nan)\n",
    "perf['cpa'] = perf['spend'] / perf['conversions'].replace(0, np.nan)\n",
    "\n",
    "perf.sort_values(['conversions', 'cpa'], ascending=[False, True]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: CPA by channel (overall)\n",
    "overall = (\n",
    "    marketing_clean\n",
    "    .groupby('channel')\n",
    "    .agg(conversions=('conversions', 'sum'), spend=('spend', 'sum'))\n",
    "    .assign(cpa=lambda d: d['spend'] / d['conversions'].replace(0, np.nan))\n",
    "    .sort_values('cpa')\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(overall.index, overall['cpa'])\n",
    "plt.title('Cost per Acquisition (CPA) by Channel')\n",
    "plt.ylabel('CPA (lower is better)')\n",
    "plt.show()\n",
    "\n",
    "overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a6913",
   "metadata": {},
   "source": [
    "### Exercise 2 (practice)\n",
    "1) Add a filter to keep only channel-message pairs with at least **200 conversions**.\n",
    "2) Among those, find the lowest CPA.\n",
    "3) Plot conversion rate by message for the best channel.\n",
    "\n",
    "Goal: practice comparing options while avoiding small-sample traps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc70716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "filtered = perf[perf['conversions'] >= 200].copy()\n",
    "best = filtered.sort_values('cpa').head(1)\n",
    "display(best)\n",
    "\n",
    "best_channel = best['channel'].iloc[0]\n",
    "subset = filtered[filtered['channel'] == best_channel].sort_values('message')\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(subset['message'], subset['conversion_rate'])\n",
    "plt.title(f'Conversion Rate by Message (Channel: {best_channel})')\n",
    "plt.ylabel('Conversion rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c88fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.4 Case study 3 — Finance analytics (credit risk screening)\n",
    "### Scenario\n",
    "A lender wants to reduce defaults. The question is: **Which applicants are higher risk?**\n",
    "\n",
    "We will:\n",
    "- Build a simple risk score (baseline model)\n",
    "- Evaluate with accuracy and confusion matrix\n",
    "- Discuss why accuracy alone can be misleading\n",
    "\n",
    "### Important note (ethics)\n",
    "Real credit models are heavily regulated and must be fair and explainable. This example is educational and uses synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic applicant data\n",
    "m = 2500\n",
    "apps = pd.DataFrame({\n",
    "    'applicant_id': np.arange(1, m + 1),\n",
    "    'age': RNG.integers(21, 70, size=m),\n",
    "    'annual_income': np.maximum(12000, RNG.normal(52000, 18000, size=m)),\n",
    "    'debt_to_income': np.clip(RNG.normal(0.28, 0.12, size=m), 0.02, 0.95),\n",
    "    'credit_history_years': np.clip(RNG.normal(7.0, 4.0, size=m), 0, 30),\n",
    "    'num_late_payments': np.clip(RNG.poisson(1.0, size=m), 0, 12),\n",
    "})\n",
    "\n",
    "# A synthetic probability of default: higher with high DTI and many late payments\n",
    "logit = (\n",
    "    -3.0\n",
    "    + 3.8 * apps['debt_to_income']\n",
    "    + 0.18 * apps['num_late_payments']\n",
    "    - 0.04 * (apps['credit_history_years'])\n",
    "    - 0.00001 * (apps['annual_income'])\n",
    ")\n",
    "p_default = 1 / (1 + np.exp(-logit))\n",
    "apps['defaulted'] = RNG.binomial(1, np.clip(p_default, 0.001, 0.9))\n",
    "\n",
    "# Inject missing values\n",
    "apps.loc[RNG.choice(apps.index, size=20, replace=False), 'annual_income'] = np.nan\n",
    "apps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feafe72",
   "metadata": {},
   "source": [
    "### Step 1: Clean and inspect\n",
    "We fill missing income with the median income. This is a simple baseline. In real finance work, missingness might be informative and needs careful handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b35bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_clean = apps.copy()\n",
    "apps_clean['annual_income'] = apps_clean['annual_income'].fillna(apps_clean['annual_income'].median())\n",
    "\n",
    "quick_check(apps_clean, 'apps_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b3060",
   "metadata": {},
   "source": [
    "### Step 2: Build a simple risk score (interpretable baseline)\n",
    "Instead of jumping to a complex model, we will build a **rule-based score**.\n",
    "\n",
    "Why?\n",
    "- Easy to explain\n",
    "- Helps you understand which variables matter\n",
    "- Gives a baseline to beat later\n",
    "\n",
    "We’ll score applicants with higher risk if they have:\n",
    "- high debt-to-income\n",
    "- many late payments\n",
    "- short credit history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_scored = apps_clean.copy()\n",
    "apps_scored['risk_score'] = (\n",
    "    60 * apps_scored['debt_to_income']\n",
    "    + 6 * apps_scored['num_late_payments']\n",
    "    - 1.2 * apps_scored['credit_history_years']\n",
    ")\n",
    "\n",
    "# Higher score => higher predicted risk\n",
    "plot_hist(apps_scored, 'risk_score', bins=40, title='Risk Score Distribution')\n",
    "apps_scored[['debt_to_income', 'num_late_payments', 'credit_history_years', 'risk_score', 'defaulted']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b188c",
   "metadata": {},
   "source": [
    "### Step 3: Choose a decision threshold and evaluate\n",
    "We need a threshold like: “If risk_score ≥ X, predict default”.\n",
    "\n",
    "In real life, the threshold depends on business costs:\n",
    "- false negative (miss a risky applicant)\n",
    "- false positive (reject a good applicant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (random is ok here because there's no time component)\n",
    "apps_scored = apps_scored.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "cut = int(len(apps_scored) * 0.8)\n",
    "train, test = apps_scored.iloc[:cut], apps_scored.iloc[cut:]\n",
    "\n",
    "# Choose threshold using training data: try a few candidates and pick the best F1-like balance\n",
    "candidates = np.quantile(train['risk_score'], [0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def precision_recall(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    return precision, recall\n",
    "\n",
    "results = []\n",
    "for thr in candidates:\n",
    "    pred = (train['risk_score'] >= thr).astype(int)\n",
    "    tp, tn, fp, fn = confusion(train['defaulted'], pred)\n",
    "    precision, recall = precision_recall(tp, fp, fn)\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    results.append({'threshold': float(thr), 'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "pd.DataFrame(results).sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42944075",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thr = float(pd.DataFrame(results).sort_values('f1', ascending=False).iloc[0]['threshold'])\n",
    "test_pred = (test['risk_score'] >= best_thr).astype(int)\n",
    "tp, tn, fp, fn = confusion(test['defaulted'], test_pred)\n",
    "precision, recall = precision_recall(tp, fp, fn)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "metrics = pd.DataFrame([{\n",
    "    'threshold': best_thr,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "}])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2310c7",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- **Precision** answers: “When we predict default, how often are we correct?”\n",
    "- **Recall** answers: “Of all true defaults, how many did we catch?”\n",
    "\n",
    "**Tip:** In many risk problems, catching risky cases (recall) matters more than overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7116c5ba",
   "metadata": {},
   "source": [
    "### Exercise 3 (practice)\n",
    "1) Try a different scoring formula (change weights).\n",
    "2) Compare your test precision/recall to the original.\n",
    "3) In one sentence, explain the trade-off you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51def359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: example alternative weights\n",
    "apps_alt = apps_clean.copy()\n",
    "apps_alt['risk_score'] = (70 * apps_alt['debt_to_income'] + 4 * apps_alt['num_late_payments'] - 1.0 * apps_alt['credit_history_years'])\n",
    "\n",
    "apps_alt = apps_alt.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "train_alt, test_alt = apps_alt.iloc[:cut], apps_alt.iloc[cut:]\n",
    "\n",
    "thr = float(np.quantile(train_alt['risk_score'], 0.8))\n",
    "pred_alt = (test_alt['risk_score'] >= thr).astype(int)\n",
    "tp, tn, fp, fn = confusion(test_alt['defaulted'], pred_alt)\n",
    "precision_alt, recall_alt = precision_recall(tp, fp, fn)\n",
    "accuracy_alt = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "pd.DataFrame([{\n",
    "    'threshold': thr,\n",
    "    'accuracy': accuracy_alt,\n",
    "    'precision': precision_alt,\n",
    "    'recall': recall_alt,\n",
    "    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73038b3f",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.5 Case study 4 — Operations / supply chain (inventory & demand)\n",
    "### Scenario\n",
    "A warehouse team wants to avoid stockouts. The question is: **Which products are likely to run out soon?**\n",
    "\n",
    "We will:\n",
    "- Create daily demand data for multiple products\n",
    "- Use a time-based train/test split (important!)\n",
    "- Build a simple forecast baseline\n",
    "- Convert forecast into a restock recommendation\n",
    "\n",
    "### Common beginner mistake (data leakage)\n",
    "Randomly splitting time-series data mixes past and future, causing overly optimistic results. Always keep the future in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic daily demand for 8 products\n",
    "products = [f'P{i:02d}' for i in range(1, 9)]\n",
    "dates = pd.date_range('2025-01-01', periods=220, freq='D')\n",
    "\n",
    "demand_rows = []\n",
    "for p in products:\n",
    "    base = RNG.integers(10, 60)\n",
    "    weekly = RNG.uniform(0.1, 0.35)\n",
    "    trend = RNG.uniform(-0.01, 0.02)\n",
    "    for t, d in enumerate(dates):\n",
    "        season = 1 + weekly * np.sin(2 * np.pi * (t % 7) / 7)\n",
    "        mean = base * season * (1 + trend * t)\n",
    "        qty = max(0, int(RNG.normal(mean, max(2, mean * 0.20))))\n",
    "        demand_rows.append((d, p, qty))\n",
    "\n",
    "demand = pd.DataFrame(demand_rows, columns=['date', 'product', 'demand'])\n",
    "demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a63f6d",
   "metadata": {},
   "source": [
    "### Step 1: Visualize demand for one product\n",
    "A quick plot helps you see patterns like seasonality or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = demand[demand['product'] == 'P01'].copy()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(one['date'], one['demand'])\n",
    "plt.title('Daily Demand for Product P01')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Units demanded')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c868b",
   "metadata": {},
   "source": [
    "### Step 2: Forecast baseline (moving average)\n",
    "A strong beginner baseline for forecasting is a **moving average**: predict tomorrow as the average of the last $k$ days.\n",
    "\n",
    "Why it works (sometimes):\n",
    "- Smooths random noise\n",
    "- Easy to explain\n",
    "- Often competitive as a baseline\n",
    "\n",
    "We’ll evaluate using RMSE (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_forecast(series: pd.Series, window: int = 7) -> pd.Series:\n",
    "    # Forecast for each day uses the *previous* window (shifted to avoid leakage)\n",
    "    return series.shift(1).rolling(window=window).mean()\n",
    "\n",
    "# Evaluate for each product\n",
    "rows = []\n",
    "for p in products:\n",
    "    dfp = demand[demand['product'] == p].copy()\n",
    "    train_p, test_p = train_test_split_time(dfp, 'date', split=0.8)\n",
    "\n",
    "    train_p['pred_7'] = moving_average_forecast(train_p['demand'], window=7)\n",
    "    test_p = pd.concat([train_p.tail(10), test_p], ignore_index=True)  # include a bit of history for rolling\n",
    "    test_p['pred_7'] = moving_average_forecast(test_p['demand'], window=7)\n",
    "    test_eval = test_p.iloc[10:].copy()\n",
    "\n",
    "    score = rmse(test_eval['demand'], test_eval['pred_7'])\n",
    "    rows.append({'product': p, 'rmse_7day_ma': score, 'avg_demand': test_eval['demand'].mean()})\n",
    "\n",
    "scores = pd.DataFrame(rows).sort_values('rmse_7day_ma')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2575c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: forecast vs actual for one product\n",
    "p = 'P01'\n",
    "dfp = demand[demand['product'] == p].copy()\n",
    "train_p, test_p = train_test_split_time(dfp, 'date', split=0.8)\n",
    "combined = pd.concat([train_p.tail(20), test_p], ignore_index=True)\n",
    "combined['pred_7'] = moving_average_forecast(combined['demand'], window=7)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(combined['date'], combined['demand'], label='Actual')\n",
    "plt.plot(combined['date'], combined['pred_7'], label='7-day MA forecast')\n",
    "plt.title(f'Actual vs Forecast (Product {p})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Demand')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7e8d5",
   "metadata": {},
   "source": [
    "### Step 3: From forecast to a restock recommendation\n",
    "Analytics becomes valuable when it supports decisions.\n",
    "\n",
    "A simple rule:\n",
    "- Forecast next 14 days demand\n",
    "- Compare to current inventory\n",
    "- If inventory is less than forecast + safety stock → recommend restock\n",
    "\n",
    "We’ll create a toy inventory table and compute recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55066c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = pd.DataFrame({\n",
    "    'product': products,\n",
    "    'on_hand': RNG.integers(150, 650, size=len(products)),\n",
    "    'safety_stock': RNG.integers(40, 140, size=len(products))\n",
    "})\n",
    "inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbaa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 14 days using the last available moving average\n",
    "horizon = 14\n",
    "recs = []\n",
    "\n",
    "for p in products:\n",
    "    dfp = demand[demand['product'] == p].sort_values('date')\n",
    "    # Last known 7-day average demand (as a simple constant forecast)\n",
    "    last_ma = dfp['demand'].tail(7).mean()\n",
    "    forecast_14 = last_ma * horizon\n",
    "    recs.append({'product': p, 'forecast_14d': forecast_14})\n",
    "\n",
    "recs = pd.DataFrame(recs)\n",
    "plan = inventory.merge(recs, on='product', how='left')\n",
    "plan['needed'] = plan['forecast_14d'] + plan['safety_stock']\n",
    "plan['restock_qty'] = np.maximum(0, np.ceil(plan['needed'] - plan['on_hand'])).astype(int)\n",
    "plan.sort_values('restock_qty', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34422a",
   "metadata": {},
   "source": [
    "### Exercise 4 (mini-project)\n",
    "Pick one product and improve the forecast baseline:\n",
    "1) Compare 7-day vs 14-day moving average using RMSE\n",
    "2) Choose the better window\n",
    "3) Recompute the restock recommendation using that window\n",
    "\n",
    "Optional: Try a weekday/weekend average (simple seasonality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (starter example for one product)\n",
    "p = 'P02'\n",
    "dfp = demand[demand['product'] == p].copy()\n",
    "train_p, test_p = train_test_split_time(dfp, 'date', split=0.8)\n",
    "\n",
    "def eval_window(window):\n",
    "    combined = pd.concat([train_p.tail(window + 3), test_p], ignore_index=True)\n",
    "    combined['pred'] = moving_average_forecast(combined['demand'], window=window)\n",
    "    eval_part = combined.iloc[window + 3:].copy()\n",
    "    return rmse(eval_part['demand'], eval_part['pred'])\n",
    "\n",
    "rmse7 = eval_window(7)\n",
    "rmse14 = eval_window(14)\n",
    "rmse7, rmse14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29ac18",
   "metadata": {},
   "source": [
    "---\n",
    "## 23.6 Lessons learned (how to think like an analyst)\n",
    "Across the case studies, several themes repeat:\n",
    "\n",
    "1) **Start with the decision** (What will someone do differently?)\n",
    "2) **Define metrics clearly** (Revenue vs profit, CTR vs conversion rate, etc.)\n",
    "3) **Clean data intentionally** (document what you dropped/filled)\n",
    "4) **Use baselines** (simple first, then improve)\n",
    "5) **Avoid leakage** (especially with time-series)\n",
    "6) **Communicate limitations** (synthetic data, missing variables, assumptions)\n",
    "\n",
    "### Common mistakes checklist\n",
    "- Confusing correlation with causation\n",
    "- Choosing metrics that don’t match the decision\n",
    "- Ignoring small sample sizes\n",
    "- Overfitting (making the model too complex)\n",
    "- Forgetting to check data types and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f8158",
   "metadata": {},
   "source": [
    "## Additional resources (optional)\n",
    "- Pandas user guide: https://pandas.pydata.org/docs/user_guide/\n",
    "- Matplotlib tutorials: https://matplotlib.org/stable/tutorials/\n",
    "- Google Data Analytics case study examples (non-Python, but useful structure): https://www.coursera.org/professional-certificates/google-data-analytics\n",
    "- Forecasting basics (time series): https://otexts.com/fpp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b456c",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary / Key Takeaways\n",
    "\n",
    "Congratulations! You've completed four end-to-end analytics case studies. Here's what you should take away:\n",
    "\n",
    "### The Analytics Workflow\n",
    "A good analytics project is a **structured story**: Question → Data → Cleaning → EDA → Analysis → Decision\n",
    "\n",
    "### Key Lessons by Case Study\n",
    "\n",
    "| Case Study | Key Insight |\n",
    "|------------|-------------|\n",
    "| **Business (Retail)** | Revenue ≠ Profit — always check margins, not just totals |\n",
    "| **Marketing** | Beware small samples — high conversion rates mean nothing with few conversions |\n",
    "| **Finance (Credit)** | Accuracy can be misleading — precision and recall matter more for imbalanced outcomes |\n",
    "| **Operations (Inventory)** | Time-series need time-based splits — random splits cause data leakage |\n",
    "\n",
    "### Universal Principles\n",
    "1. **Start simple and interpretable** — baselines are powerful and often sufficient\n",
    "2. **Document your cleaning decisions** — what you dropped, filled, or transformed\n",
    "3. **Visualize early and often** — patterns emerge faster through plots\n",
    "4. **Always report limitations** — assumptions, missing data, and uncertainty\n",
    "5. **Connect to decisions** — analytics is valuable only when it informs action\n",
    "\n",
    "### Next Steps\n",
    "- Turn one case study into a short report (1–2 pages) with charts and bullet recommendations\n",
    "- Try applying the same workflow to a dataset you find interesting\n",
    "- Practice explaining your analysis to someone non-technical — this builds real-world skills\n",
    "\n",
    "> **Final Tip:** The best analysts aren't the ones with the most complex models. They're the ones who ask the right questions, clean data carefully, and communicate clearly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
