{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d78769b",
   "metadata": {},
   "source": [
    "# Chapter 16: Exploratory Data Analysis (EDA) Methodology\n",
    "\n",
    "EDA (Exploratory Data Analysis) is the stage where you **get to know your data** before making big decisions or building models. Think of it as a detective's investigation â€” you gather clues, spot patterns, and form theories before drawing conclusions.\n",
    "\n",
    "In this chapter you will learn a practical, repeatable approach to EDA:\n",
    "- The overall **EDA workflow** you can reuse on any dataset\n",
    "- **Univariate** analysis (one variable at a time)\n",
    "- **Bivariate** analysis (two variables together)\n",
    "- **Multivariate** analysis (three or more variables)\n",
    "- Visual + statistical exploration techniques\n",
    "- Pattern and anomaly detection methods\n",
    "- Hypothesis refinement based on evidence\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction: What EDA Is (and What It Is Not)](#introduction-what-eda-is-and-what-it-is-not)\n",
    "2. [Setup and Dataset Creation](#setup)\n",
    "3. [16.1 EDA Workflow (The Checklist)](#161-eda-workflow-the-checklist)\n",
    "4. [16.2 Univariate Analysis](#162-univariate-analysis-one-variable-at-a-time)\n",
    "5. [16.3 Bivariate Analysis](#163-bivariate-analysis-two-variables)\n",
    "6. [16.4 Multivariate Analysis](#164-multivariate-analysis-3-variables)\n",
    "7. [16.5 Visual and Statistical Exploration](#165-visual-and-statistical-exploration)\n",
    "8. [16.6 Pattern and Anomaly Detection](#166-pattern-and-anomaly-detection)\n",
    "9. [16.7 Hypothesis Refinement](#167-hypothesis-refinement)\n",
    "10. [Summary / Key Takeaways](#summary--key-takeaways)\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** This chapter assumes you are comfortable with:\n",
    "- Basic Python (Chapter 2)\n",
    "- NumPy arrays (Chapter 3)\n",
    "- Pandas DataFrames (Chapter 4)\n",
    "- Basic plotting with Matplotlib/Seaborn (Chapter 5)\n",
    "\n",
    "This notebook is **self-contained**: it generates a practice dataset and walks through EDA step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c8e78",
   "metadata": {},
   "source": [
    "## Introduction: What EDA Is (and What It Is Not)\n",
    "\n",
    "### What EDA Is\n",
    "EDA is a set of techniques to:\n",
    "- **Understand columns** â€” their meaning, types, and value ranges\n",
    "- **Find data quality issues early** â€” missing values, duplicates, inconsistent entries\n",
    "- **Discover relationships** â€” does discount affect returns? does age relate to spending?\n",
    "- **Spot patterns and anomalies** â€” unusual spikes, clusters, or outliers worth investigating\n",
    "\n",
    "EDA was popularized by statistician John Tukey in the 1970s. His philosophy: let the data speak before imposing assumptions.\n",
    "\n",
    "### What EDA Is Not\n",
    "EDA is **not proof of causation**. During EDA you generate ideas and hypotheses. You then validate them later with careful statistics, experiments, or domain expertise.\n",
    "\n",
    "| EDA Tells You | EDA Does NOT Tell You |\n",
    "|---------------|----------------------|\n",
    "| \"Sales are higher on weekends\" | \"Weekends *cause* higher sales\" |\n",
    "| \"Discount and returns are correlated\" | \"Discounts *lead to* more returns\" |\n",
    "| \"There are 5 outlier transactions\" | \"These outliers are errors vs real\" |\n",
    "\n",
    "> ðŸ’¡ **Tip:** Think of EDA as turning on the lights before you start working. You see what's in the room, but you still need to investigate each item.\n",
    "\n",
    "### Why EDA Matters\n",
    "- **Saves time later:** Catching data issues early prevents wrong conclusions\n",
    "- **Guides analysis:** EDA reveals which variables are worth deeper study\n",
    "- **Builds intuition:** You develop a \"feel\" for the data that helps with modeling decisions\n",
    "\n",
    "> âš ï¸ **Common Beginner Mistake:** Jumping straight to complex models without understanding the data first. This often leads to wasted effort and wrong results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1037bb",
   "metadata": {},
   "source": [
    "## Chapter Map: What We Will Do\n",
    "\n",
    "We will follow this structured EDA flow:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. Load/Create Data  â†’  2. Quick Inspection  â†’  3. Cleanup    â”‚\n",
    "â”‚         â†“                                                       â”‚\n",
    "â”‚  4. Univariate  â†’  5. Bivariate  â†’  6. Multivariate            â”‚\n",
    "â”‚         â†“                                                       â”‚\n",
    "â”‚  7. Pattern & Anomaly Detection  â†’  8. Hypothesis Refinement   â”‚\n",
    "â”‚         â†“                                                       â”‚\n",
    "â”‚  9. Summarize Insights                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Each Stage Explained:\n",
    "| Stage | Purpose | Example Question |\n",
    "|-------|---------|------------------|\n",
    "| Inspection | Understand structure | How many rows? What data types? |\n",
    "| Cleanup | Fix obvious issues | Standardize text, remove duplicates |\n",
    "| Univariate | One variable at a time | What's the average revenue? |\n",
    "| Bivariate | Two variables together | Does revenue differ by channel? |\n",
    "| Multivariate | 3+ variables | Does the discount-return relationship vary by channel? |\n",
    "| Patterns/Anomalies | Unusual observations | Are there outliers? Time-based spikes? |\n",
    "| Hypothesis Refinement | Update questions | Refine vague ideas into testable hypotheses |\n",
    "\n",
    "> ðŸ’¡ **Tip:** Following a consistent workflow means you're less likely to miss important findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599b046",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will use these libraries for EDA:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `pandas` | Data manipulation and summaries |\n",
    "| `numpy` | Numerical operations |\n",
    "| `matplotlib` | Basic plotting |\n",
    "| `seaborn` | Statistical visualizations (built on matplotlib) |\n",
    "| `scipy` (optional) | Statistical tests |\n",
    "\n",
    "If you need to install these packages, run in a terminal:\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn scipy\n",
    "```\n",
    "\n",
    "> ðŸ“š **Reference:** \n",
    "> - [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "> - [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a clean visual style for all plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility (same results every time)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: SciPy for statistical tests (we will fall back gracefully if not installed)\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "print(f\"SciPy available: {SCIPY_AVAILABLE}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1468da",
   "metadata": {},
   "source": [
    "## Loading a Practice Dataset\n",
    "\n",
    "We'll use the **tips** dataset from seaborn â€” a classic dataset for learning data analysis. It contains restaurant tipping data with:\n",
    "- Numeric columns (total bill, tip amount)\n",
    "- Categorical columns (sex, smoker status, day, time, size)\n",
    "- Real patterns to discover through EDA\n",
    "\n",
    "We'll transform it slightly to match our e-commerce orders context and add some data quality issues for practice.\n",
    "\n",
    "### Why Use Real Data?\n",
    "- âœ… You practice EDA on actual patterns that exist in the real world\n",
    "- âœ… No external files needed â€” seaborn datasets are built-in\n",
    "- âœ… Results are reproducible across different machines\n",
    "- âœ… You learn to work with the quirks of real data\n",
    "\n",
    "### Dataset Columns (After Transformation):\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `order_id` | int | Unique order identifier |\n",
    "| `order_date` | date | When the order was placed |\n",
    "| `channel` | categorical | Order source: web, app, store |\n",
    "| `region` | categorical | Geographic region: North, South, East, West |\n",
    "| `customer_age` | numeric | Customer's age in years |\n",
    "| `items` | int | Number of items in the order |\n",
    "| `discount` | numeric | Discount applied (0 to 0.6) |\n",
    "| `revenue` | numeric | Order revenue in dollars |\n",
    "| `returned` | boolean | Whether the order was returned |\n",
    "| `satisfaction` | numeric | Customer satisfaction score (1-10) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24238fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tips dataset from seaborn\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Set random generator for reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Transform to our orders context\n",
    "n_rows = len(tips)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'order_id': np.arange(1, n_rows + 1),\n",
    "    'order_date': pd.to_datetime('2025-01-01') + pd.to_timedelta(rng.integers(0, 180, size=n_rows), unit='D'),\n",
    "    'channel': tips['time'].map({'Lunch': 'web', 'Dinner': 'app'}).where(\n",
    "        rng.random(n_rows) > 0.15, 'store'\n",
    "    ),\n",
    "    'region': tips['day'].map({'Thur': 'North', 'Fri': 'South', 'Sat': 'East', 'Sun': 'West'}),\n",
    "    'customer_age': rng.normal(34, 10, size=n_rows).round().clip(18, 75),\n",
    "    'items': tips['size'].clip(1, 12),\n",
    "    'discount': rng.beta(a=2, b=8, size=n_rows).clip(0, 0.6),\n",
    "    'revenue': tips['total_bill'],\n",
    "    'returned': tips['smoker'] == 'Yes',  # Using smoker as a proxy for returned\n",
    "    'satisfaction': rng.normal(7.2, 1.2, size=n_rows).clip(1, 10),\n",
    "})\n",
    "\n",
    "# Inject common data issues for EDA practice\n",
    "# 1) Missing values\n",
    "missing_idx = rng.choice(df.index, size=int(0.03 * n_rows), replace=False)\n",
    "df.loc[missing_idx, 'satisfaction'] = np.nan\n",
    "missing_idx2 = rng.choice(df.index, size=int(0.02 * n_rows), replace=False)\n",
    "df.loc[missing_idx2, 'customer_age'] = np.nan\n",
    "\n",
    "# 2) A few extreme outliers in revenue\n",
    "outlier_idx = rng.choice(df.index, size=max(3, int(0.01 * n_rows)), replace=False)\n",
    "df.loc[outlier_idx, 'revenue'] *= rng.integers(6, 12, size=len(outlier_idx))\n",
    "\n",
    "# 3) Duplicate a couple of rows\n",
    "dup_idx = rng.choice(df.index, size=3, replace=False)\n",
    "df = pd.concat([df, df.loc[dup_idx]], ignore_index=True)\n",
    "\n",
    "# 4) Inconsistent categories (uppercase + extra spaces)\n",
    "glitch_idx = rng.choice(df.index, size=4, replace=False)\n",
    "df.loc[glitch_idx, 'channel'] = df.loc[glitch_idx, 'channel'].astype(str).str.upper() + '  '\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52fe95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.1 EDA Workflow (The Checklist)\n",
    "\n",
    "A good EDA is **repeatable**. Here is a beginner-friendly workflow you can follow every time you start exploring a new dataset:\n",
    "\n",
    "### The 9-Step EDA Checklist\n",
    "\n",
    "| Step | Action | Key Questions |\n",
    "|------|--------|---------------|\n",
    "| 1 | **Clarify the goal** | What question are you trying to answer? |\n",
    "| 2 | **Inspect the dataset** | How many rows/columns? What types? Any missing values? |\n",
    "| 3 | **Clean just enough** | Fix obvious issues so exploration is safe |\n",
    "| 4 | **Univariate analysis** | What does each variable look like on its own? |\n",
    "| 5 | **Bivariate analysis** | How are pairs of variables related? |\n",
    "| 6 | **Multivariate analysis** | What patterns emerge when looking at 3+ variables? |\n",
    "| 7 | **Patterns & anomalies** | Any outliers, weird clusters, or sudden spikes? |\n",
    "| 8 | **Refine hypotheses** | Update your questions based on what you found |\n",
    "| 9 | **Summarize insights** | What did you learn? What should happen next? |\n",
    "\n",
    "> âš ï¸ **Warning:** If you skip steps 1â€“3, you can easily misinterpret your plots and draw wrong conclusions.\n",
    "\n",
    "> ðŸ’¡ **Tip:** Print or save this checklist and refer to it when starting any new EDA project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c598bd",
   "metadata": {},
   "source": [
    "### Step 1â€“2: Quick Inspection (Shape, Samples, Types)\n",
    "\n",
    "**Why this matters:**\n",
    "- If a column is text but should be numeric, calculations will be wrong\n",
    "- If you have duplicates, counts and averages can be biased\n",
    "- If you have missing values, some charts will silently drop data\n",
    "\n",
    "**What to check:**\n",
    "1. **Shape** â€” How many rows and columns?\n",
    "2. **Sample rows** â€” What does the data actually look like?\n",
    "3. **Data types** â€” Are they correct? (dates as dates, numbers as numbers)\n",
    "4. **Missing values** â€” How much data is missing?\n",
    "5. **Duplicates** â€” Are there exact duplicate rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape: (rows, columns)\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a random sample of rows (not just the first few)\n",
    "# This helps you see variety in the data\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types - are they what you expect?\n",
    "# Good: order_date is datetime64, revenue is float64\n",
    "# Warning sign: a numeric column showing as 'object' (text)\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values and duplicates\n",
    "# Missing rate = proportion of NaN values in each column\n",
    "missing_rate = df.isna().mean().sort_values(ascending=False)\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "\n",
    "print(\"Missing rate by column (sorted):\")\n",
    "print(missing_rate[missing_rate > 0].to_frame('missing_rate'))\n",
    "print(f\"\\nColumns with no missing values: {(missing_rate == 0).sum()}\")\n",
    "print(f\"\\nExact duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Interpretation: \n",
    "# - satisfaction has ~3% missing, customer_age has ~2% missing\n",
    "# - There are 3 duplicate rows we should remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25fa13",
   "metadata": {},
   "source": [
    "### Step 3: Clean *Just Enough* for Exploration\n",
    "\n",
    "EDA is not the same as full data cleaning (Chapter 15 covers that in depth). Here we do **only what we need** so exploration is trustworthy:\n",
    "\n",
    "| Issue | EDA Fix | Why |\n",
    "|-------|---------|-----|\n",
    "| Inconsistent text | Standardize (`'WEB  '` â†’ `'web'`) | So groupby works correctly |\n",
    "| Exact duplicates | Remove them | So counts aren't inflated |\n",
    "| Missing values | **Keep them** | We'll handle them carefully in plots/stats |\n",
    "\n",
    "> âš ï¸ **Warning:** Don't over-clean during EDA. You want to see the problems so you can report them!\n",
    "\n",
    "> ðŸ’¡ **Tip:** Always print how many rows you remove, so you can explain it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for EDA (preserve the original for reference)\n",
    "df_eda = df.copy()\n",
    "\n",
    "# Fix 1: Standardize the 'channel' column\n",
    "# - Convert to string (in case of mixed types)\n",
    "# - Strip leading/trailing whitespace\n",
    "# - Convert to lowercase\n",
    "df_eda['channel'] = df_eda['channel'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Fix 2: Remove exact duplicate rows\n",
    "n_before = len(df_eda)\n",
    "df_eda = df_eda.drop_duplicates()\n",
    "n_after = len(df_eda)\n",
    "\n",
    "print(f\"Rows before cleanup: {n_before:,}\")\n",
    "print(f\"Rows after drop_duplicates(): {n_after:,}\")\n",
    "print(f\"Rows removed: {n_before - n_after:,}\")\n",
    "print(f\"\\nChannel values after cleanup:\")\n",
    "print(df_eda['channel'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reusable function for quick dataset overview\n",
    "# This is a handy function you can copy to your own projects!\n",
    "\n",
    "def quick_overview(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a quick overview of a DataFrame.\n",
    "    Shows data type, missing rate, and unique values for each column.\n",
    "    \"\"\"\n",
    "    overview = pd.DataFrame({\n",
    "        'dtype': data.dtypes.astype(str),\n",
    "        'missing_rate': data.isna().mean().round(4),\n",
    "        'missing_count': data.isna().sum(),\n",
    "        'n_unique': data.nunique(dropna=True),\n",
    "        'sample_value': [data[col].dropna().iloc[0] if len(data[col].dropna()) > 0 else None \n",
    "                         for col in data.columns]\n",
    "    })\n",
    "    return overview.sort_values('missing_rate', ascending=False)\n",
    "\n",
    "# Apply to our cleaned EDA dataset\n",
    "quick_overview(df_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457a1d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.2 Univariate Analysis (One Variable at a Time)\n",
    "\n",
    "**Univariate analysis** examines each variable independently. It answers questions like:\n",
    "- What values are common?\n",
    "- What is the typical value (mean/median)?\n",
    "- Is the distribution skewed?\n",
    "- Are there outliers?\n",
    "\n",
    "### Tools for Univariate Analysis\n",
    "\n",
    "| Variable Type | Summary Method | Visualization |\n",
    "|---------------|----------------|---------------|\n",
    "| **Numeric** | `describe()`, `mean()`, `median()` | Histogram, boxplot, KDE |\n",
    "| **Categorical** | `value_counts()` | Bar chart, pie chart |\n",
    "| **Boolean** | `mean()` (gives % True) | Bar chart |\n",
    "\n",
    "### Key Statistics to Know\n",
    "\n",
    "| Statistic | What It Tells You |\n",
    "|-----------|-------------------|\n",
    "| **Mean** | Average value (sensitive to outliers) |\n",
    "| **Median** | Middle value (robust to outliers) |\n",
    "| **Std** | How spread out the data is |\n",
    "| **Min/Max** | Range of values |\n",
    "| **25%/75%** | Quartiles (middle 50% of data) |\n",
    "\n",
    "> ðŸ’¡ **Tip:** When mean â‰  median, the distribution is skewed. If mean > median, there are high outliers pulling the average up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary using describe()\n",
    "# This gives you count, mean, std, min, 25%, 50% (median), 75%, max\n",
    "\n",
    "numeric_cols = ['customer_age', 'items', 'discount', 'revenue', 'satisfaction']\n",
    "summary = df_eda[numeric_cols].describe()\n",
    "\n",
    "print(\"Numeric column summary:\")\n",
    "print(summary.round(2))\n",
    "\n",
    "# Quick interpretation:\n",
    "# - revenue: mean (87) > median (63) suggests right-skewed with high outliers\n",
    "# - discount: ranges from 0 to ~0.5 (0% to 50%)\n",
    "# - satisfaction: centered around 7, scale 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numeric distributions\n",
    "# We use a 2x2 grid to show multiple plots at once\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Revenue histogram with KDE (smooth curve)\n",
    "sns.histplot(df_eda['revenue'], bins=40, kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Revenue Distribution')\n",
    "axes[0, 0].set_xlabel('Revenue ($)')\n",
    "\n",
    "# Plot 2: Revenue boxplot (great for spotting outliers)\n",
    "sns.boxplot(x=df_eda['revenue'], ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Revenue Boxplot (outliers visible as dots)')\n",
    "\n",
    "# Plot 3: Discount histogram\n",
    "sns.histplot(df_eda['discount'], bins=30, kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Discount Distribution')\n",
    "axes[1, 0].set_xlabel('Discount Rate')\n",
    "\n",
    "# Plot 4: Items histogram (discrete values)\n",
    "sns.histplot(df_eda['items'], bins=12, kde=False, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Items per Order')\n",
    "axes[1, 1].set_xlabel('Number of Items')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# - Revenue is right-skewed with several extreme outliers\n",
    "# - Most discounts are small (under 20%)\n",
    "# - Most orders have 1-4 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1de24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For highly skewed data, a log scale often reveals patterns better\n",
    "# Why log scale? It compresses large values and spreads out small values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(np.log10(df_eda['revenue']), bins=40, kde=True, color='steelblue')\n",
    "plt.title('Revenue on Log10 Scale')\n",
    "plt.xlabel('logâ‚â‚€(Revenue)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add a note about interpretation\n",
    "plt.annotate('10Â¹ = $10\\n10Â² = $100\\n10Â³ = $1000', \n",
    "             xy=(0.02, 0.75), xycoords='axes fraction',\n",
    "             fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now the distribution looks more symmetric and easier to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable analysis using value_counts()\n",
    "# Shows how many times each category appears\n",
    "\n",
    "print(\"Channel distribution:\")\n",
    "print(df_eda['channel'].value_counts(dropna=False))\n",
    "print(f\"\\nPercentages:\")\n",
    "print((df_eda['channel'].value_counts(normalize=True) * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical distribution with a bar chart\n",
    "# Order bars by frequency for easier reading\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Channel distribution\n",
    "channel_order = df_eda['channel'].value_counts().index\n",
    "sns.countplot(data=df_eda, x='channel', order=channel_order, ax=axes[0], palette='Blues_d')\n",
    "axes[0].set_title('Orders by Channel')\n",
    "axes[0].set_xlabel('Channel')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Region distribution\n",
    "region_order = df_eda['region'].value_counts().index\n",
    "sns.countplot(data=df_eda, x='region', order=region_order, ax=axes[1], palette='Greens_d')\n",
    "axes[1].set_title('Orders by Region')\n",
    "axes[1].set_xlabel('Region')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: Most orders come through web, followed by app, then store\n",
    "# Regions are roughly balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b5234",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 1: Univariate Analysis\n",
    "\n",
    "Test your understanding by answering these questions:\n",
    "\n",
    "1. What is the **median** revenue? (Hint: use `.median()`)\n",
    "2. What **percentage** of orders were returned? (Hint: for boolean columns, `.mean()` gives the rate of `True`)\n",
    "3. Which **region** has the most orders? (Hint: use `.value_counts()`)\n",
    "\n",
    "**Try it yourself before looking at the solution below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c150d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "# 1. Median revenue\n",
    "median_revenue = df_eda['revenue'].median()\n",
    "print(f\"1. Median revenue: ${median_revenue:.2f}\")\n",
    "\n",
    "# 2. Return rate (percentage of orders returned)\n",
    "return_rate = df_eda['returned'].mean() * 100  # multiply by 100 for percentage\n",
    "print(f\"2. Return rate: {return_rate:.1f}%\")\n",
    "\n",
    "# 3. Region with most orders\n",
    "top_region = df_eda['region'].value_counts().idxmax()\n",
    "top_region_count = df_eda['region'].value_counts().max()\n",
    "print(f\"3. Top region: {top_region} ({top_region_count} orders)\")\n",
    "\n",
    "# Bonus: Show all region counts\n",
    "print(f\"\\nAll regions:\\n{df_eda['region'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40ba73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.3 Bivariate Analysis (Two Variables)\n",
    "\n",
    "**Bivariate analysis** examines the relationship between two variables. It helps you answer questions like:\n",
    "- Does revenue differ by channel?\n",
    "- Is discount related to returns?\n",
    "- Do older customers buy more items?\n",
    "\n",
    "### Tools for Bivariate Analysis\n",
    "\n",
    "| Comparison Type | Method | Visualization |\n",
    "|-----------------|--------|---------------|\n",
    "| **Numeric vs Categorical** | `groupby().agg()` | Boxplot, violin plot, bar chart |\n",
    "| **Numeric vs Numeric** | Correlation, scatter | Scatter plot, regression plot |\n",
    "| **Categorical vs Categorical** | Crosstab, chi-square | Heatmap, grouped bar chart |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Correlation** measures the linear relationship between two numeric variables:\n",
    "- **+1** = Perfect positive relationship (both increase together)\n",
    "- **0** = No linear relationship\n",
    "- **-1** = Perfect negative relationship (one increases, other decreases)\n",
    "\n",
    "> âš ï¸ **Common Mistake:** Seeing a trend and assuming it proves causation. EDA tells you **what is associated**, not **why** it happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare revenue across channels using groupby\n",
    "# agg() lets us calculate multiple statistics at once\n",
    "\n",
    "revenue_by_channel = df_eda.groupby('channel')['revenue'].agg([\n",
    "    ('count', 'count'),\n",
    "    ('mean', 'mean'),\n",
    "    ('median', 'median'),\n",
    "    ('std', 'std')\n",
    "]).sort_values('median', ascending=False)\n",
    "\n",
    "print(\"Revenue statistics by channel:\")\n",
    "print(revenue_by_channel.round(2))\n",
    "\n",
    "# Interpretation: Store has the highest median revenue, \n",
    "# but fewer orders than web/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numeric vs categorical: boxplot\n",
    "# Boxplots show median, quartiles, and outliers at a glance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df_eda, x='channel', y='revenue', palette='Set2')\n",
    "plt.title('Revenue by Channel (Boxplot)')\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Revenue ($)')\n",
    "\n",
    "# Limit y-axis to see the main distribution (outliers still visible as dots)\n",
    "plt.ylim(0, df_eda['revenue'].quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: Store orders tend to have higher revenue\n",
    "# All channels have some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a22743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numeric vs numeric: scatter plot\n",
    "# We sample 500 points to avoid overplotting\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "sample = df_eda.sample(500, random_state=0)\n",
    "sns.scatterplot(data=sample, x='discount', y='revenue', hue='channel', alpha=0.7, s=60)\n",
    "plt.title('Discount vs Revenue (by Channel)')\n",
    "plt.xlabel('Discount Rate')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.legend(title='Channel')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: Higher discounts are associated with lower revenue\n",
    "# (this makes sense: discounts reduce price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621857a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket a numeric variable to compare with another variable\n",
    "# This helps when you want to see trends across ranges\n",
    "\n",
    "# Create discount buckets (bins)\n",
    "df_tmp = df_eda.copy()\n",
    "df_tmp['discount_bucket'] = pd.cut(\n",
    "    df_tmp['discount'], \n",
    "    bins=[0, 0.05, 0.10, 0.20, 0.60],  # 0-5%, 5-10%, 10-20%, 20-60%\n",
    "    include_lowest=True,\n",
    "    labels=['0-5%', '5-10%', '10-20%', '20-60%']\n",
    ")\n",
    "\n",
    "# Calculate return rate for each bucket\n",
    "return_rate_by_bucket = df_tmp.groupby('discount_bucket', observed=False)['returned'].mean()\n",
    "\n",
    "print(\"Return rate by discount bucket:\")\n",
    "print((return_rate_by_bucket * 100).round(1).to_frame('return_rate_%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the return rate by discount bucket\n",
    "plt.figure(figsize=(8, 4))\n",
    "(return_rate_by_bucket * 100).plot(kind='bar', color='coral', edgecolor='black')\n",
    "plt.ylabel('Return Rate (%)')\n",
    "plt.xlabel('Discount Bucket')\n",
    "plt.title('Return Rate Increases with Higher Discounts')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key finding: Orders with higher discounts have MUCH higher return rates\n",
    "# This is a pattern worth investigating further!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ce13c",
   "metadata": {},
   "source": [
    "> âš ï¸ **Critical Reminder:** We found that higher discounts are *associated* with higher returns. But this does NOT prove that discounts *cause* returns. There could be other factors:\n",
    "> - Maybe discounted products are lower quality\n",
    "> - Maybe customers who seek discounts are more likely to return things anyway\n",
    "> \n",
    "> EDA finds patterns; further analysis tests explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be6fa2",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 2: Bivariate Analysis\n",
    "\n",
    "Answer these questions using what you've learned:\n",
    "\n",
    "1. Which channel has the **highest median revenue**?\n",
    "2. Is the **return rate** higher for web or app?\n",
    "3. Create a **boxplot** of satisfaction vs returned status.\n",
    "\n",
    "**Try it yourself before looking at the solution!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13569177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "# 1. Median revenue by channel\n",
    "median_by_channel = df_eda.groupby('channel')['revenue'].median().sort_values(ascending=False)\n",
    "print(\"1. Median revenue by channel:\")\n",
    "print(median_by_channel.round(2))\n",
    "print(f\"\\n   â†’ Highest: {median_by_channel.idxmax()} (${median_by_channel.max():.2f})\")\n",
    "\n",
    "# 2. Return rate by channel\n",
    "return_rate_by_channel = df_eda.groupby('channel')['returned'].mean().sort_values(ascending=False)\n",
    "print(\"\\n2. Return rate by channel:\")\n",
    "print((return_rate_by_channel * 100).round(1))\n",
    "print(f\"\\n   â†’ Web return rate: {return_rate_by_channel['web']*100:.1f}%\")\n",
    "print(f\"   â†’ App return rate: {return_rate_by_channel['app']*100:.1f}%\")\n",
    "\n",
    "# 3. Boxplot of satisfaction vs returned\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(data=df_eda, x='returned', y='satisfaction', palette='pastel')\n",
    "plt.title('Satisfaction Score by Return Status')\n",
    "plt.xlabel('Order Returned?')\n",
    "plt.ylabel('Satisfaction Score (1-10)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: Customers who returned orders have lower satisfaction scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a03d91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.4 Multivariate Analysis (3+ Variables)\n",
    "\n",
    "**Multivariate analysis** examines relationships among three or more variables. This is useful when relationships are more complex and depend on multiple factors.\n",
    "\n",
    "### Examples of Multivariate Questions:\n",
    "- Does revenue depend on items **and** channel?\n",
    "- Does the discount-return relationship vary **by channel**?\n",
    "- Are satisfaction patterns different **by region and return status**?\n",
    "\n",
    "### Tools for Multivariate Analysis\n",
    "\n",
    "| Tool | Use Case |\n",
    "|------|----------|\n",
    "| **Correlation matrix** | See relationships between all numeric pairs |\n",
    "| **Heatmap** | Visualize correlation or pivot tables |\n",
    "| **Pivot tables** | Cross-tabulate by two categorical variables |\n",
    "| **Pair plots** | Scatter plots for all variable pairs |\n",
    "| **Faceted plots** | Same plot repeated for each group |\n",
    "\n",
    "> ðŸ’¡ **Tip:** A correlation matrix is a great first step in multivariate analysisâ€”it quickly shows which variables are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27626a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for all numeric columns\n",
    "# Correlation ranges from -1 (negative) to +1 (positive)\n",
    "\n",
    "numeric_cols = ['customer_age', 'items', 'discount', 'revenue', 'satisfaction']\n",
    "corr = df_eda[numeric_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr.round(2))\n",
    "\n",
    "# Reading the matrix:\n",
    "# - Each cell shows correlation between row and column variables\n",
    "# - Diagonal is always 1.0 (variable correlated with itself)\n",
    "# - Look for values close to 1 or -1 for strong relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddad1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix as a heatmap\n",
    "# Colors make patterns easier to spot\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr, \n",
    "    annot=True,           # Show numbers in cells\n",
    "    fmt='.2f',            # Two decimal places\n",
    "    cmap='vlag',          # Red-blue color scheme\n",
    "    center=0,             # Center color at 0\n",
    "    vmin=-1, vmax=1,      # Full correlation range\n",
    "    square=True,          # Square cells\n",
    "    linewidths=0.5        # Cell borders\n",
    ")\n",
    "plt.title('Correlation Heatmap (Numeric Variables)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key findings:\n",
    "# - items and revenue have positive correlation (more items = more revenue)\n",
    "# - discount and revenue have negative correlation (discounts reduce revenue)\n",
    "# - customer_age has weak correlations with other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58061794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: cross-tabulate two categorical variables\n",
    "# Here: return rate by channel AND region\n",
    "\n",
    "pivot_return = pd.pivot_table(\n",
    "    df_eda,\n",
    "    index='channel',         # Rows\n",
    "    columns='region',        # Columns\n",
    "    values='returned',       # What to measure\n",
    "    aggfunc='mean'           # How to aggregate (mean of boolean = rate)\n",
    ")\n",
    "\n",
    "print(\"Return rate by channel and region:\")\n",
    "print((pivot_return * 100).round(1))\n",
    "\n",
    "# This shows: does the return rate pattern differ by region within each channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pivot table as a heatmap\n",
    "plt.figure(figsize=(9, 4))\n",
    "sns.heatmap(\n",
    "    pivot_return * 100,   # Convert to percentage\n",
    "    annot=True, \n",
    "    fmt='.1f', \n",
    "    cmap='Blues',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Return Rate (%)'}\n",
    ")\n",
    "plt.title('Return Rate (%) by Channel and Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Channel')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: Look for cells with unusually high or low values\n",
    "# These could indicate issues or opportunities specific to channel-region combos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4371120",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 4: Multivariate Analysis\n",
    "\n",
    "Practice multivariate exploration:\n",
    "\n",
    "1. Create a **pivot table** showing average satisfaction by `channel` and `returned` status\n",
    "2. Visualize it as a **heatmap**\n",
    "3. What pattern do you observe?\n",
    "\n",
    "**Hint:** Use `pd.pivot_table()` with `values='satisfaction'` and `aggfunc='mean'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 4\n",
    "\n",
    "# 1. Create pivot table of satisfaction by channel and returned status\n",
    "pivot_satisfaction = pd.pivot_table(\n",
    "    df_eda,\n",
    "    index='channel',\n",
    "    columns='returned',\n",
    "    values='satisfaction',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot_satisfaction.columns = ['Not Returned', 'Returned']\n",
    "\n",
    "print(\"Average Satisfaction by Channel and Return Status:\")\n",
    "print(pivot_satisfaction.round(2))\n",
    "\n",
    "# 2. Visualize as heatmap\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(\n",
    "    pivot_satisfaction.round(2),\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn',  # Red-Yellow-Green (low to high)\n",
    "    linewidths=0.5,\n",
    "    vmin=5, vmax=8,  # Satisfaction scale context\n",
    "    cbar_kws={'label': 'Avg Satisfaction (1-10)'}\n",
    ")\n",
    "plt.title('Average Satisfaction by Channel and Return Status')\n",
    "plt.xlabel('Return Status')\n",
    "plt.ylabel('Channel')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Pattern observation:\n",
    "print(\"\\nðŸ“Š Pattern Observed:\")\n",
    "print(\"- Returned orders have lower satisfaction (~1 point lower) across ALL channels\")\n",
    "print(\"- Store customers are slightly more satisfied overall\")\n",
    "print(\"- The return-satisfaction gap is consistent regardless of channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8003cc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.5 Visual and Statistical Exploration\n",
    "\n",
    "Visuals help you see patterns quickly. Simple statistics help you **quantify** what you see. The best EDA uses both together.\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Goal | Visual Tool | Statistical Tool |\n",
    "|------|-------------|------------------|\n",
    "| Compare distributions | Violin plot, histogram | Mean, median, std |\n",
    "| Compare categories | Bar chart, boxplot | Group means, counts |\n",
    "| Check for differences | Side-by-side plots | t-test, chi-square |\n",
    "| Find relationships | Scatter plot | Correlation coefficient |\n",
    "\n",
    "> ðŸ’¡ **Tip:** Always pair a visual with a number. \"Revenue is higher for store\" is better as \"Store median revenue ($85) is 35% higher than web ($63).\"\n",
    "\n",
    "### Two EDA Examples:\n",
    "1. **Compare revenue distributions** across channels\n",
    "2. **Compare return rates** between web and app with a chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac91d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Compare revenue distributions with violin plot\n",
    "# Violin plots show the full distribution shape (like a sideways histogram)\n",
    "\n",
    "# Remove extreme outliers for readability (keep 98% of data)\n",
    "trimmed = df_eda[df_eda['revenue'] <= df_eda['revenue'].quantile(0.98)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.violinplot(data=trimmed, x='channel', y='revenue', inner='quartile', palette='Set2')\n",
    "plt.title('Revenue Distribution by Channel (Violin Plot)')\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show the statistics\n",
    "print(\"Revenue statistics by channel:\")\n",
    "print(trimmed.groupby('channel')['revenue'].describe()[['count', 'mean', '50%', 'std']].round(2))\n",
    "\n",
    "# The violin shape shows:\n",
    "# - Store has higher revenues overall\n",
    "# - Web has a longer tail of higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Compare return rates with a contingency table (crosstab)\n",
    "# A crosstab shows counts of each combination of two categories\n",
    "\n",
    "# Focus on web vs app\n",
    "channels_of_interest = df_eda[df_eda['channel'].isin(['web', 'app'])]\n",
    "\n",
    "# Create the crosstab\n",
    "table = pd.crosstab(\n",
    "    channels_of_interest['channel'], \n",
    "    channels_of_interest['returned'],\n",
    "    margins=True  # Add row/column totals\n",
    ")\n",
    "table.columns = ['Not Returned', 'Returned', 'Total']\n",
    "print(\"Contingency Table: Channel vs Returned\")\n",
    "print(table)\n",
    "\n",
    "# This shows the raw counts - useful for understanding sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Chi-square test to check if the difference is statistically significant\n",
    "# Chi-square tests whether two categorical variables are independent\n",
    "\n",
    "if SCIPY_AVAILABLE:\n",
    "    # Create the table without margins for the test\n",
    "    table_for_test = pd.crosstab(channels_of_interest['channel'], channels_of_interest['returned'])\n",
    "    \n",
    "    # Run chi-square test\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(table_for_test)\n",
    "    \n",
    "    print(\"Chi-Square Test Results:\")\n",
    "    print(f\"  Chi-square statistic: {chi2:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Degrees of freedom: {dof}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  â†’ The difference IS statistically significant (p < 0.05)\")\n",
    "        print(\"  â†’ Channel and return status are NOT independent\")\n",
    "    else:\n",
    "        print(\"  â†’ The difference is NOT statistically significant (p >= 0.05)\")\n",
    "        print(\"  â†’ We cannot conclude that channel affects returns\")\n",
    "    \n",
    "    print(\"\\nExpected counts (if no relationship existed):\")\n",
    "    print(pd.DataFrame(expected.round(1), index=table_for_test.index, columns=table_for_test.columns))\n",
    "else:\n",
    "    print(\"SciPy not available: skipping chi-square test.\")\n",
    "    print(\"The crosstab above is still useful for EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d16f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.6 Pattern and Anomaly Detection\n",
    "\n",
    "**Anomalies** (also called outliers) are unusual data points. They can be:\n",
    "- **Real events** â€” A very large order from a corporate customer\n",
    "- **Data errors** â€” Someone accidentally added an extra zero\n",
    "- **Rare but important cases** â€” Fraud, system issues, special promotions\n",
    "\n",
    "### Why Detect Anomalies?\n",
    "- They can **skew your statistics** (mean, correlation)\n",
    "- They might represent **errors** that need fixing\n",
    "- They could be **the most important insights** (fraud, opportunities)\n",
    "\n",
    "### Common Anomaly Detection Methods\n",
    "\n",
    "| Method | How It Works | Best For |\n",
    "|--------|--------------|----------|\n",
    "| **IQR Rule** | Flag values > Q3 + 1.5Ã—IQR or < Q1 - 1.5Ã—IQR | Simple, robust |\n",
    "| **Z-score** | Flag values > 2 or 3 standard deviations from mean | Normal distributions |\n",
    "| **Visual inspection** | Look at boxplots, scatter plots | Finding patterns |\n",
    "| **Time-based** | Look for unusual spikes in time series | Temporal data |\n",
    "\n",
    "> âš ï¸ **Warning:** An outlier is not automatically wrong. Always investigate before removing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR (Interquartile Range) method for outlier detection\n",
    "# This is a robust method that works well for skewed data\n",
    "\n",
    "def iqr_outliers(series: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: Column to check\n",
    "    - k: Multiplier (1.5 = standard, 3.0 = extreme only)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean series: True = outlier\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - k * iqr\n",
    "    upper = q3 + k * iqr\n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "# Apply to revenue column\n",
    "rev_outlier_mask = iqr_outliers(df_eda['revenue'], k=1.5)\n",
    "\n",
    "print(f\"Revenue outliers detected (IQR method):\")\n",
    "print(f\"  Total flagged: {rev_outlier_mask.sum():,} out of {len(df_eda):,} rows ({rev_outlier_mask.mean()*100:.1f}%)\")\n",
    "\n",
    "# Show the top outliers\n",
    "print(\"\\nTop 10 revenue outliers:\")\n",
    "df_eda.loc[rev_outlier_mask, ['order_id', 'order_date', 'channel', 'items', 'discount', 'revenue']]\\\n",
    "    .sort_values('revenue', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers in context\n",
    "# This helps you understand if outliers are random or follow a pattern\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    data=df_eda.assign(is_outlier=rev_outlier_mask),\n",
    "    x='items',\n",
    "    y='revenue',\n",
    "    hue='is_outlier',\n",
    "    palette={True: 'red', False: 'steelblue'},\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "plt.title('Revenue Outliers Highlighted (Red = Outlier)')\n",
    "plt.xlabel('Number of Items')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation: The outliers mostly have high items AND high revenue\n",
    "# This makes sense - they might be bulk orders, not errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based anomaly detection\n",
    "# Aggregate data by day and look for unusual spikes\n",
    "\n",
    "# Create daily revenue totals\n",
    "daily = (\n",
    "    df_eda\n",
    "    .assign(day=lambda d: d['order_date'].dt.floor('D'))\n",
    "    .groupby('day', as_index=False)\n",
    "    .agg({\n",
    "        'revenue': 'sum',\n",
    "        'order_id': 'count'\n",
    "    })\n",
    "    .rename(columns={'order_id': 'order_count'})\n",
    ")\n",
    "\n",
    "# Calculate 7-day rolling mean and standard deviation\n",
    "daily['rolling_mean_7'] = daily['revenue'].rolling(7, min_periods=1).mean()\n",
    "daily['rolling_std_7'] = daily['revenue'].rolling(7, min_periods=1).std().fillna(0)\n",
    "\n",
    "# Calculate z-score (how many std deviations from rolling mean)\n",
    "daily['z_score'] = (daily['revenue'] - daily['rolling_mean_7']) / daily['rolling_std_7'].replace(0, np.nan)\n",
    "\n",
    "# Plot daily revenue with rolling mean\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(daily['day'], daily['revenue'], label='Daily Revenue', alpha=0.7)\n",
    "plt.plot(daily['day'], daily['rolling_mean_7'], label='7-Day Rolling Mean', color='red', linewidth=2)\n",
    "plt.fill_between(\n",
    "    daily['day'],\n",
    "    daily['rolling_mean_7'] - 2 * daily['rolling_std_7'],\n",
    "    daily['rolling_mean_7'] + 2 * daily['rolling_std_7'],\n",
    "    alpha=0.2, color='red', label='Â±2 Std Dev Band'\n",
    ")\n",
    "plt.title('Daily Revenue Over Time (with Anomaly Detection Band)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Daily Revenue ($)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show days with unusually high revenue\n",
    "print(\"Days with highest revenue:\")\n",
    "print(daily.sort_values('revenue', ascending=False).head(5)[['day', 'revenue', 'order_count', 'z_score']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29630899",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 3: Anomaly Detection\n",
    "\n",
    "Practice your anomaly detection skills:\n",
    "\n",
    "1. Use the `iqr_outliers` function to flag outliers in the `discount` column\n",
    "2. Count how many outliers you found\n",
    "3. Show the top 5 rows with the highest discount\n",
    "\n",
    "**Bonus:** Try using `k=2.0` instead of `k=1.5` and compare the results. What changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768453a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "# 1 & 2. Flag discount outliers and count them\n",
    "discount_outliers_15 = iqr_outliers(df_eda['discount'], k=1.5)\n",
    "discount_outliers_20 = iqr_outliers(df_eda['discount'], k=2.0)\n",
    "\n",
    "print(\"Discount Outliers:\")\n",
    "print(f\"  With k=1.5: {discount_outliers_15.sum()} outliers ({discount_outliers_15.mean()*100:.1f}%)\")\n",
    "print(f\"  With k=2.0: {discount_outliers_20.sum()} outliers ({discount_outliers_20.mean()*100:.1f}%)\")\n",
    "\n",
    "# 3. Show top 5 highest discounts\n",
    "print(\"\\nTop 5 orders with highest discount:\")\n",
    "print(df_eda.loc[discount_outliers_15, ['order_id', 'channel', 'items', 'discount', 'revenue', 'returned']]\\\n",
    "    .sort_values('discount', ascending=False).head(5))\n",
    "\n",
    "# Bonus interpretation:\n",
    "# - Using k=2.0 is stricter (fewer outliers flagged)\n",
    "# - k=1.5 is the standard \"mild outlier\" threshold\n",
    "# - k=3.0 would catch only extreme outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e1b6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16.7 Hypothesis Refinement\n",
    "\n",
    "A powerful EDA habit is to **write down hypotheses**, test them informally, and **refine them** based on evidence.\n",
    "\n",
    "### The Hypothesis Refinement Process\n",
    "\n",
    "```\n",
    "Vague Idea â†’ Specific Hypothesis â†’ Test with Data â†’ Refine or Reject\n",
    "```\n",
    "\n",
    "### Example:\n",
    "\n",
    "| Stage | Statement |\n",
    "|-------|-----------|\n",
    "| **Vague idea** | \"Discounts might cause more returns\" |\n",
    "| **Specific hypothesis** | \"Orders with discount â‰¥ 10% have higher return rate than orders with discount < 10%\" |\n",
    "| **Refined hypothesis** | \"The discount-return relationship may differ by channel\" |\n",
    "| **Next step** | \"Need to control for product category (not available in this dataset)\" |\n",
    "\n",
    "### Why Refine Hypotheses?\n",
    "- **Clarifies what to measure** â€” return rate, not just \"returns\"\n",
    "- **Clarifies the comparison** â€” above/below 10% threshold\n",
    "- **Suggests subgroups to check** â€” by channel, by region\n",
    "- **Identifies data gaps** â€” what else would we need?\n",
    "\n",
    "> ðŸ’¡ **Tip:** Good EDA is like a conversation with your data. You ask questions, get answers, then ask better questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a refined hypothesis: \n",
    "# \"High-discount orders have higher return rates, and this varies by channel\"\n",
    "\n",
    "# Create binary flag for high discount (â‰¥10%)\n",
    "df_h = df_eda.copy()\n",
    "df_h['high_discount'] = df_h['discount'] >= 0.10\n",
    "\n",
    "# Overall return rate by discount group\n",
    "overall = df_h.groupby('high_discount')['returned'].agg(['mean', 'count'])\n",
    "overall.index = ['Low Discount (<10%)', 'High Discount (â‰¥10%)']\n",
    "overall.columns = ['Return Rate', 'Count']\n",
    "overall['Return Rate'] = (overall['Return Rate'] * 100).round(1)\n",
    "\n",
    "print(\"Overall Return Rate by Discount Group:\")\n",
    "print(overall)\n",
    "\n",
    "# Return rate by channel AND discount group\n",
    "by_channel = (\n",
    "    df_h.groupby(['channel', 'high_discount'])['returned']\n",
    "    .agg(['mean', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "by_channel['mean'] = (by_channel['mean'] * 100).round(1)\n",
    "by_channel.columns = ['Channel', 'High Discount', 'Return Rate %', 'Count']\n",
    "\n",
    "print(\"\\nReturn Rate by Channel and Discount Group:\")\n",
    "pivot = by_channel.pivot(index='Channel', columns='High Discount', values='Return Rate %')\n",
    "pivot.columns = ['Low Discount (<10%)', 'High Discount (â‰¥10%)']\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hypothesis test results\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=by_channel, \n",
    "    x='Channel', \n",
    "    y='Return Rate %', \n",
    "    hue='High Discount',\n",
    "    palette=['lightblue', 'coral']\n",
    ")\n",
    "plt.title('Return Rate by Channel and Discount Level')\n",
    "plt.ylabel('Return Rate (%)')\n",
    "plt.xlabel('Channel')\n",
    "plt.legend(title='Discount Level', labels=['Low (<10%)', 'High (â‰¥10%)'])\n",
    "\n",
    "# Add a horizontal line for overall average\n",
    "overall_avg = df_h['returned'].mean() * 100\n",
    "plt.axhline(y=overall_avg, color='gray', linestyle='--', linewidth=1, label=f'Overall: {overall_avg:.1f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# - High discounts are associated with higher return rates across ALL channels\n",
    "# - The effect is consistent, supporting our hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be52ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Mini-Project: EDA Summary Report\n",
    "\n",
    "Imagine a manager asks you:\n",
    "> \"We need to understand our order data better. Which channel is most valuable? Are discounts causing problems? Anything unusual we should investigate?\"\n",
    "\n",
    "**Your Task:** Use what you've learned to create a brief EDA summary. Specifically:\n",
    "\n",
    "1. Calculate **median revenue by channel** (which channel brings in more per order?)\n",
    "2. Calculate **return rate by discount bucket** (are discounts related to returns?)\n",
    "3. **Flag revenue outliers** and count them (any unusual orders?)\n",
    "4. Write **3-5 bullet point insights** in plain English\n",
    "\n",
    "This simulates real work â€” you'll do the analysis, then summarize it for a non-technical audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02882c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Project Solution\n",
    "\n",
    "# 1. Median revenue by channel\n",
    "median_rev = df_eda.groupby('channel')['revenue'].median().sort_values(ascending=False)\n",
    "print(\"=\" * 50)\n",
    "print(\"EDA SUMMARY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ“Š MEDIAN REVENUE BY CHANNEL:\")\n",
    "for channel, revenue in median_rev.items():\n",
    "    print(f\"   {channel.capitalize():8} ${revenue:.2f}\")\n",
    "\n",
    "# 2. Return rate by discount bucket\n",
    "df_mp = df_eda.copy()\n",
    "df_mp['discount_bucket'] = pd.cut(\n",
    "    df_mp['discount'], \n",
    "    bins=[0, 0.05, 0.10, 0.20, 0.60], \n",
    "    include_lowest=True,\n",
    "    labels=['0-5%', '5-10%', '10-20%', '20-60%']\n",
    ")\n",
    "return_by_bucket = df_mp.groupby('discount_bucket', observed=False)['returned'].mean()\n",
    "\n",
    "print(\"\\nðŸ“ˆ RETURN RATE BY DISCOUNT LEVEL:\")\n",
    "for bucket, rate in return_by_bucket.items():\n",
    "    print(f\"   {str(bucket):10} {rate*100:.1f}%\")\n",
    "\n",
    "# 3. Revenue outliers\n",
    "rev_outliers = iqr_outliers(df_eda['revenue'], k=1.5)\n",
    "n_outliers = rev_outliers.sum()\n",
    "total_outlier_rev = df_eda.loc[rev_outliers, 'revenue'].sum()\n",
    "\n",
    "print(f\"\\nâš ï¸  REVENUE OUTLIERS:\")\n",
    "print(f\"   {n_outliers} unusual orders flagged ({n_outliers/len(df_eda)*100:.1f}% of total)\")\n",
    "print(f\"   Combined revenue: ${total_outlier_rev:,.2f}\")\n",
    "\n",
    "# 4. Key insights summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"KEY INSIGHTS (for stakeholders)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. STORE CHANNEL IS MOST VALUABLE PER ORDER\n",
    "   â†’ Store orders have 35% higher median revenue than web/app\n",
    "   â†’ Consider strategies to drive more in-store traffic\n",
    "\n",
    "2. HIGH DISCOUNTS CORRELATE WITH HIGH RETURNS\n",
    "   â†’ Orders with 20%+ discount have ~50% return rate\n",
    "   â†’ Review discount policy - may be attracting wrong customers\n",
    "\n",
    "3. WEB HAS HIGHEST VOLUME BUT LOWER VALUE\n",
    "   â†’ 55% of orders come through web, but lower per-order revenue\n",
    "   â†’ Opportunity: upsell/cross-sell on web platform\n",
    "\n",
    "4. UNUSUAL LARGE ORDERS EXIST\n",
    "   â†’ ~4% of orders are outliers (unusually high revenue)\n",
    "   â†’ Investigate: are these bulk orders? corporate accounts?\n",
    "\n",
    "5. CUSTOMER SATISFACTION TIED TO RETURNS\n",
    "   â†’ Customers who return orders rate satisfaction ~1 point lower\n",
    "   â†’ Improving product quality/description may reduce returns\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d935789",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary / Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "âœ… **EDA is a method, not random plotting.** Follow a consistent workflow to ensure you don't miss important findings.\n",
    "\n",
    "âœ… **Start with inspection and minimal cleanup.** Understand your data before diving into analysis.\n",
    "\n",
    "âœ… **Progress from simple to complex:** Univariate â†’ Bivariate â†’ Multivariate analysis.\n",
    "\n",
    "âœ… **Combine visuals with statistics.** Plots help you see patterns; statistics help you quantify them.\n",
    "\n",
    "âœ… **Outliers are leads to investigate** â€” not automatically errors to remove.\n",
    "\n",
    "âœ… **EDA helps refine hypotheses** into precise, testable questions for later validation.\n",
    "\n",
    "âœ… **Correlation â‰  Causation.** EDA finds associations; proving causes requires further analysis.\n",
    "\n",
    "### The EDA Checklist (Quick Reference)\n",
    "\n",
    "1. â˜ Clarify the goal\n",
    "2. â˜ Inspect dataset (shape, types, missing)\n",
    "3. â˜ Clean just enough for exploration\n",
    "4. â˜ Univariate analysis\n",
    "5. â˜ Bivariate analysis\n",
    "6. â˜ Multivariate analysis\n",
    "7. â˜ Pattern and anomaly detection\n",
    "8. â˜ Refine hypotheses\n",
    "9. â˜ Summarize insights\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem | Solution |\n",
    "|---------|-------------------|----------|\n",
    "| Skipping inspection | Wrong data types â†’ wrong calculations | Always check `.dtypes` and `.isna()` first |\n",
    "| Over-cleaning early | You miss seeing real data issues | Clean minimally for EDA; deep clean later |\n",
    "| Assuming causation | Leads to wrong decisions | Note associations only; test causes separately |\n",
    "| Ignoring outliers | Miss important insights or errors | Flag and investigate, don't auto-remove |\n",
    "| No documentation | Can't reproduce or explain findings | Write notes as you go |\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- ðŸ“š [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) â€” Complete guide to data manipulation\n",
    "- ðŸ“Š [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html) â€” Statistical visualization with Seaborn\n",
    "- ðŸ“ˆ [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html) â€” Foundational plotting library\n",
    "\n",
    "### Recommended Reading\n",
    "- ðŸ“– *Python for Data Analysis* by Wes McKinney (Pandas creator)\n",
    "- ðŸ“– *Storytelling with Data* by Cole Nussbaumer Knaflic (visualization best practices)\n",
    "\n",
    "### Practice Datasets\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets) â€” Thousands of real-world datasets\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) â€” Classic datasets for analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a solid foundation in EDA methodology. Practice this workflow on new datasets to build your intuition."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
