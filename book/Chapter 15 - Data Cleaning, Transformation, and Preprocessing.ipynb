{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6f6ea",
   "metadata": {},
   "source": [
    "# Chapter 15: Data Cleaning, Transformation, and Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand **why data cleaning is essential** for accurate analysis\n",
    "- **Detect and handle missing values** using different strategies (drop, impute)\n",
    "- **Identify and treat outliers** using statistical methods (IQR, Z-score)\n",
    "- Perform **data consistency checks** (duplicates, standardization, range validation)\n",
    "- Apply **normalization and scaling** techniques to numeric features\n",
    "- **Encode categorical variables** for machine learning models\n",
    "- **Create new features** through transformation and feature engineering\n",
    "- Build **reusable preprocessing pipelines** with scikit-learn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28215b04",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Data cleaning** improves correctness and consistency (e.g., missing values, duplicates, wrong types).\n",
    "\n",
    "**Transformation & preprocessing** reshape your data so it works well with analysis or machine learning (e.g., scaling numbers, encoding categories).\n",
    "\n",
    "A simple mental model:\n",
    "1. **Understand the data** (columns, types, ranges, meaning)\n",
    "2. **Fix quality issues** (missing, invalid, inconsistent)\n",
    "3. **Prepare features** (scaling, encoding, transformations)\n",
    "4. **Validate** (re-check summaries and sanity checks)\n",
    "\n",
    "We’ll do all of this step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3238ce3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We’ll use these libraries:\n",
    "- `pandas`, `numpy` for data work\n",
    "- `matplotlib`, `seaborn` for quick visuals\n",
    "- `scikit-learn` for scaling and encoding examples\n",
    "\n",
    "If you don’t have them installed, run (in a terminal):\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ef419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b111fa",
   "metadata": {},
   "source": [
    "## Loading a practice dataset with real-world issues\n",
    "\n",
    "We'll use the **penguins** dataset from seaborn — a real dataset about penguin measurements that naturally contains:\n",
    "- Missing values (real measurement gaps)\n",
    "- Categorical variables that need encoding\n",
    "- Numeric features that need scaling\n",
    "\n",
    "We'll also add some typical data quality issues for practice:\n",
    "- Duplicate rows\n",
    "- Inconsistent categories (different spellings/case)\n",
    "- Some outliers\n",
    "\n",
    "This lets us practice cleaning techniques on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3448d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the penguins dataset - it has natural missing values!\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Create our practice dataframe with some additional issues\n",
    "df = penguins.copy()\n",
    "\n",
    "# Rename columns to match a business context\n",
    "df = df.rename(columns={\n",
    "    \"species\": \"segment\",\n",
    "    \"island\": \"city\", \n",
    "    \"bill_length_mm\": \"income\",\n",
    "    \"bill_depth_mm\": \"age\",\n",
    "    \"flipper_length_mm\": \"spent_last_30d\",\n",
    "    \"body_mass_g\": \"customer_id\"\n",
    "})\n",
    "\n",
    "# Add signup_date\n",
    "df[\"signup_date\"] = pd.to_datetime(\"2024-01-01\") + pd.to_timedelta(\n",
    "    np.random.randint(0, 365, size=len(df)), unit=\"D\"\n",
    ")\n",
    "\n",
    "# Introduce additional issues for practice:\n",
    "# 1) Inconsistent city names (already has some variation, add more)\n",
    "inconsistent_idx = np.random.choice(df.index, size=15, replace=False)\n",
    "df.loc[inconsistent_idx[:5], \"city\"] = \"torgersen\"  # lowercase\n",
    "df.loc[inconsistent_idx[5:10], \"city\"] = \"BISCOE\"   # uppercase  \n",
    "df.loc[inconsistent_idx[10:], \"city\"] = \"Dream \"    # extra space\n",
    "\n",
    "# 2) Introduce a few more outliers\n",
    "outlier_idx = np.random.choice(df.index[df[\"income\"].notna()], size=3, replace=False)\n",
    "df.loc[outlier_idx, \"income\"] = df[\"income\"].max(skipna=True) * 3\n",
    "\n",
    "outlier_idx_spend = np.random.choice(df.index[df[\"spent_last_30d\"].notna()], size=2, replace=False)\n",
    "df.loc[outlier_idx_spend, \"spent_last_30d\"] = df[\"spent_last_30d\"].max() * 2\n",
    "\n",
    "# 3) Introduce duplicates\n",
    "df = pd.concat([df, df.sample(5, random_state=7)], ignore_index=True)\n",
    "\n",
    "# 4) Store some ages as strings (wrong type)\n",
    "bad_type_idx = np.random.choice(df.index[df[\"age\"].notna()], size=6, replace=False)\n",
    "df.loc[bad_type_idx, \"age\"] = df.loc[bad_type_idx, \"age\"].astype(str)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Natural missing values from penguins dataset + added issues\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d5e45",
   "metadata": {},
   "source": [
    "## First look: inspect & summarize\n",
    "\n",
    "Before you clean anything, always do a quick inspection:\n",
    "- **Shape**: how many rows/columns?\n",
    "- **Types**: numbers vs text vs dates\n",
    "- **Summary stats**: min/max/mean to spot impossible values\n",
    "- **Missing values**: which columns are affected?\n",
    "\n",
    "This step prevents “blind cleaning” that can accidentally remove useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "display(df.dtypes)\n",
    "display(df.describe(include=\"all\").transpose())\n",
    "\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63b252",
   "metadata": {},
   "source": [
    "### Quick visual: missing values\n",
    "\n",
    "Tables are useful, but visuals make patterns obvious. A missingness heatmap can show:\n",
    "- If missing values are random\n",
    "- Or if they happen in blocks (e.g., a system failure on certain days)\n",
    "\n",
    "For small datasets, this is a quick and helpful check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a47b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(df.isna(), cbar=False)\n",
    "plt.title(\"Missing values (True = missing)\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d1296",
   "metadata": {},
   "source": [
    "## 15.1 Importance of data cleaning\n",
    "\n",
    "Data cleaning matters because:\n",
    "- **Wrong inputs → wrong outputs** (often called *garbage in, garbage out*)\n",
    "- Missing values can bias averages and models\n",
    "- Outliers can distort statistics and charts\n",
    "- Inconsistent categories create duplicate groups (e.g., “NYC” vs “New York”)\n",
    "- Duplicates can over-count customers or transactions\n",
    "\n",
    "A good goal is not “perfect” data — it’s **data that is accurate enough for the decision you want to make**, with clear assumptions.\n",
    "\n",
    "**Tip (beginner-friendly rule):** Always document what you changed and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fb3f2",
   "metadata": {},
   "source": [
    "## 15.2 Missing data handling strategies\n",
    "\n",
    "Missing data is common (forms left blank, sensor failures, system bugs). Before choosing a strategy, ask:\n",
    "- **How much is missing?** 1% vs 40% changes the decision.\n",
    "- **Why is it missing?** (random vs systematic)\n",
    "- **Is the column important?**\n",
    "\n",
    "### Common strategies\n",
    "1. **Drop rows** (only if few rows are missing, and missingness seems random)\n",
    "2. **Drop columns** (if the column is mostly missing and not critical)\n",
    "3. **Impute** (fill missing values):\n",
    "   - Numeric: mean/median\n",
    "   - Categorical: most frequent\n",
    "   - Time series: forward-fill/back-fill\n",
    "\n",
    "**Warning:** Imputation can hide real problems. Don’t fill blindly—understand the story behind the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'age' to numeric safely (coerce invalid values to NaN)\n",
    "df_clean = df.copy()\n",
    "df_clean[\"age\"] = pd.to_numeric(df_clean[\"age\"], errors=\"coerce\")\n",
    "\n",
    "df_clean[[\"age\", \"income\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86aeed",
   "metadata": {},
   "source": [
    "### Strategy A: drop rows with missing values (simple, but can remove data)\n",
    "\n",
    "We’ll drop rows where **age or income** is missing. This is okay *only* if:\n",
    "- The number of missing rows is small\n",
    "- You believe missingness is not systematic\n",
    "\n",
    "Let’s measure how many rows we’d lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = len(df_clean)\n",
    "df_dropna = df_clean.dropna(subset=[\"age\", \"income\"])\n",
    "rows_after = len(df_dropna)\n",
    "\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after dropna: {rows_after}\")\n",
    "print(f\"Rows removed: {rows_before - rows_after} ({(rows_before - rows_after)/rows_before:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c585e4b",
   "metadata": {},
   "source": [
    "### Strategy B: impute (fill) missing numeric values\n",
    "\n",
    "A common approach is to fill numeric missing values with the **median**.\n",
    "Why median?\n",
    "- It’s less sensitive to outliers than the mean.\n",
    "\n",
    "We’ll fill missing `age` and `income` with their medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3516085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df_clean.copy()\n",
    "\n",
    "for col in [\"age\", \"income\"]:\n",
    "    median_value = df_imputed[col].median()\n",
    "    df_imputed[col] = df_imputed[col].fillna(median_value)\n",
    "\n",
    "df_imputed[[\"age\", \"income\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea27b5",
   "metadata": {},
   "source": [
    "### Exercise 1 — Missing data\n",
    "1. Compute the % missing for each column.\n",
    "2. Try mean-imputation for `income` and compare mean vs median result.\n",
    "3. (Thinking) When would dropping rows be a bad idea?\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) % missing per column\n",
    "missing_pct = df_clean.isna().mean().sort_values(ascending=False) * 100\n",
    "display(missing_pct.to_frame(name=\"missing_%\"))\n",
    "\n",
    "# 2) Compare mean vs median imputation for income\n",
    "income_mean = df_clean[\"income\"].mean()\n",
    "income_median = df_clean[\"income\"].median()\n",
    "\n",
    "df_income_mean = df_clean.copy()\n",
    "df_income_median = df_clean.copy()\n",
    "\n",
    "df_income_mean[\"income\"] = df_income_mean[\"income\"].fillna(income_mean)\n",
    "df_income_median[\"income\"] = df_income_median[\"income\"].fillna(income_median)\n",
    "\n",
    "print(\"Income mean (after filling):\", df_income_mean[\"income\"].mean())\n",
    "print(\"Income median (after filling):\", df_income_median[\"income\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d52c9",
   "metadata": {},
   "source": [
    "## 15.3 Outlier detection and treatment\n",
    "\n",
    "An **outlier** is a value that is unusually far from most other values.\n",
    "\n",
    "Outliers happen for many reasons:\n",
    "- Real rare events (a very high-spending customer)\n",
    "- Data entry errors (extra zero: 5000 instead of 500)\n",
    "- Measurement problems\n",
    "\n",
    "### Why outliers matter\n",
    "- They can distort averages (mean) and correlation\n",
    "- They can stretch chart axes so normal values look “flat”\n",
    "- Some models are sensitive to them\n",
    "\n",
    "### Common detection methods\n",
    "- **Box plot / IQR rule** (good general method)\n",
    "- **Z-score** (works best if data is roughly normal)\n",
    "- **Domain rules** (e.g., age cannot be negative)\n",
    "\n",
    "We’ll use the IQR method for `income` and `spent_last_30d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_bounds(series, k=1.5):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - k * iqr\n",
    "    upper = q3 + k * iqr\n",
    "    return lower, upper\n",
    "\n",
    "for col in [\"income\", \"spent_last_30d\"]:\n",
    "    s = df_imputed[col]\n",
    "    lower, upper = iqr_bounds(s)\n",
    "    outliers = (s < lower) | (s > upper)\n",
    "    print(f\"{col}: {outliers.sum()} outliers (IQR rule). Bounds: [{lower:,.2f}, {upper:,.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe0a0",
   "metadata": {},
   "source": [
    "### Visualize outliers with box plots\n",
    "\n",
    "A box plot quickly shows median, quartiles, and points outside the typical range.\n",
    "We’ll plot `income` and `spent_last_30d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcc528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.boxplot(y=df_imputed[\"income\"], ax=axes[0])\n",
    "axes[0].set_title(\"Income (box plot)\")\n",
    "axes[0].set_ylabel(\"income\")\n",
    "\n",
    "sns.boxplot(y=df_imputed[\"spent_last_30d\"], ax=axes[1])\n",
    "axes[1].set_title(\"Spent last 30d (box plot)\")\n",
    "axes[1].set_ylabel(\"spent_last_30d\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c390d12",
   "metadata": {},
   "source": [
    "### Treatment options\n",
    "\n",
    "What should you do with outliers? It depends. Common options:\n",
    "1. **Fix obvious errors** (best if you can verify)\n",
    "2. **Remove outliers** (risky if they are real important cases)\n",
    "3. **Cap/Winsorize** (limit extreme values to a threshold)\n",
    "4. **Transform** (e.g., log transform income)\n",
    "\n",
    "We’ll demonstrate **capping** (winsorization-like) using IQR bounds.\n",
    "\n",
    "**Common mistake:** Automatically deleting outliers can remove exactly the customers you care about (e.g., high spenders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_capped = df_imputed.copy()\n",
    "\n",
    "for col in [\"income\", \"spent_last_30d\"]:\n",
    "    lower, upper = iqr_bounds(df_outlier_capped[col])\n",
    "    df_outlier_capped[col] = df_outlier_capped[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "df_outlier_capped[[\"income\", \"spent_last_30d\"]].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5532683",
   "metadata": {},
   "source": [
    "### Exercise 2 — Outliers\n",
    "1. Use the IQR rule to flag outliers in `age`.\n",
    "2. Try a log transform on `income` and re-plot a histogram (before vs after).\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Outliers in age\n",
    "lower_age, upper_age = iqr_bounds(df_outlier_capped[\"age\"])\n",
    "age_outliers = (df_outlier_capped[\"age\"] < lower_age) | (df_outlier_capped[\"age\"] > upper_age)\n",
    "print(\"Age outliers (IQR):\", age_outliers.sum())\n",
    "\n",
    "# 2) Log transform income and compare histograms\n",
    "income_raw = df_outlier_capped[\"income\"]\n",
    "income_log = np.log1p(income_raw)  # log(1 + x) avoids issues with zeros\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(income_raw, bins=30, ax=axes[0])\n",
    "axes[0].set_title(\"Income (raw)\")\n",
    "\n",
    "sns.histplot(income_log, bins=30, ax=axes[1])\n",
    "axes[1].set_title(\"Income (log1p)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59625db7",
   "metadata": {},
   "source": [
    "## 15.4 Data consistency checks\n",
    "\n",
    "Data consistency means the data follows expected rules. Examples:\n",
    "- No duplicate records for the same event\n",
    "- Categories are standardized (same spelling/case)\n",
    "- Numeric ranges make sense (age ≥ 0)\n",
    "- Dates are valid and in the expected range\n",
    "\n",
    "### Common checks in practice\n",
    "- **Duplicate rows**: `duplicated()`\n",
    "- **Invalid ranges**: filter with logical conditions\n",
    "- **Inconsistent categories**: normalize text (strip, lower) and map to standard values\n",
    "\n",
    "We’ll fix duplicates and standardize the `city` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from our \n",
    "# - numeric fixed (age coerced),\n",
    "# - missing handled (imputed),\n",
    "# - outliers treated (capped)\n",
    "df_consistent = df_outlier_capped.copy()\n",
    "\n",
    "# 1) Duplicates\n",
    "dup_count = df_consistent.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "df_consistent = df_consistent.drop_duplicates()\n",
    "print(\"Rows after dropping duplicates:\", len(df_consistent))\n",
    "\n",
    "# 2) Standardize city names\n",
    "def standardize_city(city):\n",
    "    if pd.isna(city):\n",
    "        return city\n",
    "    city_clean = str(city).strip().lower()\n",
    "    # Mapping for the penguins dataset island names (used as cities)\n",
    "    mapping = {\n",
    "        \"torgersen\": \"Torgersen\",\n",
    "        \"biscoe\": \"Biscoe\",\n",
    "        \"dream\": \"Dream\",\n",
    "    }\n",
    "    return mapping.get(city_clean, city.title())\n",
    "\n",
    "df_consistent[\"city\"] = df_consistent[\"city\"].apply(standardize_city)\n",
    "\n",
    "df_consistent[\"city\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3bbcc",
   "metadata": {},
   "source": [
    "### Range and logic checks (examples)\n",
    "\n",
    "Let’s add a couple of simple rules:\n",
    "- `age` should be between 0 and 110\n",
    "- `spent_last_30d` should be ≥ 0\n",
    "\n",
    "In real datasets, these rules come from domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_age = (df_consistent[\"age\"] < 0) | (df_consistent[\"age\"] > 110)\n",
    "invalid_spend = df_consistent[\"spent_last_30d\"] < 0\n",
    "\n",
    "print(\"Invalid ages:\", invalid_age.sum())\n",
    "print(\"Invalid spend values:\", invalid_spend.sum())\n",
    "\n",
    "# If any existed, you might correct them (if possible) or remove those rows\n",
    "df_consistent = df_consistent.loc[~(invalid_age | invalid_spend)].copy()\n",
    "df_consistent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72decec",
   "metadata": {},
   "source": [
    "### Exercise 3 — Consistency checks\n",
    "1. Standardize `segment` to uppercase (just in case).\n",
    "2. Create a check that finds customers with `income` below 0 (should be none).\n",
    "3. Print a small “data quality report” with counts of: missing values, duplicates, invalid ages.\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex3 = df_consistent.copy()\n",
    "\n",
    "# 1) Segment to uppercase\n",
    "df_ex3[\"segment\"] = df_ex3[\"segment\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# 2) Income below 0 check\n",
    "income_below_zero = (df_ex3[\"income\"] < 0).sum()\n",
    "print(\"Income below 0 count:\", income_below_zero)\n",
    "\n",
    "# 3) Simple data quality report\n",
    "quality_report = {\n",
    "    \"rows\": len(df_ex3),\n",
    "    \"missing_total\": int(df_ex3.isna().sum().sum()),\n",
    "    \"duplicates\": int(df_ex3.duplicated().sum()),\n",
    "    \"invalid_age\": int(((df_ex3[\"age\"] < 0) | (df_ex3[\"age\"] > 110)).sum()),\n",
    "}\n",
    "quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26996a4",
   "metadata": {},
   "source": [
    "## 15.5 Data normalization and scaling\n",
    "\n",
    "Many algorithms (and even some charts) work better when numeric features are on similar scales.\n",
    "\n",
    "Example: if `income` is in tens of thousands but `age` is ~30–50, then some models may focus too much on income simply because it has larger numbers.\n",
    "\n",
    "### Common scaling methods\n",
    "- **Standardization (Z-score)**: mean 0, standard deviation 1\n",
    "- **Min-Max scaling**: scales to [0, 1]\n",
    "- **Robust scaling**: uses median and IQR (good when outliers exist)\n",
    "\n",
    "We’ll demonstrate scaling with scikit-learn.\n",
    "\n",
    "**Tip:** Scale only the columns that need it (numeric features), and usually *after* handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1901f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "numeric_cols = [\"age\", \"income\", \"spent_last_30d\"]\n",
    "X_num = df_consistent[numeric_cols].copy()\n",
    "\n",
    "scalers = {\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "}\n",
    "\n",
    "scaled_examples = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled = scaler.fit_transform(X_num)\n",
    "    scaled_examples[name] = pd.DataFrame(scaled, columns=numeric_cols)\n",
    "\n",
    "# Compare summary stats\n",
    "display(pd.concat({k: v.describe().loc[[\"mean\", \"std\", \"min\", \"max\"]] for k, v in scaled_examples.items()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51570499",
   "metadata": {},
   "source": [
    "### Exercise 4 — Scaling\n",
    "1. Apply `RobustScaler` and make a scatter plot of scaled `income` vs scaled `spent_last_30d`.\n",
    "2. Why might robust scaling be a good choice when income has outliers?\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust = RobustScaler()\n",
    "X_robust = pd.DataFrame(robust.fit_transform(X_num), columns=numeric_cols)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=X_robust, x=\"income\", y=\"spent_last_30d\")\n",
    "plt.title(\"Robust-scaled: income vs spent_last_30d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4e075",
   "metadata": {},
   "source": [
    "## 15.6 Encoding categorical variables\n",
    "\n",
    "Many machine learning models require numbers. But real datasets often contain **categories** like city, segment, product type.\n",
    "\n",
    "### Common encoding methods\n",
    "- **One-Hot Encoding**: creates a 0/1 column for each category (good for nominal categories like city)\n",
    "- **Ordinal Encoding**: converts ordered categories to integers (only if order makes sense: low < medium < high)\n",
    "\n",
    "**Warning:** Don’t assign numbers like A=1, B=2, C=3 unless there’s a real order. That can accidentally tell the model that C > B > A.\n",
    "\n",
    "We’ll use one-hot encoding for `city` and `segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_cols = [\"city\", \"segment\"]\n",
    "X_cat = df_consistent[cat_cols].copy()\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "X_cat_ohe = ohe.fit_transform(X_cat)\n",
    "ohe_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "\n",
    "df_cat_ohe = pd.DataFrame(X_cat_ohe, columns=ohe_feature_names, index=df_consistent.index)\n",
    "df_cat_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b926079",
   "metadata": {},
   "source": [
    "### Exercise 5 — Encoding\n",
    "1. Use `pd.get_dummies()` to one-hot encode the same columns.\n",
    "2. Compare the columns created by pandas vs scikit-learn.\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84128e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_consistent[cat_cols], drop_first=False)\n",
    "print(\"Pandas dummy columns (first 10):\")\n",
    "print(list(df_dummies.columns)[:10])\n",
    "\n",
    "print(\"\\nscikit-learn OHE columns (first 10):\")\n",
    "print(list(ohe_feature_names)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704bf24",
   "metadata": {},
   "source": [
    "## 15.7 Feature creation and transformation\n",
    "\n",
    "A **feature** is an input column used for analysis or modeling. Feature engineering means creating better features from existing data.\n",
    "\n",
    "### Why create/transform features?\n",
    "- Some relationships are easier to capture after transformation (e.g., log income)\n",
    "- Models can improve with meaningful derived variables (e.g., days since signup)\n",
    "- Dates and text often need extraction to become useful\n",
    "\n",
    "We’ll create a few simple features:\n",
    "- `days_since_signup` (from `signup_date`)\n",
    "- `income_log1p` (transform)\n",
    "- `spend_per_income` (ratio)\n",
    "- `signup_month` (from date)\n",
    "\n",
    "**Tip:** Start simple. Fancy features don’t help if the data is still messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_consistent.copy()\n",
    "\n",
    "# Reference date for calculation — in real work, use a real \"today\" date\n",
    "as_of_date = pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "df_features[\"days_since_signup\"] = (as_of_date - df_features[\"signup_date\"]).dt.days\n",
    "df_features[\"income_log1p\"] = np.log1p(df_features[\"income\"])\n",
    "df_features[\"spend_per_income\"] = df_features[\"spent_last_30d\"] / df_features[\"income\"]\n",
    "df_features[\"signup_month\"] = df_features[\"signup_date\"].dt.month\n",
    "\n",
    "df_features[[\"signup_date\", \"days_since_signup\", \"income\", \"income_log1p\", \"spent_last_30d\", \"spend_per_income\", \"signup_month\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf73dd",
   "metadata": {},
   "source": [
    "### Visual check: transformed vs original income\n",
    "\n",
    "A log transform often makes a skewed distribution more “balanced”, which can help both visualization and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(df_features[\"income\"], bins=30, ax=axes[0])\n",
    "axes[0].set_title(\"Income (capped)\")\n",
    "\n",
    "sns.histplot(df_features[\"income_log1p\"], bins=30, ax=axes[1])\n",
    "axes[1].set_title(\"Income log1p\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3920db",
   "metadata": {},
   "source": [
    "### Exercise 6 — Feature engineering\n",
    "1. Create a new feature `is_high_spender` where 1 means `spent_last_30d` is above the 75th percentile, else 0.\n",
    "2. Create `age_group` by binning age into groups (e.g., 0–24, 25–34, 35–44, 45–54, 55+).\n",
    "3. Plot the average spend by `age_group`.\n",
    "\n",
    "Write code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54273e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex6 = df_features.copy()\n",
    "\n",
    "# 1) High spender label\n",
    "threshold = df_ex6[\"spent_last_30d\"].quantile(0.75)\n",
    "df_ex6[\"is_high_spender\"] = (df_ex6[\"spent_last_30d\"] > threshold).astype(int)\n",
    "\n",
    "# 2) Age groups (binning)\n",
    "bins = [0, 24, 34, 44, 54, 200]\n",
    "labels = [\"0-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
    "df_ex6[\"age_group\"] = pd.cut(df_ex6[\"age\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "# 3) Average spend by age group\n",
    "avg_spend_by_age = df_ex6.groupby(\"age_group\", observed=True)[\"spent_last_30d\"].mean().sort_index()\n",
    "display(avg_spend_by_age)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.barplot(x=avg_spend_by_age.index, y=avg_spend_by_age.values)\n",
    "plt.title(\"Average spend (last 30d) by age group\")\n",
    "plt.xlabel(\"Age group\")\n",
    "plt.ylabel(\"Average spend\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b302338",
   "metadata": {},
   "source": [
    "## Mini-project — Build a preprocessing pipeline (clean → transform → model-ready)\n",
    "\n",
    "Goal: create a clean, model-ready dataset using a *repeatable* preprocessing pipeline.\n",
    "\n",
    "Why pipelines are useful:\n",
    "- You apply the **same steps** every time (less mistakes)\n",
    "- It helps with reproducibility\n",
    "- It prevents some common errors when training/testing models\n",
    "\n",
    "We will:\n",
    "1. Select numeric and categorical columns\n",
    "2. Impute missing values\n",
    "3. Scale numeric columns\n",
    "4. One-hot encode categorical columns\n",
    "\n",
    "This is a common “preprocessing template” used in real projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b20fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# We'll build from df_clean (after type fixing) to show imputation inside the pipeline\n",
    "df_project = df_clean.copy()\n",
    "\n",
    "# Standardize city early (string clean) — in real systems this could be a custom transformer\n",
    "df_project[\"city\"] = df_project[\"city\"].apply(standardize_city)\n",
    "\n",
    "numeric_features = [\"age\", \"income\", \"spent_last_30d\"]\n",
    "categorical_features = [\"city\", \"segment\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", RobustScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_ready = preprocessor.fit_transform(df_project)\n",
    "X_ready.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611072c",
   "metadata": {},
   "source": [
    "### Inspect the generated feature matrix\n",
    "\n",
    "After preprocessing, we have a numeric matrix with:\n",
    "- Scaled numeric columns\n",
    "- One-hot encoded categorical columns\n",
    "\n",
    "Let’s view the feature names so we can interpret what the pipeline produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output feature names (scikit-learn >= 1.0 supports this for many transformers)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "df_ready = pd.DataFrame(X_ready, columns=feature_names)\n",
    "df_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de17e8",
   "metadata": {},
   "source": [
    "### Mini-project exercise\n",
    "1. Add a new feature to the raw data: `days_since_signup` (from `signup_date`).\n",
    "2. Update the pipeline to include it as a numeric feature.\n",
    "3. Confirm the output matrix has one extra numeric column.\n",
    "\n",
    "Hint: you can create the column in pandas first, then include it in `numeric_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project2 = df_project.copy()\n",
    "as_of_date = pd.to_datetime(\"2025-01-01\")\n",
    "df_project2[\"days_since_signup\"] = (as_of_date - df_project2[\"signup_date\"]).dt.days\n",
    "\n",
    "numeric_features2 = numeric_features + [\"days_since_signup\"]\n",
    "\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features2),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_ready2 = preprocessor2.fit_transform(df_project2)\n",
    "print(\"Old shape:\", X_ready.shape)\n",
    "print(\"New shape:\", X_ready2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90c15f",
   "metadata": {},
   "source": [
    "## Tips, warnings, and common beginner mistakes\n",
    "\n",
    "- **Don’t “fix” data without understanding it.** Always inspect first (`info`, `describe`, missing counts).\n",
    "- **Avoid deleting too much.** Dropping rows/columns is easy but can bias your results.\n",
    "- **Be careful with outliers.** An outlier might be a valuable rare case, not an error.\n",
    "- **Don’t treat categories as numbers** unless the category truly has an order.\n",
    "- **Document your steps.** Future-you (and teammates) will thank you.\n",
    "- **Re-check after cleaning.** After each major step, re-run summaries to verify the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed2588",
   "metadata": {},
   "source": [
    "## Additional resources (optional)\n",
    "\n",
    "- pandas user guide (missing data): https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    "- scikit-learn preprocessing docs: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- scikit-learn pipelines: https://scikit-learn.org/stable/modules/compose.html\n",
    "- Seaborn categorical plots: https://seaborn.pydata.org/tutorial/categorical.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bd136",
   "metadata": {},
   "source": [
    "## Summary / Key takeaways\n",
    "\n",
    "- Cleaning is essential: it improves accuracy, consistency, and trust in results.\n",
    "- Handle missing values thoughtfully: drop only when appropriate, otherwise impute.\n",
    "- Detect outliers (IQR/box plot) and choose a treatment that matches your business context.\n",
    "- Use consistency checks (duplicates, category standardization, range rules) to prevent silent errors.\n",
    "- Scale numeric data when needed, especially before many ML algorithms.\n",
    "- Encode categorical variables (one-hot for non-ordered categories).\n",
    "- Create simple, meaningful features (dates → durations, log transforms, ratios).\n",
    "- Pipelines make preprocessing repeatable and less error-prone."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
