{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e32a3",
   "metadata": {},
   "source": [
    "# Chapter 7: Exploratory Data Analysis and Descriptive Statistics\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the critical first step in any data analytics project. Before building models, creating reports, or making predictions, you must *understand* your data. EDA is the process of systematically examining datasets to discover patterns, spot anomalies, test assumptions, and summarize main characteristics‚Äîoften using visual methods.\n",
    "\n",
    "Think of EDA as a detective's investigation: you're gathering clues about what the data contains, what stories it might tell, and what problems or surprises might be lurking beneath the surface.\n",
    "\n",
    "### Why is EDA so important?\n",
    "\n",
    "1. **Prevents costly mistakes** ‚Äî Wrong assumptions about data lead to wrong conclusions\n",
    "2. **Reveals data quality issues** ‚Äî Missing values, duplicates, and inconsistencies become visible\n",
    "3. **Guides further analysis** ‚Äî EDA helps you decide which techniques and models are appropriate\n",
    "4. **Builds intuition** ‚Äî You develop a \"feel\" for the data that helps throughout your project\n",
    "5. **Communicates findings** ‚Äî Visualizations and summaries help explain data to stakeholders\n",
    "\n",
    "### What we'll cover in this chapter\n",
    "\n",
    "This chapter introduces the core techniques of EDA and descriptive statistics:\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Data Quality Checks** | Inspecting types, missing values, duplicates |\n",
    "| **Distribution Analysis** | Understanding how values are spread out |\n",
    "| **Central Tendency** | Mean, median, mode ‚Äî what's \"typical\"? |\n",
    "| **Dispersion** | Spread and variability of data |\n",
    "| **Outlier Detection** | Finding unusual or extreme values |\n",
    "| **Correlation Analysis** | How variables relate to each other |\n",
    "| **Multivariate Exploration** | Looking at multiple variables together |\n",
    "| **Automated Reports** | Tools that generate EDA summaries |\n",
    "\n",
    "> **Prerequisites**: This chapter assumes you're familiar with Python basics (Chapter 2), NumPy (Chapter 3), Pandas (Chapter 4), and basic plotting (Chapter 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a936bc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "‚úÖ Explain what EDA is and why it matters for data analytics projects\n",
    "\n",
    "‚úÖ Summarize data using measures of **central tendency** (mean, median, mode)\n",
    "\n",
    "‚úÖ Describe data spread using measures of **dispersion** (range, variance, standard deviation, IQR)\n",
    "\n",
    "‚úÖ Analyze **distributions** and identify **outliers** using multiple techniques\n",
    "\n",
    "‚úÖ Explore relationships between variables using **correlation** analysis\n",
    "\n",
    "‚úÖ Perform **multivariate** exploration to find patterns across multiple variables\n",
    "\n",
    "‚úÖ Create a simple, repeatable EDA checklist for your projects\n",
    "\n",
    "‚úÖ (Optional) Generate an automated EDA report using Python tools\n",
    "\n",
    "‚úÖ Interpret EDA results carefully and avoid common analytical traps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65547cbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.1 Purpose and Importance of EDA\n",
    "\n",
    "EDA is the process of *looking at your data* to understand:\n",
    "\n",
    "| Question | What You're Looking For |\n",
    "|----------|------------------------|\n",
    "| **What are the columns?** | Column names, data types (numbers, categories, dates) |\n",
    "| **Is the data quality good?** | Missing values, duplicates, inconsistent values |\n",
    "| **What are typical values?** | Averages, common categories, expected ranges |\n",
    "| **How do values vary?** | Spread (dispersion) and unusual values (outliers) |\n",
    "| **Do variables relate?** | Patterns, trends, correlations between columns |\n",
    "\n",
    "### The EDA Mindset\n",
    "\n",
    "EDA is not about finding \"the answer\"‚Äîit's about asking the right questions. You should approach EDA with curiosity and skepticism:\n",
    "\n",
    "- **Be curious**: What patterns exist? What surprises are hiding in the data?\n",
    "- **Be skeptical**: Could this pattern be a data error? Is this outlier real?\n",
    "- **Be systematic**: Use a consistent checklist so you don't miss important checks\n",
    "\n",
    "### Why Good EDA Prevents Expensive Mistakes\n",
    "\n",
    "```\n",
    "‚ùå Wrong assumptions ‚Üí Wrong charts ‚Üí Wrong models ‚Üí Wrong conclusions ‚Üí Bad decisions\n",
    "‚úÖ Good EDA ‚Üí Right understanding ‚Üí Right approach ‚Üí Valid conclusions ‚Üí Good decisions\n",
    "```\n",
    "\n",
    "> **üí° Tip**: EDA is not a one-time step. You'll often go back and forth as you discover issues and refine questions. Think of it as an iterative conversation with your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28d7fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.2 Setup: Import Libraries\n",
    "\n",
    "Before we begin our EDA, let's import the libraries we'll use:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| **NumPy** | Numerical computing and array operations |\n",
    "| **Pandas** | Data manipulation and analysis |\n",
    "| **Matplotlib** | Basic plotting and visualization |\n",
    "| **Seaborn** | Statistical visualizations (optional but recommended) |\n",
    "| **SciPy** | Statistical functions like z-scores (optional) |\n",
    "\n",
    "> **‚ö†Ô∏è Note**: If you see `ModuleNotFoundError`, install missing packages in your environment:\n",
    "> ```\n",
    "> pip install pandas numpy matplotlib seaborn scipy\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ad59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Seaborn for prettier charts\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "except ImportError:\n",
    "    sns = None\n",
    "    print(\"Seaborn is not installed. Plots will use Matplotlib only.\")\n",
    "\n",
    "# Optional: SciPy for z-scores and some stats helpers\n",
    "try:\n",
    "    from scipy import stats\n",
    "except ImportError:\n",
    "    stats = None\n",
    "    print(\"SciPy is not installed. Some stats examples will be skipped.\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097a2c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.3 Loading a Practice Dataset\n",
    "\n",
    "To focus on EDA skills, we'll use the **diamonds** dataset from seaborn ‚Äî a real-world dataset containing information about diamond prices and characteristics. This is a great dataset for practicing EDA because it has:\n",
    "- Multiple numeric columns (price, carat, depth, table, dimensions)\n",
    "- Categorical columns (cut, color, clarity)\n",
    "- A good size for exploration (~54,000 rows)\n",
    "\n",
    "We'll also inject a few realistic data issues (missing values, duplicates) to practice data quality checks.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "| Column | Description | Type |\n",
    "|--------|-------------|------|\n",
    "| `carat` | Weight of the diamond | Float |\n",
    "| `cut` | Quality of the cut (Fair, Good, Very Good, Premium, Ideal) | Category |\n",
    "| `color` | Diamond color, from D (best) to J (worst) | Category |\n",
    "| `clarity` | Clarity rating (I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF) | Category |\n",
    "| `depth` | Total depth percentage | Float |\n",
    "| `table` | Width of top relative to widest point | Float |\n",
    "| `price` | Price in US dollars | Integer |\n",
    "| `x`, `y`, `z` | Dimensions in mm | Float |\n",
    "\n",
    "> **‚ö†Ô∏è Warning (common mistake)**: In real projects, you must understand how the data was collected (tracking rules, definitions, time windows). EDA can't fix bad definitions or unclear business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diamonds dataset from seaborn\n",
    "df = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Add an order_id for reference\n",
    "df = df.reset_index().rename(columns={\"index\": \"order_id\"})\n",
    "df[\"order_id\"] = df[\"order_id\"] + 1\n",
    "\n",
    "# Add order_date for time-series examples\n",
    "rng = np.random.default_rng(7)\n",
    "start = np.datetime64('2025-01-01')\n",
    "dates = start + rng.integers(0, 180, size=len(df)).astype('timedelta64[D]')\n",
    "df[\"order_date\"] = pd.to_datetime(dates)\n",
    "\n",
    "# Rename some columns to match our business context\n",
    "df = df.rename(columns={\"price\": \"revenue\", \"carat\": \"units\"})\n",
    "\n",
    "# Create segment from cut quality\n",
    "df[\"segment\"] = df[\"cut\"].map({\n",
    "    \"Fair\": \"Budget\", \"Good\": \"Standard\", \"Very Good\": \"Standard\",\n",
    "    \"Premium\": \"Premium\", \"Ideal\": \"Premium\"\n",
    "})\n",
    "\n",
    "# Create region from color\n",
    "df[\"region\"] = df[\"color\"].map({\n",
    "    \"D\": \"North\", \"E\": \"North\", \"F\": \"East\", \"G\": \"East\",\n",
    "    \"H\": \"South\", \"I\": \"South\", \"J\": \"West\"\n",
    "})\n",
    "\n",
    "# Add discount_rate and returned columns\n",
    "df[\"discount_rate\"] = rng.beta(2, 8, size=len(df)) * 0.4\n",
    "df[\"unit_price\"] = df[\"revenue\"] / df[\"units\"]\n",
    "df[\"returned\"] = rng.random(len(df)) < (0.03 + 0.20 * df[\"discount_rate\"])\n",
    "\n",
    "# Keep only the columns we need for EDA\n",
    "df = df[[\"order_id\", \"order_date\", \"segment\", \"region\", \"units\", \"unit_price\", \n",
    "         \"discount_rate\", \"revenue\", \"returned\"]].copy()\n",
    "\n",
    "# Inject a few realistic data issues:\n",
    "# 1) Missing values\n",
    "missing_idx = rng.choice(df.index, size=50, replace=False)\n",
    "df.loc[missing_idx[:25], \"unit_price\"] = np.nan\n",
    "df.loc[missing_idx[25:], \"segment\"] = None\n",
    "\n",
    "# 2) Duplicates (duplicate some rows but change order_id)\n",
    "dupe_rows = df.sample(20, random_state=1).copy()\n",
    "dupe_rows[\"order_id\"] = np.arange(len(df) + 1, len(df) + 1 + len(dupe_rows))\n",
    "df = pd.concat([df, dupe_rows], ignore_index=True)\n",
    "\n",
    "# 3) Outliers: some extreme revenue values already exist in diamonds data\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916fec7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.4 A Simple EDA Checklist (Your Repeatable Workflow)\n",
    "\n",
    "When you start EDA, use a consistent checklist. This ensures you don't miss important checks and makes your analysis reproducible.\n",
    "\n",
    "### The 8-Step EDA Checklist\n",
    "\n",
    "| Step | Action | Key Tools |\n",
    "|------|--------|-----------|\n",
    "| 1Ô∏è‚É£ | **Preview rows** ‚Äî Look at actual data | `head()`, `sample()` |\n",
    "| 2Ô∏è‚É£ | **Check column types and missing values** | `info()`, `isna()` |\n",
    "| 3Ô∏è‚É£ | **Summarize numeric columns** | `describe()` |\n",
    "| 4Ô∏è‚É£ | **Summarize categorical columns** | `value_counts()` |\n",
    "| 5Ô∏è‚É£ | **Visualize distributions** | Histogram, box plot |\n",
    "| 6Ô∏è‚É£ | **Look for outliers and data quality issues** | IQR rule, z-scores |\n",
    "| 7Ô∏è‚É£ | **Explore relationships** | Correlation, groupby, scatter |\n",
    "| 8Ô∏è‚É£ | **Document observations** | Notes, comments |\n",
    "\n",
    "> **üí° Tip**: Keep notes as you do EDA. Your future self (or teammates) will thank you. Consider creating a \"findings\" section in your notebook.\n",
    "\n",
    "### Step 1: Preview Your Data\n",
    "\n",
    "Let's start by looking at the first few rows to get a sense of what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af618537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d4ab8",
   "metadata": {},
   "source": [
    "### Step 2: Data Types and Missing Values\n",
    "\n",
    "The `info()` method answers two critical questions for beginners:\n",
    "\n",
    "1. **Which columns are numeric vs text vs dates?** ‚Äî This determines what operations you can perform\n",
    "2. **Which columns have missing values?** ‚Äî Non-null counts reveal gaps in your data\n",
    "\n",
    "> **‚ö†Ô∏è Common mistake**: Treating numeric-looking text as numbers (e.g., `'100'` stored as string instead of `100` as integer). Always verify dtypes before doing math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing[missing > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163bd8c",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "Duplicates can happen during data imports, merges, or repeated API pulls.\n",
    "\n",
    "Here we check duplicates *ignoring* `order_id` (because `order_id` is unique even for duplicated content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bf0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [c for c in df.columns if c != 'order_id']\n",
    "duplicate_mask = df.duplicated(subset=cols_to_check, keep=False)\n",
    "df.loc[duplicate_mask, cols_to_check].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621d06b",
   "metadata": {},
   "source": [
    "### Basic numeric summary (`describe`)\n",
    "`describe()` gives quick descriptive statistics for numeric columns:\n",
    "- Count (non-missing)\n",
    "- Mean and standard deviation\n",
    "- Min, quartiles (25%, 50%, 75%), max\n",
    "\n",
    "> **Tip**: Quartiles are key for understanding spread and for detecting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[np.number]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c040e21",
   "metadata": {},
   "source": [
    "### Categorical summary (value counts)\n",
    "For text / category columns, look at the most common values and whether you have unexpected categories (typos, inconsistent labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd998fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['segment', 'region', 'returned']:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9d86b",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7.1 ‚Äî Quick data understanding\n",
    "1. How many rows and columns are in `df`?\n",
    "2. Which columns have missing values, and how many?\n",
    "3. What are the *top 2* regions by count?\n",
    "\n",
    "Write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) shape\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 2) missing values per column\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3) top 2 regions\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399505f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.5 Data Distribution Analysis\n",
    "\n",
    "A **distribution** tells you how values are spread out across your data. Understanding distributions is fundamental to EDA because it reveals:\n",
    "\n",
    "| Pattern | What It Means | Example |\n",
    "|---------|---------------|---------|\n",
    "| **Right-skewed** | Most values are small, with a long tail of large values | Income, home prices |\n",
    "| **Left-skewed** | Most values are large, with a long tail of small values | Age at retirement |\n",
    "| **Normal (bell curve)** | Values cluster around the mean symmetrically | Heights, test scores |\n",
    "| **Bimodal** | Two distinct peaks (possible multiple groups) | Mixed populations |\n",
    "| **Uniform** | Values spread evenly across the range | Random IDs |\n",
    "\n",
    "### Key Questions When Analyzing Distributions\n",
    "\n",
    "- Are values mostly small with a few large ones (right-skewed)?\n",
    "- Are there multiple peaks (possible multiple groups)?\n",
    "- Are there strange values or impossible values (negative ages, future dates)?\n",
    "\n",
    "### Common Beginner-Friendly Plots\n",
    "\n",
    "| Plot Type | Best For | Shows |\n",
    "|-----------|----------|-------|\n",
    "| **Histogram** | Continuous data | Frequency distribution (counts in bins) |\n",
    "| **Box plot** | Continuous data | Median, quartiles, and potential outliers |\n",
    "| **Bar chart** | Categorical data | Counts per category |\n",
    "| **KDE plot** | Continuous data | Smoothed density estimate |\n",
    "\n",
    "> **‚ö†Ô∏è Common mistake**: Using a mean to describe a heavily skewed variable without checking the distribution first. Always visualize before summarizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17061a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['units', 'unit_price', 'discount_rate', 'revenue']\n",
    "df[numeric_cols].hist(bins=30)\n",
    "plt.suptitle('Histograms of numeric columns')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sns is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.boxplot(x=df['revenue'], ax=axes[0])\n",
    "    axes[0].set_title('Revenue (box plot)')\n",
    "    sns.histplot(df['revenue'], bins=40, kde=True, ax=axes[1])\n",
    "    axes[1].set_title('Revenue (hist + KDE)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.boxplot(df['revenue'].dropna(), vert=False)\n",
    "    plt.title('Revenue (box plot)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb7572",
   "metadata": {},
   "source": [
    "### Distributions for categorical variables\n",
    "For categories, a bar chart (counts) is often enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1934182",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['segment'].value_counts(dropna=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sns is not None:\n",
    "    sns.countplot(data=df, x='segment', order=df['segment'].value_counts(dropna=False).index)\n",
    "    plt.title('Count by segment')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "else:\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title('Count by segment')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97537c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.6 Measures of Central Tendency\n",
    "\n",
    "**Central tendency** describes a *typical* or *representative* value in your data. Think of it as answering the question: \"What's a normal value?\"\n",
    "\n",
    "### The Three Main Measures\n",
    "\n",
    "| Measure | Definition | Pros | Cons |\n",
    "|---------|------------|------|------|\n",
    "| **Mean** | Sum of all values √∑ count | Uses all data points | Sensitive to outliers |\n",
    "| **Median** | Middle value when sorted | Robust to outliers | Ignores magnitude |\n",
    "| **Mode** | Most frequent value | Works for categories | May not be unique |\n",
    "\n",
    "### Mathematical Definitions\n",
    "\n",
    "For a dataset with values $x_1, x_2, ..., x_n$:\n",
    "\n",
    "- **Mean**: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$\n",
    "- **Median**: The value at position $\\frac{n+1}{2}$ when sorted (or average of two middle values)\n",
    "- **Mode**: The value that appears most frequently\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "| Situation | Recommended Measure | Why |\n",
    "|-----------|--------------------| ----|\n",
    "| Symmetric distribution | Mean | Accurate representation |\n",
    "| Skewed distribution | Median | Not affected by extreme values |\n",
    "| Categorical data | Mode | Only measure that works |\n",
    "| Data with outliers | Median | More robust |\n",
    "\n",
    "> **üí° Tip**: If the distribution is skewed or has outliers, median is often more representative than mean. When in doubt, report both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = df['revenue']\n",
    "summary_central = {\n",
    "    'mean': revenue.mean(),\n",
    "    'median': revenue.median(),\n",
    "    'mode_first': revenue.mode(dropna=True).iloc[0] if not revenue.mode(dropna=True).empty else np.nan\n",
    "}\n",
    "pd.Series(summary_central).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central tendency by segment (grouped summary)\n",
    "df.groupby('segment', dropna=False)['revenue'].agg(['count', 'mean', 'median']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00073df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.7 Measures of Dispersion (Spread)\n",
    "\n",
    "**Dispersion** tells you *how much values vary* from the center. A small dispersion means values are clustered together; a large dispersion means they're spread out.\n",
    "\n",
    "### Key Measures of Dispersion\n",
    "\n",
    "| Measure | Formula | Interpretation |\n",
    "|---------|---------|----------------|\n",
    "| **Range** | $max - min$ | Total spread (very sensitive to outliers) |\n",
    "| **Variance** | $\\sigma^2 = \\frac{1}{n}\\sum(x_i - \\bar{x})^2$ | Average squared deviation |\n",
    "| **Standard Deviation** | $\\sigma = \\sqrt{variance}$ | Spread in original units |\n",
    "| **IQR** | $Q3 - Q1$ | Spread of middle 50% (robust to outliers) |\n",
    "\n",
    "### Understanding Quartiles and Percentiles\n",
    "\n",
    "Quartiles divide your sorted data into four equal parts:\n",
    "\n",
    "```\n",
    "         Q1 (25%)      Q2 (50% = Median)      Q3 (75%)\n",
    "            ‚Üì                ‚Üì                    ‚Üì\n",
    "    |-------|----------------|--------------------|----|\n",
    "   Min              IQR (Interquartile Range)         Max\n",
    "```\n",
    "\n",
    "- **Q1 (25th percentile)**: 25% of data falls below this value\n",
    "- **Q2 (50th percentile)**: The median\n",
    "- **Q3 (75th percentile)**: 75% of data falls below this value\n",
    "- **IQR**: $Q3 - Q1$ ‚Äî the range containing the middle 50% of data\n",
    "\n",
    "### Common Percentiles in Practice\n",
    "\n",
    "| Percentile | Common Use |\n",
    "|------------|------------|\n",
    "| P90 | \"90% of orders are below this amount\" |\n",
    "| P95 | Used for SLA thresholds (e.g., response times) |\n",
    "| P99 | Extreme but not outlier territory |\n",
    "\n",
    "> **‚ö†Ô∏è Common mistake**: Reporting the mean without reporting spread (e.g., standard deviation or IQR). \"Average revenue is $50\" is incomplete‚Äîyou need to know if most values are $45-$55 or $10-$90!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['revenue'].dropna()\n",
    "q1, q3 = x.quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "dispersion = {\n",
    "    'min': x.min(),\n",
    "    'max': x.max(),\n",
    "    'range': x.max() - x.min(),\n",
    "    'std': x.std(),\n",
    "    'var': x.var(),\n",
    "    'q1': q1,\n",
    "    'q3': q3,\n",
    "    'iqr': iqr,\n",
    "    'p90': x.quantile(0.90),\n",
    "    'p95': x.quantile(0.95)\n",
    "}\n",
    "pd.Series(dispersion).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361716b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.8 Outlier Detection Techniques\n",
    "\n",
    "An **outlier** is a value that is unusually far from most other values. Outliers can be:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Real but rare** | Legitimate extreme events | A very large corporate purchase |\n",
    "| **Data errors** | Mistakes in recording | Extra zeros (100 ‚Üí 10000) |\n",
    "| **Mixed populations** | Different groups behaving differently | Consumer vs enterprise customers |\n",
    "| **Measurement issues** | Problems with data collection | Sensor malfunction |\n",
    "\n",
    "### Two Beginner-Friendly Detection Methods\n",
    "\n",
    "#### Method 1: IQR Rule (Tukey's Fences)\n",
    "\n",
    "Values are outliers if they fall outside the \"fences\":\n",
    "\n",
    "$$\\text{Lower fence} = Q1 - 1.5 \\times IQR$$\n",
    "$$\\text{Upper fence} = Q3 + 1.5 \\times IQR$$\n",
    "\n",
    "**Pros**: Robust, doesn't assume normal distribution  \n",
    "**Cons**: May be too conservative for some datasets\n",
    "\n",
    "#### Method 2: Z-Score\n",
    "\n",
    "The z-score measures how many standard deviations a value is from the mean:\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{\\sigma}$$\n",
    "\n",
    "Values with $|z| > 3$ are typically considered outliers.\n",
    "\n",
    "**Pros**: Easy to understand  \n",
    "**Cons**: Assumes roughly normal distribution, sensitive to extreme outliers\n",
    "\n",
    "### What to Do with Outliers\n",
    "\n",
    "> **‚ö†Ô∏è Warning**: Don't automatically delete outliers! First, investigate *why* they exist.\n",
    "\n",
    "| Action | When to Use |\n",
    "|--------|-------------|\n",
    "| **Keep** | Outlier is a valid, important data point |\n",
    "| **Remove** | Outlier is clearly an error |\n",
    "| **Cap/Winsorize** | Reduce impact without removing |\n",
    "| **Separate analysis** | Analyze outliers as their own group |\n",
    "| **Transform** | Use log scale to reduce impact |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7611a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['revenue'].dropna()\n",
    "q1, q3 = x.quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "\n",
    "outliers_iqr = df[(df['revenue'] < lower) | (df['revenue'] > upper)]\n",
    "lower, upper, outliers_iqr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_iqr.sort_values('revenue', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats is not None:\n",
    "    # z-score method (skips NaNs)\n",
    "    z = pd.Series(stats.zscore(df['revenue'], nan_policy='omit'), index=df.index)\n",
    "    df_z = df.assign(revenue_z=z)\n",
    "    df_z.loc[df_z['revenue_z'].abs() > 3].sort_values('revenue_z', key=lambda s: s.abs(), ascending=False).head(10)\n",
    "else:\n",
    "    print('SciPy not installed: skipping z-score example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c742f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.9 Correlation Analysis\n",
    "\n",
    "**Correlation** measures how two numeric variables move together. It's one of the most important tools for finding relationships in your data.\n",
    "\n",
    "### Understanding Correlation Values\n",
    "\n",
    "Correlation ranges from **-1** to **+1**:\n",
    "\n",
    "| Value | Interpretation | Example |\n",
    "|-------|----------------|---------|\n",
    "| **+1** | Perfect positive relationship | Temperature ‚Üë ‚Üí Ice cream sales ‚Üë |\n",
    "| **+0.7 to +0.9** | Strong positive | Study hours ‚Üë ‚Üí Test scores ‚Üë |\n",
    "| **+0.3 to +0.7** | Moderate positive | Advertising ‚Üë ‚Üí Sales ‚Üë |\n",
    "| **0** | No linear relationship | Shoe size and IQ |\n",
    "| **-0.3 to -0.7** | Moderate negative | Price ‚Üë ‚Üí Demand ‚Üì |\n",
    "| **-0.7 to -1** | Strong negative | Exercise ‚Üë ‚Üí Body fat ‚Üì |\n",
    "\n",
    "### Types of Correlation\n",
    "\n",
    "| Type | Best For | Sensitivity |\n",
    "|------|----------|-------------|\n",
    "| **Pearson** | Linear relationships | Sensitive to outliers |\n",
    "| **Spearman** | Monotonic relationships (rank-based) | Robust to outliers |\n",
    "| **Kendall** | Ordinal data, small samples | Most robust |\n",
    "\n",
    "### Pearson vs Spearman: When to Use Which?\n",
    "\n",
    "- **Pearson**: When you expect a linear relationship and data is roughly normal\n",
    "- **Spearman**: When the relationship might be non-linear but monotonic (always increasing or decreasing), or when you have outliers\n",
    "\n",
    "> **‚ö†Ô∏è Critical Warning**: **Correlation does NOT mean causation!**\n",
    "> \n",
    "> Just because two variables move together doesn't mean one causes the other. There could be:\n",
    "> - A third variable causing both (confounding)\n",
    "> - Pure coincidence (spurious correlation)\n",
    "> - Reverse causation (B causes A, not A causes B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b92802",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df[['units', 'unit_price', 'discount_rate', 'revenue']].copy()\n",
    "pearson = num.corr(method='pearson')\n",
    "spearman = num.corr(method='spearman')\n",
    "pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c31a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = spearman  # change to pearson if you want\n",
    "\n",
    "if sns is not None:\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation heatmap')\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.imshow(corr.values, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.title('Correlation heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b773c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.10 Multivariate Exploration\n",
    "\n",
    "**Multivariate analysis** means looking at more than two variables together. This helps you answer complex questions like:\n",
    "\n",
    "- Do patterns change by segment or region?\n",
    "- Is revenue higher because units are higher, or because price is higher?\n",
    "- Are returns connected to discounts *and* segment?\n",
    "\n",
    "### Techniques for Multivariate Exploration\n",
    "\n",
    "| Technique | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| **GroupBy summaries** | Aggregate by categories | \"Average revenue by segment and region\" |\n",
    "| **Pivot tables** | Cross-tabulation | Compare metrics across two dimensions |\n",
    "| **Colored scatter plots** | Add category as color | See if groups behave differently |\n",
    "| **Pair plots** | Matrix of all pairwise plots | Quick overview of all relationships |\n",
    "| **Faceted charts** | Small multiples by category | Compare distributions across groups |\n",
    "\n",
    "### Why Multivariate Analysis Matters\n",
    "\n",
    "Looking at one or two variables at a time can be misleading. For example:\n",
    "- Overall revenue might be increasing, but *only* in one region\n",
    "- Average price might be stable, but *only* for returning customers\n",
    "- A correlation might be strong overall, but disappear within subgroups (Simpson's Paradox)\n",
    "\n",
    "> **üí° Tip**: Always ask \"Does this pattern hold across all groups?\" before drawing conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by segment and region\n",
    "df.groupby(['segment', 'region'], dropna=False)['revenue'].agg(['count', 'mean', 'median']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d386e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table can be easier to read for a 2D grouping\n",
    "pivot = df.pivot_table(index='segment', columns='region', values='revenue', aggfunc='mean', dropna=False)\n",
    "pivot.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sns is not None:\n",
    "    sns.scatterplot(data=df, x='discount_rate', y='revenue', hue='segment', alpha=0.6)\n",
    "    plt.title('Revenue vs discount rate (colored by segment)')\n",
    "    plt.show()\n",
    "else:\n",
    "    for seg in df['segment'].dropna().unique():\n",
    "        d = df[df['segment'] == seg]\n",
    "        plt.scatter(d['discount_rate'], d['revenue'], alpha=0.5, label=seg)\n",
    "    plt.xlabel('discount_rate')\n",
    "    plt.ylabel('revenue')\n",
    "    plt.title('Revenue vs discount rate (by segment)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sns is not None:\n",
    "    # Pairplot: great for quick scanning, but can be slow on huge datasets\n",
    "    sample = df[['units', 'unit_price', 'discount_rate', 'revenue', 'segment']].dropna().sample(300, random_state=0)\n",
    "    sns.pairplot(sample, hue='segment', corner=True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Seaborn not installed: skipping pairplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67228e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.11 Automated EDA Reports\n",
    "\n",
    "In real work, you may want a quick *automated report* that summarizes your data. These tools generate comprehensive reports with minimal code.\n",
    "\n",
    "### Popular Automated EDA Tools\n",
    "\n",
    "| Tool | Description | Install Command |\n",
    "|------|-------------|-----------------|\n",
    "| **ydata-profiling** | Comprehensive HTML report | `pip install ydata-profiling` |\n",
    "| **sweetviz** | Comparison-focused reports | `pip install sweetviz` |\n",
    "| **dataprep** | Fast, interactive reports | `pip install dataprep` |\n",
    "| **dtale** | Interactive browser-based EDA | `pip install dtale` |\n",
    "\n",
    "### What Automated Reports Typically Include\n",
    "\n",
    "- ‚úÖ Data types and missing values\n",
    "- ‚úÖ Descriptive statistics for all columns\n",
    "- ‚úÖ Distribution plots (histograms, bar charts)\n",
    "- ‚úÖ Correlation matrices\n",
    "- ‚úÖ Duplicate detection\n",
    "- ‚úÖ Outlier identification\n",
    "- ‚úÖ Alerts for potential data quality issues\n",
    "\n",
    "### When to Use Automated Reports\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|----------------|\n",
    "| Quick initial overview | ‚úÖ Great for first look |\n",
    "| Sharing with stakeholders | ‚úÖ Professional appearance |\n",
    "| Deep investigation | ‚ö†Ô∏è Supplement with custom analysis |\n",
    "| Large datasets | ‚ö†Ô∏è May be slow‚Äîuse `minimal=True` |\n",
    "\n",
    "> **üí° Tip**: Automated tools are helpers, not substitutes. Always sanity-check results and dig deeper where needed.\n",
    "\n",
    "### Custom Simple Report Function\n",
    "\n",
    "Sometimes you need a quick summary without installing extra packages. Here's a simple custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_eda_report(data: pd.DataFrame) -> dict:\n",
    "    numeric = data.select_dtypes(include=[np.number])\n",
    "    categorical = data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "    report = {\n",
    "        'shape': data.shape,\n",
    "        'missing_per_column': data.isna().sum().sort_values(ascending=False),\n",
    "        'duplicate_rows': int(data.duplicated().sum()),\n",
    "        'numeric_describe': numeric.describe().T if not numeric.empty else None,\n",
    "        'categorical_unique_counts': categorical.nunique(dropna=False).sort_values(ascending=False) if not categorical.empty else None,\n",
    "    }\n",
    "    return report\n",
    "\n",
    "report = simple_eda_report(df)\n",
    "report['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c77120",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['missing_per_column'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: ydata-profiling automated report\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "    profile = ProfileReport(df, title='EDA Profile Report', minimal=True)\n",
    "    profile\n",
    "except Exception as e:\n",
    "    print('ydata-profiling not available (or failed to run).')\n",
    "    print('Reason:', e)\n",
    "    print('You can install it with: pip install ydata-profiling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba830a61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.12 Interpreting EDA Results\n",
    "\n",
    "EDA produces *observations*. Your job is to turn them into *insights* carefully. Here's a practical framework:\n",
    "\n",
    "### The 4-Step Interpretation Process\n",
    "\n",
    "| Step | Action | Example |\n",
    "|------|--------|---------|\n",
    "| 1Ô∏è‚É£ **Describe** | State what you see (facts only) | \"Revenue is right-skewed; a few orders are extremely large\" |\n",
    "| 2Ô∏è‚É£ **Hypothesize** | Propose possible reasons | \"Large orders might be corporate bulk purchases or data errors\" |\n",
    "| 3Ô∏è‚É£ **Test** | Investigate the hypothesis | \"Check those orders by segment/region; verify unit counts\" |\n",
    "| 4Ô∏è‚É£ **Decide** | Choose an action | \"Cap outliers for some plots, but keep for totals; flag suspicious records\" |\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "| Pitfall | Problem | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Correlation = Causation** | Assuming one variable causes another | Remember: correlation shows relationship, not cause |\n",
    "| **Ignoring missing values** | Missing data can bias all results | Always check and handle appropriately |\n",
    "| **Mean on skewed data** | Mean is misleading for skewed distributions | Use median; report both |\n",
    "| **Deleting outliers blindly** | May remove valid important data | Investigate before removing |\n",
    "| **Ignoring context** | Data without business context is meaningless | Understand definitions and collection methods |\n",
    "| **Overgeneralizing** | Patterns in subgroups may differ | Check if patterns hold across segments |\n",
    "\n",
    "### Questions to Always Ask\n",
    "\n",
    "Before concluding your EDA, ask yourself:\n",
    "\n",
    "1. **Could this be a data issue?** Before assuming a real-world pattern\n",
    "2. **Does this make business sense?** Validate against domain knowledge\n",
    "3. **Does the pattern hold for all subgroups?** Watch for Simpson's Paradox\n",
    "4. **What am I NOT seeing?** Consider survivorship bias, selection bias\n",
    "5. **What would change my conclusion?** Think about edge cases\n",
    "\n",
    "> **üí° Tip**: The best analysts are skeptics. Always question your findings before presenting them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb36b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7.2 ‚Äî Distributions and interpretation\n",
    "1. Plot the distribution of `unit_price` (histogram + box plot if possible).\n",
    "2. In 2‚Äì3 sentences, describe the distribution (skewed? outliers?)\n",
    "3. Which measure of central tendency would you trust more for `revenue`: mean or median? Why?\n",
    "\n",
    "Write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) plots\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 2) and 3) write your answers as Python comments\n",
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b73d5e",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7.3 ‚Äî Outliers\n",
    "Use the IQR rule to flag outliers in `revenue`.\n",
    "1. Compute $Q1$, $Q3$, and $IQR$\n",
    "2. Compute lower/upper bounds\n",
    "3. Create a DataFrame of outlier rows, sorted by revenue\n",
    "4. Suggest one possible real-world explanation (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c5078",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 7.4 ‚Äî Correlation choice\n",
    "1. Compute Pearson and Spearman correlation matrices for `units`, `unit_price`, `discount_rate`, `revenue`.\n",
    "2. Which correlation do you prefer here and why? (comment)\n",
    "3. Pick one strong correlation and explain what it might mean *and what it does NOT prove*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69914d3f",
   "metadata": {},
   "source": [
    "---\n",
    "## Mini-project ‚Äî A complete EDA walkthrough\n",
    "Pretend you‚Äôre a data analyst and your manager asks:\n",
    "- ‚ÄúWhat does our order revenue look like?‚Äù\n",
    "- ‚ÄúAre discounts related to revenue or returns?‚Äù\n",
    "- ‚ÄúDo segments behave differently?‚Äù\n",
    "\n",
    "### Your tasks\n",
    "1. Create a short EDA checklist (bullets in a Markdown cell)\n",
    "2. Run the checklist on `df`\n",
    "3. Create **at least 2 plots** (one distribution, one relationship plot)\n",
    "4. Write **3 observations** and **2 follow-up questions**\n",
    "\n",
    "> **Tip**: Keep it simple and clear. Your goal is to communicate, not to show off code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter: you can reuse the earlier checks, but try writing your own clean steps.\n",
    "\n",
    "# Example: a compact summary table by segment\n",
    "segment_summary = (\n",
    "    df.groupby('segment', dropna=False)\n",
    "      .agg(orders=('order_id', 'count'),\n",
    "           avg_revenue=('revenue', 'mean'),\n",
    "           median_revenue=('revenue', 'median'),\n",
    "           return_rate=('returned', 'mean'),\n",
    "           avg_discount=('discount_rate', 'mean'))\n",
    "      .sort_values('avg_revenue', ascending=False)\n",
    ")\n",
    "segment_summary.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605eb635",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Topic | Key Points |\n",
    "|-------|------------|\n",
    "| **Purpose of EDA** | Understand data quality, distributions, and relationships before deeper analysis |\n",
    "| **EDA Checklist** | Preview ‚Üí Types ‚Üí Describe ‚Üí Value counts ‚Üí Visualize ‚Üí Outliers ‚Üí Relationships ‚Üí Document |\n",
    "| **Central Tendency** | Mean (sensitive to outliers), Median (robust), Mode (for categories) |\n",
    "| **Dispersion** | Range, Variance, Standard Deviation, IQR ‚Äî always report spread with center |\n",
    "| **Distributions** | Visualize before summarizing; watch for skewness and multiple peaks |\n",
    "| **Outliers** | Use IQR or z-score to detect; investigate before removing |\n",
    "| **Correlation** | Pearson (linear), Spearman (rank-based); correlation ‚â† causation |\n",
    "| **Multivariate** | Look at multiple variables together; check if patterns hold across groups |\n",
    "\n",
    "### EDA Best Practices Checklist\n",
    "\n",
    "‚úÖ Use both tables AND plots ‚Äî each reveals different issues  \n",
    "‚úÖ Document your findings as you go  \n",
    "‚úÖ Always check for missing values and duplicates first  \n",
    "‚úÖ Visualize distributions before calculating summary statistics  \n",
    "‚úÖ Report both center (mean/median) AND spread (std/IQR)  \n",
    "‚úÖ Investigate outliers before removing them  \n",
    "‚úÖ Remember that correlation does not prove causation  \n",
    "‚úÖ Check if patterns hold across subgroups  \n",
    "‚úÖ Keep your EDA workflow consistent and repeatable  \n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Chapter 8**, we'll build on these EDA foundations to explore **Statistical Methods for Data Analytics**, including hypothesis testing, confidence intervals, and regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0709a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources and References\n",
    "\n",
    "### Official Documentation\n",
    "- **Pandas**: https://pandas.pydata.org/docs/\n",
    "- **NumPy**: https://numpy.org/doc/stable/\n",
    "- **Seaborn**: https://seaborn.pydata.org/tutorial.html\n",
    "- **Matplotlib**: https://matplotlib.org/stable/tutorials/\n",
    "- **SciPy Statistics**: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "### Automated EDA Tools\n",
    "- **ydata-profiling**: https://github.com/ydataai/ydata-profiling\n",
    "- **sweetviz**: https://github.com/fbdesignpro/sweetviz\n",
    "- **dataprep**: https://dataprep.ai/\n",
    "\n",
    "### Further Reading\n",
    "- \"Exploratory Data Analysis\" by John Tukey ‚Äî The classic text that introduced EDA\n",
    "- \"Python for Data Analysis\" by Wes McKinney ‚Äî Pandas creator's guide\n",
    "- \"Storytelling with Data\" by Cole Nussbaumer Knaflic ‚Äî Visualization best practices\n",
    "\n",
    "### Online Courses\n",
    "- Kaggle Learn: Data Visualization ‚Äî https://www.kaggle.com/learn/data-visualization\n",
    "- DataCamp: Exploratory Data Analysis in Python\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 7**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
