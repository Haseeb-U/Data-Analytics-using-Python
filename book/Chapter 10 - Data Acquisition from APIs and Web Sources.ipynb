{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3f28e8",
   "metadata": {},
   "source": [
    "# Chapter 10: Data Acquisition from APIs and Web Sources\n",
    "\n",
    "In earlier chapters, we focused on analyzing data that already exists in files or databases. In the real world, a lot of your work starts one step earlier: **getting the data**.\n",
    "\n",
    "Data rarely comes to you in a perfect format. As a data analyst, you'll often need to:\n",
    "- Pull live data from web services (APIs)\n",
    "- Extract information from websites (web scraping)\n",
    "- Handle different data formats (JSON, XML, HTML)\n",
    "- Deal with authentication, rate limits, and errors\n",
    "\n",
    "This chapter teaches you beginner-friendly, practical workflows to acquire data from external sources reliably and ethically.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll learn in this chapter\n",
    "\n",
    "| Section | Topic | Key Skills |\n",
    "|---------|-------|------------|\n",
    "| 10.1 | Types of data sources | Identify where data comes from |\n",
    "| 10.2 | REST API fundamentals | Understand how APIs work |\n",
    "| 10.3 | Making API requests | Use Python to call APIs |\n",
    "| 10.4 | Authentication and tokens | Secure API access |\n",
    "| 10.5 | Handling JSON and XML | Parse common data formats |\n",
    "| 10.6 | Web scraping principles | Ethical data extraction |\n",
    "| 10.7 | HTML parsing | Extract data from web pages |\n",
    "| 10.8 | Dynamic content scraping | Handle JavaScript-rendered pages |\n",
    "| 10.9 | Rate limits and error handling | Build robust data pipelines |\n",
    "| 10.10 | Legal and ethical considerations | Stay compliant and ethical |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "1. **Explain** the different types of data sources (files, databases, APIs, web pages)\n",
    "2. **Understand** REST API basics: endpoints, methods, parameters, and status codes\n",
    "3. **Make** reliable API requests with proper error handling and timeouts\n",
    "4. **Store** API tokens safely using environment variables\n",
    "5. **Parse** JSON, XML, and HTML data into clean DataFrames\n",
    "6. **Recognize** when Selenium is needed for dynamic content\n",
    "7. **Apply** polite rate-limiting and retry logic\n",
    "8. **Describe** key legal and ethical considerations for data acquisition\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "> \"80% of a data analyst's time is spent getting and cleaning data.\"  \n",
    "> â€” Common industry observation\n",
    "\n",
    "Understanding data acquisition is essential because:\n",
    "- **Real-world data is messy**: It comes from multiple sources in different formats\n",
    "- **APIs are everywhere**: Weather, finance, social media, government dataâ€”all available via APIs\n",
    "- **Automation saves time**: Once you can fetch data programmatically, you can automate reports\n",
    "- **Ethics matter**: Knowing what you can and cannot do protects you and your organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6de33",
   "metadata": {},
   "source": [
    "## Setup: Required Libraries\n",
    "\n",
    "Before we begin, let's import all the libraries we'll use throughout this chapter.\n",
    "\n",
    "### Libraries overview\n",
    "\n",
    "| Library | Purpose | Installation |\n",
    "|---------|---------|--------------|\n",
    "| `requests` | Make HTTP requests to APIs | `pip install requests` |\n",
    "| `beautifulsoup4` | Parse HTML/XML documents | `pip install beautifulsoup4` |\n",
    "| `pandas` | Data manipulation | Usually pre-installed |\n",
    "| `json` | Parse JSON data | Built-in (no install needed) |\n",
    "| `xml.etree.ElementTree` | Parse XML data | Built-in (no install needed) |\n",
    "\n",
    "### Offline-friendly examples\n",
    "\n",
    "This notebook is designed to work even without internet access. When we make API calls, we include fallback sample data so you can continue learning.\n",
    "\n",
    "> **Tip:** If you get import errors, open a terminal and run:\n",
    "> ```\n",
    "> pip install requests beautifulsoup4 matplotlib\n",
    "> ```\n",
    ">\n",
    "> Avoid running `pip` directly inside notebooks unless you understand your environment wellâ€”it can sometimes install to the wrong location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS: Libraries used throughout this chapter\n",
    "# =============================================================================\n",
    "\n",
    "# Built-in libraries (no installation needed)\n",
    "import json                          # Parse JSON data\n",
    "import os                            # Access environment variables\n",
    "import time                          # Add delays between requests\n",
    "import xml.etree.ElementTree as ET   # Parse XML data\n",
    "from typing import Any, Dict, Optional  # Type hints for cleaner code\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# HTTP requests library (needs installation)\n",
    "try:\n",
    "    import requests\n",
    "    print(\"âœ“ requests library loaded successfully\")\n",
    "except ImportError:\n",
    "    requests = None\n",
    "    print(\"âœ— requests not installed. Run: pip install requests\")\n",
    "\n",
    "# HTML parsing library (needs installation)\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    print(\"âœ“ BeautifulSoup library loaded successfully\")\n",
    "except ImportError:\n",
    "    BeautifulSoup = None\n",
    "    print(\"âœ— beautifulsoup4 not installed. Run: pip install beautifulsoup4\")\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ“ matplotlib library loaded successfully\")\n",
    "except ImportError:\n",
    "    plt = None\n",
    "    print(\"âœ— matplotlib not installed. Run: pip install matplotlib\")\n",
    "\n",
    "# Configure pandas display options for better output\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print(\"\\n--- Setup complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804a461",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.1 Types of Data Sources\n",
    "\n",
    "As a data analyst, you'll acquire data from many different places. Understanding where data comes from helps you choose the right tools and techniques.\n",
    "\n",
    "### Common data sources\n",
    "\n",
    "| Source Type | Examples | Typical Format | How to Access |\n",
    "|-------------|----------|----------------|---------------|\n",
    "| **Files** | CSV, Excel, JSON files | Structured / Semi-structured | `pandas.read_csv()`, `pandas.read_excel()` |\n",
    "| **Databases** | SQLite, PostgreSQL, MySQL | Structured (tables) | SQL queries via Python |\n",
    "| **APIs** | Weather APIs, Finance APIs, Social media | Usually JSON (sometimes XML) | HTTP requests |\n",
    "| **Web pages** | News sites, e-commerce, dashboards | Unstructured HTML | Web scraping |\n",
    "| **Logs / Events** | App logs, clickstream data | Semi-structured (text/JSON) | File parsing, streaming |\n",
    "\n",
    "### Understanding data structure levels\n",
    "\n",
    "Think of data structure as a spectrum:\n",
    "\n",
    "```\n",
    "STRUCTURED â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º UNSTRUCTURED\n",
    "    â”‚                      â”‚                                â”‚\n",
    "  Tables               JSON/XML                        Raw HTML\n",
    "  (rows & columns)     (nested but organized)          (must extract)\n",
    "```\n",
    "\n",
    "- **Structured data**: Neat rows and columns, like a spreadsheet. Easy to analyze directly.\n",
    "- **Semi-structured data**: Has organization (like JSON objects), but not a flat table. Needs some transformation.\n",
    "- **Unstructured data**: Raw text or HTML where you must identify and extract the pieces you need.\n",
    "\n",
    "### Choosing the right approach\n",
    "\n",
    "```\n",
    "Do you need external data?\n",
    "         â”‚\n",
    "         â–¼\n",
    "    Is there an API?\n",
    "      /         \\\n",
    "    YES          NO\n",
    "     â”‚            â”‚\n",
    "     â–¼            â–¼\n",
    "  Use API    Is the data in HTML?\n",
    "                /         \\\n",
    "              YES          NO\n",
    "               â”‚            â”‚\n",
    "               â–¼            â–¼\n",
    "           Scrape       Look for\n",
    "           (carefully)   other sources\n",
    "```\n",
    "\n",
    "> **Best Practice:** Always prefer an **official API** when available. APIs are:\n",
    "> - More reliable (designed for programmatic access)\n",
    "> - More stable (less likely to change without notice)\n",
    "> - Usually faster and cleaner\n",
    ">\n",
    "> Scraping should be your **last resort** when no API exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Visualizing data source types\n",
    "# =============================================================================\n",
    "\n",
    "# Let's create a simple visualization showing different data source types\n",
    "# and their relative usage in data analytics\n",
    "\n",
    "data_sources = {\n",
    "    'Source': ['Files (CSV/Excel)', 'Databases (SQL)', 'APIs', 'Web Scraping', 'Logs/Events'],\n",
    "    'Ease of Use': [5, 4, 3, 2, 3],  # 1-5 scale (5 = easiest)\n",
    "    'Data Quality': [4, 5, 4, 2, 3],  # 1-5 scale (5 = highest quality)\n",
    "    'Common in Industry': [5, 5, 4, 2, 4]  # 1-5 scale (5 = most common)\n",
    "}\n",
    "\n",
    "df_sources = pd.DataFrame(data_sources)\n",
    "df_sources.set_index('Source', inplace=True)\n",
    "\n",
    "print(\"Data Source Comparison (1-5 scale, 5 = best):\")\n",
    "print(\"=\" * 60)\n",
    "display(df_sources)\n",
    "\n",
    "# Create a simple bar chart\n",
    "if plt:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    df_sources.plot(kind='bar', ax=ax, rot=15)\n",
    "    ax.set_title('Comparison of Data Source Types', fontsize=14)\n",
    "    ax.set_ylabel('Score (1-5)')\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3ded5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.2 REST API Fundamentals\n",
    "\n",
    "**API** stands for **Application Programming Interface**. It's a way for programs to talk to each other.\n",
    "\n",
    "When you use an app on your phone to check the weather, that app is making an **API call** to a weather service to get the data. We can do the same thing with Python!\n",
    "\n",
    "### What is REST?\n",
    "\n",
    "**REST** (Representational State Transfer) is the most common style for web APIs. Almost every API you'll encounter as a data analyst follows REST principles.\n",
    "\n",
    "Think of a REST API like a restaurant:\n",
    "- The **menu** = API documentation (tells you what's available)\n",
    "- Your **order** = API request (what you want)\n",
    "- The **waiter** = HTTP protocol (delivers your request)\n",
    "- Your **food** = API response (the data you receive)\n",
    "\n",
    "### Key REST Concepts\n",
    "\n",
    "#### 1. Endpoints (URLs)\n",
    "\n",
    "An **endpoint** is a specific URL that gives you access to a resource.\n",
    "\n",
    "```\n",
    "https://api.example.com/weather         â† Get weather data\n",
    "https://api.example.com/users           â† Get user data\n",
    "https://api.example.com/products/123    â† Get product #123\n",
    "```\n",
    "\n",
    "#### 2. HTTP Methods\n",
    "\n",
    "The **method** tells the API what you want to do:\n",
    "\n",
    "| Method | Purpose | Analogy |\n",
    "|--------|---------|---------|\n",
    "| `GET` | Read/retrieve data | \"Show me the menu\" |\n",
    "| `POST` | Create new data | \"Place a new order\" |\n",
    "| `PUT` | Update existing data | \"Change my order\" |\n",
    "| `DELETE` | Remove data | \"Cancel my order\" |\n",
    "\n",
    "> **For data analytics, you'll use `GET` 99% of the time** because you're reading data, not creating or modifying it.\n",
    "\n",
    "#### 3. Query Parameters\n",
    "\n",
    "**Parameters** filter or customize your request. They appear after a `?` in the URL:\n",
    "\n",
    "```\n",
    "https://api.example.com/weather?city=London&units=metric\n",
    "                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   Query parameters\n",
    "```\n",
    "\n",
    "#### 4. Status Codes\n",
    "\n",
    "The server responds with a **status code** telling you what happened:\n",
    "\n",
    "| Code | Meaning | What to Do |\n",
    "|------|---------|------------|\n",
    "| **200** | âœ… Success | Process the data |\n",
    "| **400** | âŒ Bad request | Check your parameters |\n",
    "| **401** | ðŸ” Unauthorized | Check your API key |\n",
    "| **403** | ðŸš« Forbidden | You don't have permission |\n",
    "| **404** | â“ Not found | Check the endpoint URL |\n",
    "| **429** | â±ï¸ Rate limited | Slow down, wait before retrying |\n",
    "| **500** | ðŸ’¥ Server error | Try again later |\n",
    "\n",
    "> **Warning:** Always set a **timeout** on your requests. Without it, your code can hang forever if the server doesn't respond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15847965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Understanding URL structure\n",
    "# =============================================================================\n",
    "\n",
    "# Let's break down a typical API URL\n",
    "\n",
    "example_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "example_params = {\n",
    "    'latitude': 51.5072,\n",
    "    'longitude': -0.1276,\n",
    "    'hourly': 'temperature_2m',\n",
    "    'timezone': 'UTC'\n",
    "}\n",
    "\n",
    "print(\"Breaking down an API request:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. BASE URL: {example_url}\")\n",
    "print(f\"   - Protocol: https (secure)\")\n",
    "print(f\"   - Domain: api.open-meteo.com\")\n",
    "print(f\"   - Path: /v1/forecast\")\n",
    "print(f\"\\n2. PARAMETERS:\")\n",
    "for key, value in example_params.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "# Show what the full URL would look like\n",
    "if requests:\n",
    "    from urllib.parse import urlencode\n",
    "    full_url = f\"{example_url}?{urlencode(example_params)}\"\n",
    "    print(f\"\\n3. FULL URL (what gets sent to the server):\")\n",
    "    print(f\"   {full_url}\")\n",
    "else:\n",
    "    print(\"\\n   (Install requests library to see the full URL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd70e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.3 Making API Requests\n",
    "\n",
    "Now let's actually make some API requests! We'll use the `requests` library, which is the standard tool for HTTP requests in Python.\n",
    "\n",
    "### The basic pattern\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.get(url, params=params, timeout=10)\n",
    "response.raise_for_status()  # Raise an error if request failed\n",
    "data = response.json()       # Convert JSON response to Python dict\n",
    "```\n",
    "\n",
    "### Why we need best practices\n",
    "\n",
    "Without proper handling, your code can:\n",
    "- âŒ Hang forever (no timeout)\n",
    "- âŒ Crash on network errors\n",
    "- âŒ Produce confusing error messages\n",
    "\n",
    "Let's build a **safe helper function** that handles these issues.\n",
    "\n",
    "> **Common Beginner Mistake:** Building URLs by hand with string concatenation.\n",
    "> \n",
    "> âŒ Bad: `url = base + \"?city=\" + city + \"&units=\" + units`\n",
    "> \n",
    "> âœ… Good: `requests.get(url, params={'city': city, 'units': units})`\n",
    ">\n",
    "> Using `params={}` lets Python handle URL encoding safely (special characters, spaces, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTION: Safe API request with timeout and error handling\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_json(\n",
    "    url: str,\n",
    "    params: Optional[Dict[str, Any]] = None,\n",
    "    headers: Optional[Dict[str, str]] = None,\n",
    "    timeout_s: int = 20\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch JSON data from an API safely.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        The API endpoint URL\n",
    "    params : dict, optional\n",
    "        Query parameters to include in the request\n",
    "    headers : dict, optional\n",
    "        HTTP headers (for authentication, etc.)\n",
    "    timeout_s : int\n",
    "        Maximum seconds to wait for response (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        The JSON response as a Python dictionary\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    ImportError\n",
    "        If requests library is not installed\n",
    "    requests.HTTPError\n",
    "        If the server returns an error status code\n",
    "    requests.Timeout\n",
    "        If the request takes longer than timeout_s\n",
    "    \"\"\"\n",
    "    # Check if requests library is available\n",
    "    if requests is None:\n",
    "        raise ImportError('requests is not installed. Run: pip install requests')\n",
    "    \n",
    "    # Make the request with timeout\n",
    "    response = requests.get(url, params=params, headers=headers, timeout=timeout_s)\n",
    "    \n",
    "    # Raise an exception if status code indicates an error (4xx or 5xx)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse JSON and return\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: Call a real API (Open-Meteo weather API - no key required!)\n",
    "# =============================================================================\n",
    "\n",
    "# Open-Meteo is a free weather API that doesn't require registration\n",
    "open_meteo_url = 'https://api.open-meteo.com/v1/forecast'\n",
    "\n",
    "# Parameters for London, UK\n",
    "open_meteo_params = {\n",
    "    'latitude': 51.5072,      # London's latitude\n",
    "    'longitude': -0.1276,     # London's longitude\n",
    "    'hourly': 'temperature_2m',  # We want hourly temperature\n",
    "    'timezone': 'UTC',        # Use UTC timezone\n",
    "}\n",
    "\n",
    "# Fallback sample data (so the notebook works without internet)\n",
    "fallback_api_json = {\n",
    "    'hourly': {\n",
    "        'time': [\n",
    "            '2026-01-01T00:00', '2026-01-01T01:00', '2026-01-01T02:00',\n",
    "            '2026-01-01T03:00', '2026-01-01T04:00', '2026-01-01T05:00',\n",
    "            '2026-01-01T06:00', '2026-01-01T07:00', '2026-01-01T08:00',\n",
    "        ],\n",
    "        'temperature_2m': [7.1, 6.9, 6.6, 6.4, 6.2, 6.0, 5.9, 6.1, 6.5],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Try to make the API call\n",
    "try:\n",
    "    api_json = fetch_json(open_meteo_url, params=open_meteo_params)\n",
    "    print(\"âœ… API call succeeded!\")\n",
    "    print(f\"   Response contains {len(api_json.get('hourly', {}).get('time', []))} hourly records\")\n",
    "    print(f\"   Top-level keys: {list(api_json.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ API call failed (this is OK if you're offline)\")\n",
    "    print(f\"   Error: {type(e).__name__} - {e}\")\n",
    "    print(\"   Using fallback sample data instead...\")\n",
    "    api_json = fallback_api_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c77f",
   "metadata": {},
   "source": [
    "### Converting API responses to DataFrames\n",
    "\n",
    "APIs often return **nested JSON** (dictionaries inside dictionaries). For analysis, we need to extract the relevant data and put it into a flat table (DataFrame).\n",
    "\n",
    "**Strategy:** Don't try to force ALL the nested JSON into a DataFrame. Instead:\n",
    "1. Explore the response structure\n",
    "2. Identify the data you need\n",
    "3. Extract just that part\n",
    "4. Convert to DataFrame\n",
    "\n",
    "Let's look at what our API response contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84148908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: Explore the API response structure\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Exploring the API response structure:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show the top-level keys\n",
    "print(f\"\\nTop-level keys: {list(api_json.keys())}\")\n",
    "\n",
    "# Look at what's inside 'hourly'\n",
    "if 'hourly' in api_json:\n",
    "    hourly_data = api_json['hourly']\n",
    "    print(f\"\\nKeys inside 'hourly': {list(hourly_data.keys())}\")\n",
    "    print(f\"Number of time records: {len(hourly_data.get('time', []))}\")\n",
    "    print(f\"\\nFirst 3 timestamps: {hourly_data.get('time', [])[:3]}\")\n",
    "    print(f\"First 3 temperatures: {hourly_data.get('temperature_2m', [])[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc112ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Create a function to convert this specific API response to DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "def open_meteo_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert Open-Meteo API response to a clean pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        The JSON response from Open-Meteo API\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with 'time' and 'temperature_2m' columns\n",
    "    \"\"\"\n",
    "    # Extract the hourly data section\n",
    "    hourly = data.get('hourly', {})\n",
    "    \n",
    "    # Create DataFrame from the parallel lists\n",
    "    df = pd.DataFrame({\n",
    "        'time': hourly.get('time', []),\n",
    "        'temperature_2m': hourly.get('temperature_2m', []),\n",
    "    })\n",
    "    \n",
    "    # Convert time strings to proper datetime objects\n",
    "    # errors='coerce' turns invalid dates into NaT (Not a Time) instead of crashing\n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce', utc=True)\n",
    "    \n",
    "    # Remove any rows where time conversion failed\n",
    "    df = df.dropna(subset=['time'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert our API data to a DataFrame\n",
    "df_weather = open_meteo_to_dataframe(api_json)\n",
    "\n",
    "print(\"Converted to DataFrame:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_weather.shape[0]} rows Ã— {df_weather.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df_weather.columns)}\")\n",
    "print(f\"Data types:\\n{df_weather.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cbbda",
   "metadata": {},
   "source": [
    "### Visual validation: Quick sanity check\n",
    "\n",
    "Even during data acquisition, it's useful to quickly visualize the data. This helps you catch obvious issues:\n",
    "- âŒ Missing time periods (gaps in the line)\n",
    "- âŒ Wrong units (values way too high or low)\n",
    "- âŒ Wrong timezone (times shifted unexpectedly)\n",
    "- âŒ Data quality issues (sudden spikes or drops)\n",
    "\n",
    "> **Common Mistake:** Plotting before converting timestamps properly. Always convert time strings to datetime objects first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUAL VALIDATION: Plot the temperature data\n",
    "# =============================================================================\n",
    "\n",
    "if plt:\n",
    "    # Sort by time to ensure proper line plot\n",
    "    df_plot = df_weather.sort_values('time').copy()\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    ax.plot(df_plot['time'], df_plot['temperature_2m'], \n",
    "            linewidth=2, color='steelblue', marker='o', markersize=3)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_title('Temperature Over Time (API Data)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time (UTC)', fontsize=11)\n",
    "    ax.set_ylabel('Temperature (Â°C)', fontsize=11)\n",
    "    \n",
    "    # Add grid for easier reading\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a horizontal line at freezing point for reference\n",
    "    ax.axhline(y=0, color='lightblue', linestyle='--', linewidth=1, label='Freezing point')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quick statistics\n",
    "    print(f\"\\nQuick statistics:\")\n",
    "    print(f\"  Min temperature: {df_plot['temperature_2m'].min():.1f}Â°C\")\n",
    "    print(f\"  Max temperature: {df_plot['temperature_2m'].max():.1f}Â°C\")\n",
    "    print(f\"  Mean temperature: {df_plot['temperature_2m'].mean():.1f}Â°C\")\n",
    "else:\n",
    "    print(\"matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f4aa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.4 Authentication and Tokens\n",
    "\n",
    "Many APIs require you to prove who you are before they give you data. This is called **authentication**.\n",
    "\n",
    "### Why APIs require authentication\n",
    "\n",
    "- **Rate limiting**: Track how many requests you make\n",
    "- **Billing**: Charge for API usage\n",
    "- **Access control**: Limit what data you can see\n",
    "- **Security**: Prevent abuse\n",
    "\n",
    "### Common authentication methods\n",
    "\n",
    "| Method | How it works | Example |\n",
    "|--------|--------------|---------|\n",
    "| **API Key in URL** | Pass key as query parameter | `?api_key=abc123` |\n",
    "| **API Key in Header** | Pass key in HTTP header | `X-API-Key: abc123` |\n",
    "| **Bearer Token** | OAuth-style token in header | `Authorization: Bearer abc123` |\n",
    "| **Basic Auth** | Username:password encoded | Less common for APIs |\n",
    "\n",
    "### ðŸ” CRITICAL: Keep secrets safe!\n",
    "\n",
    "> **Warning:** Never hard-code API keys or tokens directly in your code!\n",
    ">\n",
    "> If you commit secrets to Git, they become public. Assume any leaked token is **compromised** and rotate it immediately.\n",
    "\n",
    "### Best practice: Use environment variables\n",
    "\n",
    "Environment variables store sensitive values outside your code.\n",
    "\n",
    "**Setting environment variables:**\n",
    "\n",
    "```powershell\n",
    "# Windows PowerShell (temporary - current session only)\n",
    "$env:MY_API_TOKEN = \"your_secret_token_here\"\n",
    "\n",
    "# Windows Command Prompt\n",
    "set MY_API_TOKEN=your_secret_token_here\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Linux/Mac\n",
    "export MY_API_TOKEN=\"your_secret_token_here\"\n",
    "```\n",
    "\n",
    "**Reading environment variables in Python:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "token = os.environ.get('MY_API_TOKEN')  # Returns None if not set\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Using environment variables for API tokens\n",
    "# =============================================================================\n",
    "\n",
    "# Try to read a token from environment variable\n",
    "token = os.environ.get('MY_API_TOKEN')\n",
    "\n",
    "print(\"Reading API token from environment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if token:\n",
    "    # Mask the token for display (show only first/last few characters)\n",
    "    masked = token[:4] + '...' + token[-4:] if len(token) > 10 else '****'\n",
    "    print(f\"âœ… Token found: {masked}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No token found (MY_API_TOKEN environment variable not set)\")\n",
    "    print(\"   This is expected - we're just demonstrating the pattern\")\n",
    "\n",
    "# Build headers dictionary for authenticated requests\n",
    "headers: Dict[str, str] = {}\n",
    "\n",
    "if token:\n",
    "    # Bearer token format (most common for modern APIs)\n",
    "    headers['Authorization'] = f'Bearer {token}'\n",
    "    print(f\"\\n   Headers prepared for authenticated request\")\n",
    "else:\n",
    "    print(f\"\\n   Headers will be empty (no authentication)\")\n",
    "\n",
    "print(f\"\\nHeaders dict: {headers if headers else '(empty - no auth needed for Open-Meteo)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46228f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Different authentication patterns\n",
    "# =============================================================================\n",
    "\n",
    "def create_auth_headers(auth_type: str, token: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create headers for different authentication methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    auth_type : str\n",
    "        One of: 'bearer', 'api_key_header', 'basic'\n",
    "    token : str\n",
    "        The authentication token or API key\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Headers dictionary ready to use with requests\n",
    "    \"\"\"\n",
    "    if auth_type == 'bearer':\n",
    "        # OAuth 2.0 Bearer Token (most common for modern APIs)\n",
    "        return {'Authorization': f'Bearer {token}'}\n",
    "    \n",
    "    elif auth_type == 'api_key_header':\n",
    "        # API Key in header (common for simpler APIs)\n",
    "        return {'X-API-Key': token}\n",
    "    \n",
    "    elif auth_type == 'basic':\n",
    "        # Basic authentication (username:password base64 encoded)\n",
    "        import base64\n",
    "        encoded = base64.b64encode(token.encode()).decode()\n",
    "        return {'Authorization': f'Basic {encoded}'}\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown auth_type: {auth_type}\")\n",
    "\n",
    "# Demonstrate the different patterns (with a dummy token)\n",
    "demo_token = \"demo_token_12345\"\n",
    "\n",
    "print(\"Different authentication header formats:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. Bearer Token:\")\n",
    "print(f\"   {create_auth_headers('bearer', demo_token)}\")\n",
    "print(f\"\\n2. API Key Header:\")\n",
    "print(f\"   {create_auth_headers('api_key_header', demo_token)}\")\n",
    "print(f\"\\n3. Basic Auth:\")\n",
    "print(f\"   {create_auth_headers('basic', 'user:password')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752b5e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.5 Handling JSON and XML Data\n",
    "\n",
    "APIs return data in standard formats. The two most common are:\n",
    "- **JSON** (JavaScript Object Notation) - Modern, widely used\n",
    "- **XML** (eXtensible Markup Language) - Older, still used in some systems\n",
    "\n",
    "### JSON: The modern standard\n",
    "\n",
    "JSON is the most common format for APIs today. It looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"city\": \"London\",\n",
    "    \"temperature\": 7.5,\n",
    "    \"conditions\": [\"cloudy\", \"mild\"],\n",
    "    \"metadata\": {\n",
    "        \"source\": \"weather-api\",\n",
    "        \"updated\": \"2026-01-03T10:00:00Z\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key characteristics:**\n",
    "- Uses `{}` for objects (like Python dictionaries)\n",
    "- Uses `[]` for arrays (like Python lists)\n",
    "- Supports strings, numbers, booleans, null\n",
    "- Easy to convert to/from Python objects\n",
    "\n",
    "### XML: The legacy format\n",
    "\n",
    "XML uses tags and is more verbose:\n",
    "\n",
    "```xml\n",
    "<weather>\n",
    "    <city>London</city>\n",
    "    <temperature>7.5</temperature>\n",
    "    <conditions>\n",
    "        <condition>cloudy</condition>\n",
    "        <condition>mild</condition>\n",
    "    </conditions>\n",
    "</weather>\n",
    "```\n",
    "\n",
    "> **Tip:** When working with XML APIs, always check if there's a JSON alternative. JSON is usually easier to work with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WORKING WITH JSON\n",
    "# =============================================================================\n",
    "\n",
    "# JSON text (as you might receive from an API)\n",
    "json_text = '''\n",
    "{\n",
    "    \"city\": \"London\",\n",
    "    \"country\": \"UK\",\n",
    "    \"measurements\": [\n",
    "        {\"type\": \"temperature\", \"value\": 7.1, \"unit\": \"celsius\"},\n",
    "        {\"type\": \"humidity\", \"value\": 80, \"unit\": \"percent\"},\n",
    "        {\"type\": \"wind_speed\", \"value\": 15, \"unit\": \"km/h\"}\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"source\": \"weather-station-42\",\n",
    "        \"last_updated\": \"2026-01-03T08:00:00Z\"\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse JSON string into Python objects\n",
    "data = json.loads(json_text)\n",
    "\n",
    "print(\"Working with JSON data:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nType of parsed data: {type(data)}\")\n",
    "print(f\"Top-level keys: {list(data.keys())}\")\n",
    "print(f\"\\nAccessing simple values:\")\n",
    "print(f\"  City: {data['city']}\")\n",
    "print(f\"  Country: {data['country']}\")\n",
    "print(f\"\\nAccessing nested values:\")\n",
    "print(f\"  Source: {data['metadata']['source']}\")\n",
    "print(f\"\\nAccessing list items:\")\n",
    "print(f\"  First measurement: {data['measurements'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4203b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONVERTING JSON TO DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "# The 'measurements' list is perfect for a DataFrame\n",
    "df_measurements = pd.DataFrame(data['measurements'])\n",
    "\n",
    "print(\"Converting JSON list to DataFrame:\")\n",
    "print(\"=\" * 60)\n",
    "display(df_measurements)\n",
    "\n",
    "# You can also add context from the parent object\n",
    "df_measurements['city'] = data['city']\n",
    "df_measurements['source'] = data['metadata']['source']\n",
    "\n",
    "print(\"\\nWith added context:\")\n",
    "display(df_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee26e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WORKING WITH XML\n",
    "# =============================================================================\n",
    "\n",
    "# XML text (as you might receive from a legacy API)\n",
    "xml_text = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<products>\n",
    "    <product id=\"1\">\n",
    "        <name>Apple</name>\n",
    "        <price currency=\"USD\">1.20</price>\n",
    "        <category>Fruit</category>\n",
    "        <in_stock>true</in_stock>\n",
    "    </product>\n",
    "    <product id=\"2\">\n",
    "        <name>Banana</name>\n",
    "        <price currency=\"USD\">0.80</price>\n",
    "        <category>Fruit</category>\n",
    "        <in_stock>true</in_stock>\n",
    "    </product>\n",
    "    <product id=\"3\">\n",
    "        <name>Orange Juice</name>\n",
    "        <price currency=\"USD\">3.50</price>\n",
    "        <category>Beverage</category>\n",
    "        <in_stock>false</in_stock>\n",
    "    </product>\n",
    "</products>\n",
    "'''\n",
    "\n",
    "# Parse XML string into an ElementTree object\n",
    "root = ET.fromstring(xml_text)\n",
    "\n",
    "print(\"Working with XML data:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRoot element: <{root.tag}>\")\n",
    "print(f\"Number of products: {len(root.findall('product'))}\")\n",
    "\n",
    "# Extract data into a list of dictionaries\n",
    "rows = []\n",
    "for product in root.findall('product'):\n",
    "    rows.append({\n",
    "        'id': product.get('id'),  # Attributes use .get()\n",
    "        'name': product.findtext('name'),  # Text content uses .findtext()\n",
    "        'price': float(product.findtext('price')),\n",
    "        'currency': product.find('price').get('currency'),\n",
    "        'category': product.findtext('category'),\n",
    "        'in_stock': product.findtext('in_stock') == 'true'\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_products = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nXML data converted to DataFrame:\")\n",
    "display(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6607867",
   "metadata": {},
   "source": [
    "### JSON vs XML: Quick comparison\n",
    "\n",
    "| Aspect | JSON | XML |\n",
    "|--------|------|-----|\n",
    "| **Readability** | Compact, easy to read | Verbose, more tags |\n",
    "| **Python parsing** | `json.loads()` â†’ dict/list | `ET.fromstring()` â†’ ElementTree |\n",
    "| **Data types** | Native (string, number, bool, null) | Everything is text |\n",
    "| **Attributes** | Not supported | Supported (e.g., `id=\"1\"`) |\n",
    "| **Modern APIs** | Most common | Less common |\n",
    "\n",
    "> **Tip:** Always validate your assumptions about data types when parsing XML. Numbers and booleans come as strings and need manual conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dece5e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.6 Web Scraping Principles\n",
    "\n",
    "**Web scraping** means downloading web pages and extracting data from the HTML. It's a powerful technique, but comes with responsibilities.\n",
    "\n",
    "### When to scrape vs when NOT to scrape\n",
    "\n",
    "| âœ… Good reasons to scrape | âŒ Bad reasons to scrape |\n",
    "|--------------------------|-------------------------|\n",
    "| No API available | There's an API you're ignoring |\n",
    "| Public data for research | Private/personal data |\n",
    "| One-time data collection | Continuous high-volume access |\n",
    "| Terms of Service allow it | Terms explicitly forbid it |\n",
    "| You respect rate limits | You want to \"get all the data fast\" |\n",
    "\n",
    "### The polite scraping checklist\n",
    "\n",
    "Before you scrape any website, go through this checklist:\n",
    "\n",
    "1. **ðŸ“‹ Check Terms of Service (ToS)**\n",
    "   - Many sites explicitly prohibit scraping\n",
    "   - Violating ToS can have legal consequences\n",
    "\n",
    "2. **ðŸ¤– Check robots.txt**\n",
    "   - Visit `https://example.com/robots.txt`\n",
    "   - It tells crawlers what's allowed/disallowed\n",
    "   - Not legally binding, but a strong ethical guideline\n",
    "\n",
    "3. **â±ï¸ Respect rate limits**\n",
    "   - Add delays between requests (at least 1 second)\n",
    "   - Don't overload servers\n",
    "\n",
    "4. **ðŸ†” Identify yourself**\n",
    "   - Use a reasonable User-Agent header\n",
    "   - Include contact info if doing research\n",
    "\n",
    "5. **ðŸ”’ Avoid personal data**\n",
    "   - Don't scrape emails, names, or private information\n",
    "   - Consider GDPR and other privacy regulations\n",
    "\n",
    "> **Warning:** Just because you *can* scrape something doesn't mean you *should*. When in doubt, ask for permission or look for an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Checking robots.txt\n",
    "# =============================================================================\n",
    "\n",
    "# Let's see what a robots.txt file looks like\n",
    "sample_robots_txt = \"\"\"\n",
    "# Example robots.txt file\n",
    "User-agent: *\n",
    "Allow: /public/\n",
    "Disallow: /private/\n",
    "Disallow: /admin/\n",
    "Disallow: /api/internal/\n",
    "Crawl-delay: 10\n",
    "\n",
    "User-agent: Googlebot\n",
    "Allow: /\n",
    "\n",
    "Sitemap: https://example.com/sitemap.xml\n",
    "\"\"\"\n",
    "\n",
    "print(\"Understanding robots.txt:\")\n",
    "print(\"=\" * 60)\n",
    "print(sample_robots_txt)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey points:\")\n",
    "print(\"â€¢ 'User-agent: *' applies to all crawlers\")\n",
    "print(\"â€¢ 'Disallow: /private/' means don't access /private/ paths\")\n",
    "print(\"â€¢ 'Crawl-delay: 10' means wait 10 seconds between requests\")\n",
    "print(\"â€¢ Always check the site's actual robots.txt before scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952adb00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.7 HTML Parsing with BeautifulSoup\n",
    "\n",
    "**BeautifulSoup** is Python's most popular library for parsing HTML. It turns messy HTML into a navigable tree structure.\n",
    "\n",
    "### How HTML is structured\n",
    "\n",
    "HTML documents are made of nested **elements** (tags):\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Page Title</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Main Heading</h1>\n",
    "    <p class=\"intro\">This is a paragraph.</p>\n",
    "    <table id=\"data\">\n",
    "      <tr><th>Name</th><th>Value</th></tr>\n",
    "      <tr><td>Item 1</td><td>100</td></tr>\n",
    "    </table>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### Key BeautifulSoup methods\n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|---------|\n",
    "| `soup.find(tag)` | Find first matching element | `soup.find('h1')` |\n",
    "| `soup.find_all(tag)` | Find all matching elements | `soup.find_all('tr')` |\n",
    "| `soup.find(tag, {'attr': 'value'})` | Find by attribute | `soup.find('table', {'id': 'data'})` |\n",
    "| `element.get_text()` | Get text content | `h1.get_text()` |\n",
    "| `element.get('attr')` | Get attribute value | `link.get('href')` |\n",
    "| `element.find_all('child')` | Find within element | `table.find_all('tr')` |\n",
    "\n",
    "Let's practice with a sample HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HTML PARSING: Sample web page\n",
    "# =============================================================================\n",
    "\n",
    "# Check if BeautifulSoup is available\n",
    "if BeautifulSoup is None:\n",
    "    raise ImportError('beautifulsoup4 not installed. Run: pip install beautifulsoup4')\n",
    "\n",
    "# A sample HTML page (simulating what you'd get from requests.get())\n",
    "sample_html = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Online Store - Products</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to Our Store</h1>\n",
    "    \n",
    "    <p class=\"description\">Find the best products at great prices!</p>\n",
    "    \n",
    "    <table id=\"products\" class=\"data-table\">\n",
    "      <thead>\n",
    "        <tr>\n",
    "          <th>Product</th>\n",
    "          <th>Category</th>\n",
    "          <th>Price</th>\n",
    "          <th>Rating</th>\n",
    "        </tr>\n",
    "      </thead>\n",
    "      <tbody>\n",
    "        <tr>\n",
    "          <td>Laptop Pro 15</td>\n",
    "          <td>Electronics</td>\n",
    "          <td>$1299.99</td>\n",
    "          <td>4.5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>Wireless Mouse</td>\n",
    "          <td>Accessories</td>\n",
    "          <td>$29.99</td>\n",
    "          <td>4.2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>USB-C Hub</td>\n",
    "          <td>Accessories</td>\n",
    "          <td>$49.99</td>\n",
    "          <td>4.7</td>\n",
    "        </tr>\n",
    "      </tbody>\n",
    "    </table>\n",
    "    \n",
    "    <div class=\"links\">\n",
    "      <p>Useful links:</p>\n",
    "      <ul>\n",
    "        <li><a href=\"/about\">About Us</a></li>\n",
    "        <li><a href=\"/contact\">Contact</a></li>\n",
    "        <li><a href=\"https://external.com/reviews\">Customer Reviews</a></li>\n",
    "      </ul>\n",
    "    </div>\n",
    "    \n",
    "    <footer>\n",
    "      <p>Last updated: January 2026</p>\n",
    "    </footer>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"HTML document parsed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract basic information\n",
    "print(f\"\\nPage title: {soup.find('title').get_text()}\")\n",
    "print(f\"Main heading: {soup.find('h1').get_text()}\")\n",
    "print(f\"Description: {soup.find('p', {'class': 'description'}).get_text()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTING TABLE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Find the products table by its id attribute\n",
    "table = soup.find('table', {'id': 'products'})\n",
    "\n",
    "# Extract column headers from <th> elements\n",
    "headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "print(f\"Table headers: {headers}\")\n",
    "\n",
    "# Extract data rows from <tbody>\n",
    "tbody = table.find('tbody')\n",
    "rows = []\n",
    "\n",
    "for tr in tbody.find_all('tr'):\n",
    "    # Get all <td> elements in this row\n",
    "    cells = tr.find_all('td')\n",
    "    \n",
    "    row_data = {\n",
    "        'product': cells[0].get_text(strip=True),\n",
    "        'category': cells[1].get_text(strip=True),\n",
    "        'price': cells[2].get_text(strip=True),\n",
    "        'rating': cells[3].get_text(strip=True)\n",
    "    }\n",
    "    rows.append(row_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_scraped = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nExtracted table data:\")\n",
    "display(df_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANING SCRAPED DATA\n",
    "# =============================================================================\n",
    "\n",
    "# The price column has $ symbols and the rating is a string\n",
    "# Let's clean these up for analysis\n",
    "\n",
    "df_clean = df_scraped.copy()\n",
    "\n",
    "# Remove $ and convert to float\n",
    "df_clean['price'] = df_clean['price'].str.replace('$', '', regex=False).astype(float)\n",
    "\n",
    "# Convert rating to float\n",
    "df_clean['rating'] = df_clean['rating'].astype(float)\n",
    "\n",
    "print(\"Cleaned DataFrame:\")\n",
    "print(\"=\" * 60)\n",
    "display(df_clean)\n",
    "print(f\"\\nData types after cleaning:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTING LINKS\n",
    "# =============================================================================\n",
    "\n",
    "# Find all <a> tags (hyperlinks)\n",
    "all_links = soup.find_all('a')\n",
    "\n",
    "# Extract link information\n",
    "links_data = []\n",
    "for link in all_links:\n",
    "    links_data.append({\n",
    "        'text': link.get_text(strip=True),\n",
    "        'href': link.get('href'),\n",
    "        'is_external': link.get('href', '').startswith('http')\n",
    "    })\n",
    "\n",
    "df_links = pd.DataFrame(links_data)\n",
    "\n",
    "print(\"Extracted links:\")\n",
    "display(df_links)\n",
    "\n",
    "# Filter to just external links\n",
    "external_links = df_links[df_links['is_external']]\n",
    "print(f\"\\nExternal links found: {len(external_links)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12f8be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.8 Dynamic Content and JavaScript-Rendered Pages\n",
    "\n",
    "Some websites load their content using JavaScript **after** the initial HTML loads. This means:\n",
    "- When you download the HTML, the data isn't there yet\n",
    "- BeautifulSoup only sees the \"skeleton\" page\n",
    "- The actual data gets filled in by JavaScript running in a browser\n",
    "\n",
    "### How to detect dynamic content\n",
    "\n",
    "1. **View page source** vs **Inspect element**\n",
    "   - Right-click â†’ \"View Page Source\" shows raw HTML (what requests sees)\n",
    "   - Right-click â†’ \"Inspect\" shows the DOM after JavaScript runs\n",
    "   - If they're different, the page uses dynamic loading\n",
    "\n",
    "2. **Look for JavaScript frameworks**\n",
    "   - React, Vue, Angular often load data dynamically\n",
    "   - Single-page applications (SPAs) are almost always dynamic\n",
    "\n",
    "### Your options for dynamic content\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Find the API** | Fast, clean data, most reliable | Requires investigation |\n",
    "| **Selenium** | Can handle any dynamic content | Slow, resource-heavy, complex |\n",
    "| **Playwright** | Modern alternative to Selenium | Still complex |\n",
    "| **Wait for static version** | Some sites offer one | Not always available |\n",
    "\n",
    "> **Best Practice:** Before using Selenium, open your browser's Developer Tools (F12) â†’ Network tab. Look for XHR/Fetch requests that load data. You can often call those APIs directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0972585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SELENIUM EXAMPLE (Demonstration - not executed)\n",
    "# =============================================================================\n",
    "\n",
    "# Selenium automates a real browser, so it can handle JavaScript\n",
    "# Installation: pip install selenium\n",
    "# Also requires a browser driver (ChromeDriver, GeckoDriver, etc.)\n",
    "\n",
    "selenium_example_code = '''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Start a Chrome browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Navigate to the page\n",
    "    driver.get('https://example.com/dynamic-page')\n",
    "    \n",
    "    # Wait for a specific element to appear (max 10 seconds)\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, 'data-table'))\n",
    "    )\n",
    "    \n",
    "    # Now the page is fully loaded - extract data\n",
    "    table_html = element.get_attribute('outerHTML')\n",
    "    \n",
    "    # You can then parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(table_html, 'html.parser')\n",
    "    # ... extract data as before ...\n",
    "    \n",
    "finally:\n",
    "    # Always close the browser\n",
    "    driver.quit()\n",
    "'''\n",
    "\n",
    "print(\"Selenium example (not executed - for reference):\")\n",
    "print(\"=\" * 60)\n",
    "print(selenium_example_code)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey points about Selenium:\")\n",
    "print(\"â€¢ Requires browser driver installation (ChromeDriver, etc.)\")\n",
    "print(\"â€¢ Much slower than direct requests (starts real browser)\")\n",
    "print(\"â€¢ Uses more memory and resources\")\n",
    "print(\"â€¢ Can handle login forms, clicking, scrolling\")\n",
    "print(\"â€¢ Consider it a last resort after trying to find APIs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5190cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.9 Rate Limits and Error Handling\n",
    "\n",
    "In real-world data acquisition, **things go wrong**. Networks fail, servers get busy, and APIs limit how fast you can make requests.\n",
    "\n",
    "### Common problems and solutions\n",
    "\n",
    "| Problem | Status Code | Solution |\n",
    "|---------|-------------|----------|\n",
    "| Network timeout | N/A | Set reasonable timeout, retry |\n",
    "| Rate limited | 429 | Wait and retry (exponential backoff) |\n",
    "| Server overloaded | 503 | Wait and retry |\n",
    "| Bad request | 400 | Fix your parameters |\n",
    "| Unauthorized | 401 | Check your API key |\n",
    "| Not found | 404 | Check the URL |\n",
    "| Server error | 500 | Retry later |\n",
    "\n",
    "### Exponential backoff\n",
    "\n",
    "Instead of retrying immediately (which can make things worse), **exponential backoff** waits longer after each failure:\n",
    "\n",
    "```\n",
    "Attempt 1 fails â†’ wait 1 second\n",
    "Attempt 2 fails â†’ wait 2 seconds\n",
    "Attempt 3 fails â†’ wait 4 seconds\n",
    "Attempt 4 fails â†’ wait 8 seconds\n",
    "...give up after max retries\n",
    "```\n",
    "\n",
    "This is polite to servers and more likely to succeed.\n",
    "\n",
    "> **Common Beginner Mistake:** Retrying forever in a tight loop. This can get your IP blocked and wastes resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b721ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROBUST API FUNCTION: With retries and exponential backoff\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_json_with_retries(\n",
    "    url: str,\n",
    "    params: Optional[Dict[str, Any]] = None,\n",
    "    headers: Optional[Dict[str, str]] = None,\n",
    "    timeout_s: int = 20,\n",
    "    max_retries: int = 3,\n",
    "    base_sleep: float = 1.0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch JSON from an API with automatic retry logic.\n",
    "    \n",
    "    Uses exponential backoff: waits longer after each failure.\n",
    "    Handles rate limits (429) specially.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        The API endpoint URL\n",
    "    params : dict, optional\n",
    "        Query parameters\n",
    "    headers : dict, optional\n",
    "        HTTP headers\n",
    "    timeout_s : int\n",
    "        Request timeout in seconds\n",
    "    max_retries : int\n",
    "        Maximum number of retry attempts\n",
    "    base_sleep : float\n",
    "        Base sleep time (doubles with each retry)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        The JSON response as a Python dictionary\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    The last exception if all retries fail\n",
    "    \"\"\"\n",
    "    if requests is None:\n",
    "        raise ImportError('requests not installed. Run: pip install requests')\n",
    "    \n",
    "    last_error: Optional[Exception] = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Make the request\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=timeout_s)\n",
    "            \n",
    "            # Handle rate limiting specially\n",
    "            if response.status_code == 429:\n",
    "                sleep_time = base_sleep * (2 ** (attempt - 1))\n",
    "                print(f\"  â±ï¸ Rate limited (429). Waiting {sleep_time:.1f}s (attempt {attempt}/{max_retries})...\")\n",
    "                time.sleep(sleep_time)\n",
    "                continue\n",
    "            \n",
    "            # Raise exception for other HTTP errors\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Success!\n",
    "            return response.json()\n",
    "            \n",
    "        except requests.exceptions.Timeout as e:\n",
    "            last_error = e\n",
    "            print(f\"  â±ï¸ Timeout on attempt {attempt}/{max_retries}\")\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            last_error = e\n",
    "            print(f\"  ðŸ”Œ Connection error on attempt {attempt}/{max_retries}\")\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            last_error = e\n",
    "            print(f\"  âŒ HTTP error on attempt {attempt}/{max_retries}: {e}\")\n",
    "            # Don't retry client errors (4xx except 429)\n",
    "            if 400 <= response.status_code < 500 and response.status_code != 429:\n",
    "                raise\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"  âŒ Unexpected error on attempt {attempt}/{max_retries}: {e}\")\n",
    "        \n",
    "        # If we're going to retry, wait with exponential backoff\n",
    "        if attempt < max_retries:\n",
    "            sleep_time = base_sleep * (2 ** (attempt - 1))\n",
    "            print(f\"  ðŸ’¤ Waiting {sleep_time:.1f}s before retry...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    # All retries exhausted\n",
    "    raise last_error if last_error else RuntimeError('Request failed')\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing fetch_json_with_retries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    test_data = fetch_json_with_retries(\n",
    "        open_meteo_url,\n",
    "        params=open_meteo_params,\n",
    "        max_retries=2\n",
    "    )\n",
    "    print(\"âœ… Request succeeded!\")\n",
    "    print(f\"   Got {len(test_data.get('hourly', {}).get('time', []))} hourly records\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Request failed (OK if offline): {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a1c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# POLITE SCRAPING: Adding delays between requests\n",
    "# =============================================================================\n",
    "\n",
    "def polite_delay(min_seconds: float = 1.0, max_seconds: float = 3.0) -> None:\n",
    "    \"\"\"\n",
    "    Add a random delay between requests to be polite to servers.\n",
    "    \n",
    "    Using a random range prevents predictable patterns that might\n",
    "    look like bot behavior.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    delay = random.uniform(min_seconds, max_seconds)\n",
    "    print(f\"  ðŸ’¤ Polite delay: {delay:.2f}s\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# Example of polite scraping pattern (not actually executed against a site)\n",
    "print(\"Polite scraping pattern:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Example: scraping multiple pages politely\n",
    "\n",
    "urls_to_scrape = [\n",
    "    'https://example.com/page1',\n",
    "    'https://example.com/page2',\n",
    "    'https://example.com/page3',\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for url in urls_to_scrape:\n",
    "    try:\n",
    "        # Fetch the page\n",
    "        response = requests.get(url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse and extract data\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # ... extract data ...\n",
    "        \n",
    "        all_data.append(extracted_data)\n",
    "        \n",
    "        # BE POLITE: wait before next request\n",
    "        polite_delay(1.0, 3.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        continue  # Move to next URL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ab365",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.10 Legal and Ethical Considerations\n",
    "\n",
    "Data acquisition comes with responsibilities. Just because you *can* access data doesn't mean you *should*.\n",
    "\n",
    "### Key considerations\n",
    "\n",
    "#### 1. ðŸ”’ Privacy and Consent\n",
    "- Avoid collecting personal data without explicit consent\n",
    "- Consider: \"Would this person expect their data to be collected?\"\n",
    "- Be especially careful with sensitive data (health, finances, etc.)\n",
    "\n",
    "#### 2. ðŸ“‹ Terms of Service\n",
    "- Read the ToS of sites/APIs you use\n",
    "- Many prohibit automated access or commercial use\n",
    "- Violations can lead to legal action\n",
    "\n",
    "#### 3. ðŸ¤– robots.txt\n",
    "- Not legally binding, but ethically important\n",
    "- Shows what the site owner wants crawlers to access\n",
    "- Respecting it shows good faith\n",
    "\n",
    "#### 4. ðŸ“œ Copyright and Licensing\n",
    "- Data may be protected by copyright\n",
    "- Check licensing before republishing\n",
    "- Attribution requirements may apply\n",
    "\n",
    "#### 5. ðŸŒ Regulatory Compliance\n",
    "- **GDPR** (EU): Strict rules on personal data\n",
    "- **CCPA** (California): Consumer privacy rights\n",
    "- **Other regional laws**: Many countries have data protection laws\n",
    "\n",
    "#### 6. ðŸ¢ Organizational Policies\n",
    "- Your company may have approved data sources\n",
    "- Ask your legal/compliance team when in doubt\n",
    "- Document your data sources and methods\n",
    "\n",
    "### Ethical decision framework\n",
    "\n",
    "Ask yourself these questions before collecting data:\n",
    "\n",
    "```\n",
    "1. Is this data truly necessary for my analysis?\n",
    "2. Am I authorized to access and use this data?\n",
    "3. Could this data collection harm anyone?\n",
    "4. Am I being transparent about my methods?\n",
    "5. Would I be comfortable if this collection was made public?\n",
    "```\n",
    "\n",
    "> **Golden Rule:** Treat data collection the way you'd want your own data treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4deef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHECKLIST: Before starting any data acquisition project\n",
    "# =============================================================================\n",
    "\n",
    "data_acquisition_checklist = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    DATA ACQUISITION ETHICAL CHECKLIST                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ PURPOSE: Is this data necessary for my legitimate business purpose?      â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ AUTHORIZATION: Do I have permission to access this data source?          â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ TERMS OF SERVICE: Have I read and do I comply with the ToS?              â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ ROBOTS.TXT: Does robots.txt allow my access pattern?                     â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ RATE LIMITS: Am I respecting rate limits and being polite?               â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ PERSONAL DATA: Am I avoiding unnecessary personal data collection?       â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ PRIVACY LAWS: Do I comply with GDPR, CCPA, and other regulations?        â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ DOCUMENTATION: Have I documented my data sources and methods?            â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ SECURITY: Am I storing any credentials/tokens securely?                  â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•‘  â–¡ ORGANIZATIONAL: Does this comply with my company's policies?             â•‘\n",
    "â•‘                                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(data_acquisition_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263bf1e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Practice what you've learned with these exercises. Try to solve them **before** looking at the solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Build a Safe API Request Function\n",
    "\n",
    "**Goal:** Create a function that fetches data from an API safely.\n",
    "\n",
    "**Requirements:**\n",
    "1. Accept a URL and optional parameters\n",
    "2. Set a reasonable timeout (e.g., 15 seconds)\n",
    "3. Raise an error if the request fails\n",
    "4. Return the JSON response as a Python dictionary\n",
    "\n",
    "**Test your function** with the Open-Meteo API for New York City:\n",
    "- Latitude: 40.7128\n",
    "- Longitude: -74.0060\n",
    "- Hourly data: temperature_2m\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Convert Nested JSON to DataFrame\n",
    "\n",
    "**Goal:** Practice extracting specific data from nested JSON structures.\n",
    "\n",
    "**Given this JSON structure:**\n",
    "```python\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"location\": {\"city\": \"Tokyo\", \"country\": \"Japan\"},\n",
    "    \"forecast\": [\n",
    "        {\"date\": \"2026-01-03\", \"high\": 10, \"low\": 3, \"conditions\": \"sunny\"},\n",
    "        {\"date\": \"2026-01-04\", \"high\": 8, \"low\": 2, \"conditions\": \"cloudy\"},\n",
    "        {\"date\": \"2026-01-05\", \"high\": 12, \"low\": 5, \"conditions\": \"sunny\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Extract the forecast list into a DataFrame\n",
    "2. Add a column for the city name\n",
    "3. Calculate the temperature range (high - low) as a new column\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Parse HTML and Extract Table Data\n",
    "\n",
    "**Goal:** Use BeautifulSoup to extract a table from HTML.\n",
    "\n",
    "**Given HTML:** (provided in the code cell below)\n",
    "\n",
    "**Tasks:**\n",
    "1. Parse the HTML with BeautifulSoup\n",
    "2. Find the table by its class name\n",
    "3. Extract all rows into a list of dictionaries\n",
    "4. Convert to a DataFrame\n",
    "5. Clean the data types (convert strings to numbers where appropriate)\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: Thinking Exercise - API vs Scraping\n",
    "\n",
    "**Goal:** Practice making decisions about data acquisition approaches.\n",
    "\n",
    "**Scenario:** You want to get historical stock prices for analysis.\n",
    "\n",
    "**Questions to answer:**\n",
    "1. What are some potential sources for this data?\n",
    "2. For each source, would you use an API or scraping?\n",
    "3. What ethical/legal considerations apply?\n",
    "4. What could go wrong and how would you handle it?\n",
    "\n",
    "Write your answers in a markdown cell or comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 1: Build a Safe API Request Function\n",
    "# =============================================================================\n",
    "\n",
    "# YOUR SOLUTION HERE:\n",
    "# -------------------\n",
    "# def my_fetch_json(url, params=None, timeout=15):\n",
    "#     \"\"\"Fetch JSON from an API safely.\"\"\"\n",
    "#     pass  # Implement this!\n",
    "\n",
    "# Test parameters for New York City\n",
    "exercise1_params = {\n",
    "    'latitude': 40.7128,\n",
    "    'longitude': -74.0060,\n",
    "    'hourly': 'temperature_2m',\n",
    "    'timezone': 'UTC',\n",
    "}\n",
    "\n",
    "# Test your function:\n",
    "# result = my_fetch_json(open_meteo_url, params=exercise1_params)\n",
    "# print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca0699",
   "metadata": {},
   "source": [
    "### Exercise 1 - Sample Solution\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def my_fetch_json(url, params=None, timeout=15):\n",
    "    \"\"\"Fetch JSON from an API safely.\"\"\"\n",
    "    if requests is None:\n",
    "        raise ImportError('requests not installed')\n",
    "    \n",
    "    response = requests.get(url, params=params, timeout=timeout)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 2: Convert Nested JSON to DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "# Given data\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"location\": {\"city\": \"Tokyo\", \"country\": \"Japan\"},\n",
    "    \"forecast\": [\n",
    "        {\"date\": \"2026-01-03\", \"high\": 10, \"low\": 3, \"conditions\": \"sunny\"},\n",
    "        {\"date\": \"2026-01-04\", \"high\": 8, \"low\": 2, \"conditions\": \"cloudy\"},\n",
    "        {\"date\": \"2026-01-05\", \"high\": 12, \"low\": 5, \"conditions\": \"sunny\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# YOUR SOLUTION HERE:\n",
    "# -------------------\n",
    "# 1. Extract the forecast list into a DataFrame\n",
    "# df_forecast = ...\n",
    "\n",
    "# 2. Add a column for the city name\n",
    "# df_forecast['city'] = ...\n",
    "\n",
    "# 3. Calculate temperature range\n",
    "# df_forecast['temp_range'] = ...\n",
    "\n",
    "# Display your result\n",
    "# display(df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc503a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 2 - Sample Solution\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Exercise 2 Solution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Extract the forecast list into a DataFrame\n",
    "df_forecast = pd.DataFrame(api_response['forecast'])\n",
    "\n",
    "# 2. Add a column for the city name\n",
    "df_forecast['city'] = api_response['location']['city']\n",
    "\n",
    "# 3. Calculate temperature range\n",
    "df_forecast['temp_range'] = df_forecast['high'] - df_forecast['low']\n",
    "\n",
    "# Display result\n",
    "display(df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3: Parse HTML and Extract Table Data\n",
    "# =============================================================================\n",
    "\n",
    "# Given HTML\n",
    "exercise_html = '''\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Sales Report</h1>\n",
    "    <table class=\"sales-data\">\n",
    "        <tr>\n",
    "            <th>Product</th>\n",
    "            <th>Q1 Sales</th>\n",
    "            <th>Q2 Sales</th>\n",
    "            <th>Total</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Widget A</td>\n",
    "            <td>$12,500</td>\n",
    "            <td>$15,200</td>\n",
    "            <td>$27,700</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Widget B</td>\n",
    "            <td>$8,300</td>\n",
    "            <td>$9,100</td>\n",
    "            <td>$17,400</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Widget C</td>\n",
    "            <td>$22,000</td>\n",
    "            <td>$24,500</td>\n",
    "            <td>$46,500</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# YOUR SOLUTION HERE:\n",
    "# -------------------\n",
    "# 1. Parse the HTML\n",
    "# soup_ex = BeautifulSoup(exercise_html, 'html.parser')\n",
    "\n",
    "# 2. Find the table by class name\n",
    "# table_ex = ...\n",
    "\n",
    "# 3. Extract rows into a list of dictionaries\n",
    "# rows_ex = []\n",
    "# for tr in ...:\n",
    "#     ...\n",
    "\n",
    "# 4. Convert to DataFrame\n",
    "# df_sales = pd.DataFrame(rows_ex)\n",
    "\n",
    "# 5. Clean the data (remove $ and , from numbers)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3 - Sample Solution\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Exercise 3 Solution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Parse the HTML\n",
    "soup_ex = BeautifulSoup(exercise_html, 'html.parser')\n",
    "\n",
    "# 2. Find the table by class name\n",
    "table_ex = soup_ex.find('table', {'class': 'sales-data'})\n",
    "\n",
    "# 3. Extract rows into a list of dictionaries\n",
    "rows_ex = []\n",
    "data_rows = table_ex.find_all('tr')[1:]  # Skip header row\n",
    "\n",
    "for tr in data_rows:\n",
    "    cells = tr.find_all('td')\n",
    "    rows_ex.append({\n",
    "        'product': cells[0].get_text(strip=True),\n",
    "        'q1_sales': cells[1].get_text(strip=True),\n",
    "        'q2_sales': cells[2].get_text(strip=True),\n",
    "        'total': cells[3].get_text(strip=True)\n",
    "    })\n",
    "\n",
    "# 4. Convert to DataFrame\n",
    "df_sales = pd.DataFrame(rows_ex)\n",
    "print(\"\\nRaw extracted data:\")\n",
    "display(df_sales)\n",
    "\n",
    "# 5. Clean the data (remove $ and , from numbers)\n",
    "def clean_currency(value):\n",
    "    \"\"\"Remove $ and commas, convert to float.\"\"\"\n",
    "    return float(value.replace('$', '').replace(',', ''))\n",
    "\n",
    "for col in ['q1_sales', 'q2_sales', 'total']:\n",
    "    df_sales[col] = df_sales[col].apply(clean_currency)\n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "display(df_sales)\n",
    "print(f\"\\nData types:\\n{df_sales.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dbba1",
   "metadata": {},
   "source": [
    "### Exercise 4 - Thinking Exercise: Sample Answer\n",
    "\n",
    "**Scenario:** Getting historical stock prices\n",
    "\n",
    "**Potential sources:**\n",
    "\n",
    "| Source | API or Scrape? | Considerations |\n",
    "|--------|---------------|----------------|\n",
    "| Yahoo Finance | API (yfinance library) | Free, reliable, well-documented |\n",
    "| Alpha Vantage | API | Free tier available, requires API key |\n",
    "| Financial news sites | Scrape (last resort) | ToS often prohibit, data may be copyrighted |\n",
    "| Bloomberg Terminal | API (if you have access) | Expensive, professional-grade |\n",
    "\n",
    "**Ethical/Legal considerations:**\n",
    "- Most financial data has copyright restrictions\n",
    "- Redistribution may be prohibited\n",
    "- Rate limits apply to free APIs\n",
    "- Some data requires paid subscriptions\n",
    "\n",
    "**What could go wrong:**\n",
    "- API limits exceeded â†’ Use rate limiting and caching\n",
    "- API discontinued â†’ Have backup data source\n",
    "- Data format changes â†’ Validate data structure\n",
    "- Historical data gaps â†’ Handle missing data gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a1137",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Mini-Project: Complete Data Acquisition Pipeline\n",
    "\n",
    "In this mini-project, you'll build a **complete, repeatable data pipeline** that:\n",
    "1. Fetches data from an API (with error handling)\n",
    "2. Converts the response to a clean DataFrame\n",
    "3. Validates the data with a quick visualization\n",
    "4. Saves the data to a CSV file for later analysis\n",
    "\n",
    "This is a realistic workflow you'll use in real data analytics projects!\n",
    "\n",
    "### Pipeline steps:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   1. FETCH      â”‚â”€â”€â”€â–¶â”‚   2. TRANSFORM  â”‚â”€â”€â”€â–¶â”‚   3. VALIDATE   â”‚\n",
    "â”‚   (API call)    â”‚    â”‚   (to DataFrame)â”‚    â”‚   (visualize)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                      â”‚\n",
    "                                                      â–¼\n",
    "                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                              â”‚   4. SAVE       â”‚\n",
    "                                              â”‚   (to CSV)      â”‚\n",
    "                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5239bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MINI-PROJECT: Complete Data Acquisition Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# Configuration\n",
    "API_URL = 'https://api.open-meteo.com/v1/forecast'\n",
    "PARAMS = {\n",
    "    'latitude': 51.5072,      # London\n",
    "    'longitude': -0.1276,\n",
    "    'hourly': 'temperature_2m,relative_humidity_2m',  # Multiple variables\n",
    "    'timezone': 'UTC',\n",
    "}\n",
    "OUTPUT_FILE = 'chapter10_weather_data.csv'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MINI-PROJECT: Weather Data Acquisition Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1: Fetch data from API (with fallback for offline use)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nðŸ“¥ STEP 1: Fetching data from API...\")\n",
    "\n",
    "fallback_data = {\n",
    "    'hourly': {\n",
    "        'time': [f'2026-01-01T{h:02d}:00' for h in range(24)],\n",
    "        'temperature_2m': [5.0 + h * 0.3 for h in range(24)],\n",
    "        'relative_humidity_2m': [80 - h * 1.5 for h in range(24)],\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    raw_data = fetch_json_with_retries(API_URL, params=PARAMS, max_retries=2)\n",
    "    print(\"   âœ… API call successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ API call failed: {e}\")\n",
    "    print(\"   Using fallback sample data...\")\n",
    "    raw_data = fallback_data\n",
    "\n",
    "print(f\"   Data contains {len(raw_data.get('hourly', {}).get('time', []))} hourly records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 2: Transform to clean DataFrame\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nðŸ”„ STEP 2: Transforming to DataFrame...\")\n",
    "\n",
    "def transform_weather_data(raw: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Transform raw API response to a clean DataFrame.\"\"\"\n",
    "    hourly = raw.get('hourly', {})\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': hourly.get('time', []),\n",
    "        'temperature_c': hourly.get('temperature_2m', []),\n",
    "        'humidity_pct': hourly.get('relative_humidity_2m', []),\n",
    "    })\n",
    "    \n",
    "    # Convert timestamp to proper datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
    "    \n",
    "    # Add derived columns\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Drop any rows with missing timestamps\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "pipeline_df = transform_weather_data(raw_data)\n",
    "\n",
    "print(f\"   âœ… Created DataFrame with {len(pipeline_df)} rows and {len(pipeline_df.columns)} columns\")\n",
    "print(f\"   Columns: {list(pipeline_df.columns)}\")\n",
    "print(f\"\\n   First 5 rows:\")\n",
    "display(pipeline_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b6575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 3: Validate with visualization\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š STEP 3: Validating data with visualization...\")\n",
    "\n",
    "if plt:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    df_plot = pipeline_df.sort_values('timestamp')\n",
    "    \n",
    "    # Plot 1: Temperature\n",
    "    axes[0].plot(df_plot['timestamp'], df_plot['temperature_c'], \n",
    "                 color='orangered', linewidth=2)\n",
    "    axes[0].set_ylabel('Temperature (Â°C)', fontsize=11)\n",
    "    axes[0].set_title('Weather Data Validation', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0, color='lightblue', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Plot 2: Humidity\n",
    "    axes[1].plot(df_plot['timestamp'], df_plot['humidity_pct'], \n",
    "                 color='steelblue', linewidth=2)\n",
    "    axes[1].set_ylabel('Humidity (%)', fontsize=11)\n",
    "    axes[1].set_xlabel('Time (UTC)', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 100)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quick statistics\n",
    "    print(\"\\n   ðŸ“ˆ Quick statistics:\")\n",
    "    print(f\"   Temperature: min={df_plot['temperature_c'].min():.1f}Â°C, \"\n",
    "          f\"max={df_plot['temperature_c'].max():.1f}Â°C, \"\n",
    "          f\"mean={df_plot['temperature_c'].mean():.1f}Â°C\")\n",
    "    print(f\"   Humidity: min={df_plot['humidity_pct'].min():.0f}%, \"\n",
    "          f\"max={df_plot['humidity_pct'].max():.0f}%, \"\n",
    "          f\"mean={df_plot['humidity_pct'].mean():.0f}%\")\n",
    "else:\n",
    "    print(\"   âš ï¸ matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bafdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 4: Save to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nðŸ’¾ STEP 4: Saving to CSV...\")\n",
    "\n",
    "# Save the DataFrame\n",
    "pipeline_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"   âœ… Data saved to: {OUTPUT_FILE}\")\n",
    "print(f\"   File size: {os.path.getsize(OUTPUT_FILE):,} bytes\")\n",
    "\n",
    "# Verify by reading back\n",
    "df_verify = pd.read_csv(OUTPUT_FILE)\n",
    "print(f\"   Verification: Read back {len(df_verify)} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabd62b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## What we covered in this chapter\n",
    "\n",
    "### 10.1 Types of Data Sources\n",
    "- Data comes from files, databases, APIs, web pages, and logs\n",
    "- Structured â†’ Semi-structured â†’ Unstructured spectrum\n",
    "- **Always prefer APIs over scraping when available**\n",
    "\n",
    "### 10.2 REST API Fundamentals\n",
    "- REST APIs use endpoints (URLs), HTTP methods, parameters, and status codes\n",
    "- GET method is used for reading data (most common in analytics)\n",
    "- Status codes tell you if requests succeeded or failed\n",
    "\n",
    "### 10.3 Making API Requests\n",
    "- Use the `requests` library for HTTP calls\n",
    "- **Always set a timeout** to prevent hanging\n",
    "- Use `params={}` instead of building URLs manually\n",
    "\n",
    "### 10.4 Authentication and Tokens\n",
    "- Many APIs require API keys or tokens\n",
    "- **Never hard-code secrets in your code**\n",
    "- Use environment variables to store credentials\n",
    "\n",
    "### 10.5 Handling JSON and XML\n",
    "- JSON is the modern standard (use `json.loads()`)\n",
    "- XML is more verbose (use `xml.etree.ElementTree`)\n",
    "- Convert API responses to DataFrames for analysis\n",
    "\n",
    "### 10.6 Web Scraping Principles\n",
    "- Check ToS, robots.txt, and ethical considerations first\n",
    "- Be polite: add delays, respect rate limits\n",
    "- **Scraping is a last resort**\n",
    "\n",
    "### 10.7 HTML Parsing\n",
    "- BeautifulSoup makes HTML parsing easy\n",
    "- Use `find()` and `find_all()` to navigate the DOM\n",
    "- Clean extracted data (remove currency symbols, convert types)\n",
    "\n",
    "### 10.8 Dynamic Content\n",
    "- Some pages load content via JavaScript\n",
    "- Try to find the underlying API first\n",
    "- Use Selenium/Playwright as a last resort\n",
    "\n",
    "### 10.9 Rate Limits and Error Handling\n",
    "- Build robust code with retries and exponential backoff\n",
    "- Handle specific HTTP errors appropriately\n",
    "- Add polite delays between requests\n",
    "\n",
    "### 10.10 Legal and Ethical Considerations\n",
    "- Respect privacy, ToS, and copyright\n",
    "- Know the regulations (GDPR, CCPA, etc.)\n",
    "- When in doubt, ask for permission\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Python Libraries\n",
    "\n",
    "| Task | Library | Installation |\n",
    "|------|---------|-------------|\n",
    "| HTTP requests | `requests` | `pip install requests` |\n",
    "| HTML parsing | `beautifulsoup4` | `pip install beautifulsoup4` |\n",
    "| JSON parsing | `json` | Built-in |\n",
    "| XML parsing | `xml.etree.ElementTree` | Built-in |\n",
    "| Browser automation | `selenium` | `pip install selenium` |\n",
    "| Data manipulation | `pandas` | `pip install pandas` |\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- **Requests documentation:** https://requests.readthedocs.io/\n",
    "- **BeautifulSoup documentation:** https://www.crummy.com/software/BeautifulSoup/\n",
    "- **HTTP status codes (MDN):** https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n",
    "- **robots.txt (MDN):** https://developer.mozilla.org/en-US/docs/Glossary/Robots.txt\n",
    "- **Selenium documentation:** https://selenium-python.readthedocs.io/\n",
    "- **REST API tutorial:** https://restfulapi.net/\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you can acquire data from external sources, you're ready to:\n",
    "1. Combine multiple data sources for richer analysis\n",
    "2. Build automated data pipelines that run on schedules\n",
    "3. Create dashboards that update with live API data\n",
    "4. Handle larger datasets with the techniques from Chapter 11 (Big Data)\n",
    "\n",
    "**Practice tip:** Find a free API that interests you (weather, sports, finance, government data) and build a complete data acquisition pipeline!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
