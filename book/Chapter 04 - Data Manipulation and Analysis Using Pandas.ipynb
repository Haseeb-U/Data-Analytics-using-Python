{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae41100",
   "metadata": {},
   "source": [
    "# Chapter 4: Data Manipulation and Analysis Using Pandas\n",
    "\n",
    "This chapter teaches you how to load, inspect, clean, transform, and analyze data using **Pandas** â€” with hands-on examples, exercises, and a mini-project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50219d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Why Pandas is Essential for Data Analysts**\n",
    "\n",
    "In data analytics, most of your time is spent on things like:\n",
    "- ðŸ“¥ Importing data (CSV, Excel, databases)\n",
    "- ðŸ” Understanding what the data looks like\n",
    "- ðŸ§¹ Cleaning issues (missing values, wrong types, duplicates)\n",
    "- ðŸ”„ Transforming it into a useful shape\n",
    "- ðŸ“Š Summarizing it (grouping, aggregating)\n",
    "\n",
    "Pandas is the most common Python library for these jobs â€” and mastering it will make you productive in nearly any data analytics role.\n",
    "\n",
    "**What you'll learn in this chapter:**\n",
    "- How to load data from files and databases\n",
    "- How to inspect, clean, and transform data\n",
    "- How to filter, sort, and aggregate data\n",
    "- How to merge multiple datasets\n",
    "- How to work with dates and time series\n",
    "- How to export your results\n",
    "\n",
    "**What you'll build:** A complete workflow that goes from \"raw data\" â†’ \"clean data\" â†’ \"summary + chart\" â†’ \"exported results\".\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge (Chapter 2) and familiarity with NumPy arrays (Chapter 3) will help, but are not strictly required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a6bea",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you havenâ€™t installed Pandas yet, you can install it in your environment with:\n",
    "\n",
    "- `pip install pandas`\n",
    "\n",
    "Weâ€™ll also use NumPy (for numeric helpers) and Matplotlib (for a few quick plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0726cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdf9c9",
   "metadata": {},
   "source": [
    "## 1) Pandas architecture: `Series` and `DataFrame`\n",
    "\n",
    "Think of Pandas like this:\n",
    "- A **Series** is a *single column* (1D) with an **index** (labels).\n",
    "- A **DataFrame** is a *table of columns* (2D). Every column is a Series, and rows are aligned by an index.\n",
    "\n",
    "Why this matters:\n",
    "- Most analysis is â€œcolumn-basedâ€: you compute new columns, filter rows, group, and summarize.\n",
    "- The **index** helps Pandas align data safely during operations (especially when merging or adding data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\n",
    "df = pd.DataFrame({\n",
    "    \"product\": [\"Pen\", \"Notebook\", \"Pencil\"],\n",
    "    \"price\": [1.50, 3.00, 0.75],\n",
    "    \"in_stock\": [True, True, False]\n",
    "})\n",
    "\n",
    "s, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56d5a4",
   "metadata": {},
   "source": [
    "> **Tip (Beginner-friendly rule):** When you see â€œindexâ€ in Pandas, think â€œrow labelsâ€.\n",
    ">\n",
    "> By default, Pandas uses `0, 1, 2, ...` but you can use something meaningful (like an `order_id`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bc1ec",
   "metadata": {},
   "source": [
    "## 2) Importing data from files and databases\n",
    "\n",
    "In the real world, you often start by importing data from:\n",
    "- CSV files (`pd.read_csv`)\n",
    "- Excel files (`pd.read_excel`)\n",
    "- JSON (`pd.read_json`)\n",
    "- databases (e.g., SQLite) using `pd.read_sql_query`\n",
    "- built-in datasets from libraries like **seaborn** using `sns.load_dataset`\n",
    "\n",
    "We'll use the famous **tips** dataset from seaborn â€” a real-world dataset of restaurant tips that's perfect for learning Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the tips dataset from seaborn (no download needed - it's built-in)\n",
    "# This dataset contains information about restaurant bills and tips\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# For consistency with our examples, we'll rename and select columns\n",
    "orders = tips.rename(columns={\n",
    "    \"total_bill\": \"unit_price\",\n",
    "    \"size\": \"quantity\"\n",
    "}).copy()\n",
    "\n",
    "# Add order_id and order_date for our exercises\n",
    "orders[\"order_id\"] = range(1001, 1001 + len(orders))\n",
    "orders[\"order_date\"] = pd.date_range(start=\"2025-01-01\", periods=len(orders), freq=\"h\")\n",
    "orders[\"customer\"] = orders[\"sex\"].map({\"Male\": \"Alex\", \"Female\": \"Jordan\"}) + \"_\" + orders.index.astype(str)\n",
    "orders[\"product\"] = orders[\"time\"].map({\"Lunch\": \"Lunch Special\", \"Dinner\": \"Dinner Menu\"})\n",
    "\n",
    "orders = orders[[\"order_id\", \"customer\", \"product\", \"quantity\", \"unit_price\", \"order_date\", \"tip\", \"day\", \"smoker\"]].head(20)\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "orders.to_sql(\"orders\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT customer, SUM(quantity * unit_price) AS revenue\n",
    "FROM orders\n",
    "GROUP BY customer\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "revenue_by_customer = pd.read_sql_query(query, conn)\n",
    "revenue_by_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340651f",
   "metadata": {},
   "source": [
    "> **Common mistake:** letting Pandas guess the wrong types.\n",
    ">\n",
    "> For example, dates usually load as plain text first. Weâ€™ll fix that using `pd.to_datetime` later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12507b",
   "metadata": {},
   "source": [
    "## 3) Data inspection and summary methods\n",
    "\n",
    "Before you clean or analyze, do a quick inspection. Your goals are:\n",
    "- see a few rows\n",
    "- check column names\n",
    "- check data types\n",
    "- look for missing values\n",
    "- get quick summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c88bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d96a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336783dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4f478",
   "metadata": {},
   "source": [
    "### Exercise 1 â€” Quick inspection\n",
    "1. How many rows and columns are in `orders`?\n",
    "2. What are the numeric columns?\n",
    "3. Which customers appear in the data, and how many orders does each have?\n",
    "\n",
    "*Try it yourself first, then run the solution cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape (rows, cols):\", orders.shape)\n",
    "\n",
    "numeric_cols = orders.select_dtypes(include=\"number\").columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "orders_per_customer = orders[\"customer\"].value_counts()\n",
    "orders_per_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21d4c4",
   "metadata": {},
   "source": [
    "## 4) Indexing and selection techniques\n",
    "\n",
    "Selecting the right rows/columns is a daily task. The two most important tools are:\n",
    "- `loc` for **label-based** selection\n",
    "- `iloc` for **position-based** selection\n",
    "\n",
    "Youâ€™ll also use boolean filtering (conditions) a lot.\n",
    "\n",
    "> **Warning (very common bug):** avoid chained indexing like `df[df['x'] > 0]['y'] = 1`.\n",
    "> It can silently fail. Prefer `df.loc[mask, 'y'] = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection\n",
    "orders[[\"order_id\", \"customer\", \"product\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean filtering (rows) - filter for Dinner Menu with party size >= 3\n",
    "mask = (orders[\"product\"] == \"Dinner Menu\") & (orders[\"quantity\"] >= 3)\n",
    "orders.loc[mask, [\"order_id\", \"customer\", \"product\", \"quantity\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00872e",
   "metadata": {},
   "source": [
    "### Exercise 2 â€” Filtering practice\n",
    "Create a filtered DataFrame containing only orders where `smoker` is \"Yes\", and only show the columns `product`, `quantity`, and `unit_price`.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee86b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: filter for smokers\n",
    "smoker_orders = orders.loc[orders[\"smoker\"] == \"Yes\", [\"product\", \"quantity\", \"unit_price\"]]\n",
    "smoker_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce9943",
   "metadata": {},
   "source": [
    "## 5) Sorting and filtering data\n",
    "\n",
    "Once you can select rows, youâ€™ll often want to:\n",
    "- sort rows (e.g., biggest sales first)\n",
    "- filter using helper methods like `isin`, `between`, or `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df994d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders2 = orders.copy()\n",
    "orders2[\"total\"] = orders2[\"quantity\"] * orders2[\"unit_price\"]\n",
    "\n",
    "orders2.sort_values(by=\"total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only Lunch Special or Dinner Menu products\n",
    "orders2[orders2[\"product\"].isin([\"Lunch Special\", \"Dinner Menu\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d29a5",
   "metadata": {},
   "source": [
    "## 6) Handling missing and duplicate data\n",
    "\n",
    "Real datasets often contain:\n",
    "- missing values (`NaN`)\n",
    "- duplicate rows\n",
    "\n",
    "A good cleaning approach is:\n",
    "1. *measure* the problem (how many missing/duplicates?)\n",
    "2. decide a strategy (drop, fill, or fix)\n",
    "3. apply the strategy and re-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aaf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty = orders2.copy()\n",
    "dirty.loc[1, \"unit_price\"] = np.nan  # introduce missing value\n",
    "dirty = pd.concat([dirty, dirty.iloc[[2]]], ignore_index=True)  # duplicate a row\n",
    "dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values per column:\")\n",
    "print(dirty.isna().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows (count):\", dirty.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy example: fill missing unit_price using the median price\n",
    "cleaned = dirty.copy()\n",
    "median_price = cleaned[\"unit_price\"].median()\n",
    "cleaned[\"unit_price\"] = cleaned[\"unit_price\"].fillna(median_price)\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "cleaned = cleaned.drop_duplicates()\n",
    "\n",
    "cleaned.isna().sum(), cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a806625",
   "metadata": {},
   "source": [
    "> **Tip:** Donâ€™t automatically drop rows with missing values.\n",
    ">\n",
    "> Ask: *Is missingness meaningful?* For example, missing prices might mean â€œprice not recordedâ€, which could be a data quality problem you need to report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a71a4",
   "metadata": {},
   "source": [
    "## 7) Data type conversion\n",
    "\n",
    "Data types matter because they control what you can do:\n",
    "- numeric columns can be summed/averaged\n",
    "- datetime columns can be sorted, resampled, and grouped by time\n",
    "\n",
    "Two very common conversions:\n",
    "- `pd.to_numeric(...)`\n",
    "- `pd.to_datetime(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f886b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "typed = cleaned.copy()\n",
    "typed[\"order_date\"] = pd.to_datetime(typed[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "typed.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d5b05",
   "metadata": {},
   "source": [
    "> **Warning:** `errors='coerce'` turns invalid values into `NaT` (missing datetime).\n",
    ">\n",
    "> That's often useful, but it can also hide a data quality problem. Always check how many values became missing after conversion.\n",
    "\n",
    "### Exercise 4 â€” Data type conversion\n",
    "The `orders` DataFrame has `order_date` as a string. Convert it to datetime and extract the day of the week (Monday=0, Sunday=6) into a new column called `weekday`.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78874869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 4\n",
    "orders_with_date = orders.copy()\n",
    "orders_with_date[\"order_date\"] = pd.to_datetime(orders_with_date[\"order_date\"], errors=\"coerce\")\n",
    "orders_with_date[\"weekday\"] = orders_with_date[\"order_date\"].dt.dayofweek\n",
    "orders_with_date[[\"order_id\", \"order_date\", \"weekday\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "typed[\"order_date\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce329f",
   "metadata": {},
   "source": [
    "## 8) GroupBy and aggregation\n",
    "\n",
    "`groupby` answers questions like:\n",
    "- total revenue per customer\n",
    "- average quantity per product\n",
    "- number of orders per day\n",
    "\n",
    "A typical pattern is:\n",
    "1. create a useful measure (like `total = quantity * unit_price`)\n",
    "2. group by one or more columns\n",
    "3. aggregate (sum, mean, count, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = typed.copy()\n",
    "analysis[\"total\"] = analysis[\"quantity\"] * analysis[\"unit_price\"]\n",
    "\n",
    "by_customer = (\n",
    "    analysis.groupby(\"customer\", as_index=False)\n",
    "    .agg(orders=(\"order_id\", \"count\"), revenue=(\"total\", \"sum\"), avg_order_value=(\"total\", \"mean\"))\n",
    "    .sort_values(by=\"revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "by_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual example: revenue per customer\n",
    "ax = by_customer.set_index(\"customer\")[\"revenue\"].plot(kind=\"bar\", title=\"Revenue by Customer\")\n",
    "ax.set_xlabel(\"Customer\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2869aa1",
   "metadata": {},
   "source": [
    "### Exercise 3 â€” GroupBy\n",
    "Compute total quantity sold per product and sort from highest to lowest.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_by_product = (\n",
    "    analysis.groupby(\"product\", as_index=False)\n",
    "    .agg(total_qty=(\"quantity\", \"sum\"))\n",
    "    .sort_values(by=\"total_qty\", ascending=False)\n",
    ")\n",
    "qty_by_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337530",
   "metadata": {},
   "source": [
    "## 9) Merging, joining, and concatenation\n",
    "\n",
    "Data often comes in multiple tables. You combine them using:\n",
    "- `pd.merge(...)` (SQL-style joins: inner, left, right, outer)\n",
    "- `df.join(...)` (join by index, or by a key column)\n",
    "- `pd.concat([...])` (stack tables vertically or place them side-by-side)\n",
    "\n",
    "Weâ€™ll create a small product table and join it to orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a15316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a product category table to join with our orders\n",
    "products = pd.DataFrame({\n",
    "    \"product\": [\"Lunch Special\", \"Dinner Menu\"],\n",
    "    \"category\": [\"Daytime\", \"Evening\"],\n",
    "    \"avg_prep_time_mins\": [15, 25]\n",
    "})\n",
    "\n",
    "merged = pd.merge(analysis, products, on=\"product\", how=\"left\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation example (stack rows)\n",
    "more_orders = pd.DataFrame({\n",
    "    \"order_id\": [2001],\n",
    "    \"customer\": [\"Dana_999\"],\n",
    "    \"product\": [\"Lunch Special\"],\n",
    "    \"quantity\": [4],\n",
    "    \"unit_price\": [18.50],\n",
    "    \"order_date\": [pd.Timestamp(\"2025-01-15\")],\n",
    "    \"total\": [74.00],\n",
    "})\n",
    "\n",
    "combined = pd.concat([analysis, more_orders], ignore_index=True)\n",
    "combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b18c0",
   "metadata": {},
   "source": [
    "> **Common mistakes when merging:**\n",
    "> - joining on columns with different spelling/case (`Product` vs `product`)\n",
    "> - joining on columns with different types (string vs integer)\n",
    "> - accidental \"many-to-many\" joins that duplicate rows\n",
    ">\n",
    "> Tip: `validate='many_to_one'` (as shown) can catch mistakes early.\n",
    "\n",
    "### Exercise 5 â€” Merging practice\n",
    "Create a `customers` DataFrame with columns `customer` and `city` (make up 3-4 customers). Then merge it with the `analysis` DataFrame to add the city information to each order.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fe36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 5 - Get unique customers first, then create city mapping\n",
    "unique_customers = analysis[\"customer\"].unique()[:4]\n",
    "customers = pd.DataFrame({\n",
    "    \"customer\": unique_customers,\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"][:len(unique_customers)]\n",
    "})\n",
    "\n",
    "orders_with_city = pd.merge(analysis, customers, on=\"customer\", how=\"left\")\n",
    "orders_with_city[[\"order_id\", \"customer\", \"city\", \"product\", \"total\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f98d1",
   "metadata": {},
   "source": [
    "## 10) Reshaping data: `pivot` and `melt`\n",
    "\n",
    "Data can be **wide** or **long**:\n",
    "- **wide:** many columns (e.g., one column per month)\n",
    "- **long:** fewer columns, more rows (e.g., month stored as values)\n",
    "\n",
    "Why reshape?\n",
    "- many visualizations and groupby operations prefer long data\n",
    "- reporting tables often prefer wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_revenue = (\n",
    "    analysis.groupby([\"order_date\", \"product\"], as_index=False)\n",
    "    .agg(revenue=(\"total\", \"sum\"))\n",
    ")\n",
    "\n",
    "wide = daily_revenue.pivot(index=\"order_date\", columns=\"product\", values=\"revenue\").fillna(0)\n",
    "wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_again = wide.reset_index().melt(id_vars=\"order_date\", var_name=\"product\", value_name=\"revenue\")\n",
    "long_again.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62d946",
   "metadata": {},
   "source": [
    "> **Tip:** Use `pivot` when you want to reshape from long to wide format (e.g., for reporting tables).\n",
    "> Use `melt` when you want to go from wide to long format (e.g., for plotting or groupby operations).\n",
    "\n",
    "### Exercise 6 â€” Reshaping practice\n",
    "Using the `wide` DataFrame created above, convert it back to long format using `melt`, then filter to show only rows where revenue > 0.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 6\n",
    "long_filtered = wide.reset_index().melt(id_vars=\"order_date\", var_name=\"product\", value_name=\"revenue\")\n",
    "long_filtered = long_filtered[long_filtered[\"revenue\"] > 0]\n",
    "long_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4acb56",
   "metadata": {},
   "source": [
    "## 11) Working with dates and time series\n",
    "\n",
    "When a column is a datetime type, Pandas gives you powerful tools:\n",
    "- `.dt` accessor (year, month, day, weekday, etc.)\n",
    "- sorting by time\n",
    "- resampling (daily â†’ weekly/monthly)\n",
    "- rolling averages\n",
    "\n",
    "Weâ€™ll build a simple daily revenue time series and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (\n",
    "    analysis.groupby(\"order_date\", as_index=True)[\"total\"]\n",
    "    .sum()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f41962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ts.plot(marker=\"o\", title=\"Daily Revenue\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1904348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling average smooths short-term noise (window=2 days here)\n",
    "rolling = ts.rolling(window=2, min_periods=1).mean()\n",
    "\n",
    "ax = ts.plot(marker=\"o\", label=\"Daily\")\n",
    "rolling.plot(ax=ax, label=\"2-day rolling mean\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Daily Revenue with Rolling Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4744",
   "metadata": {},
   "source": [
    "> **Tip:** Rolling averages are useful for smoothing noisy data and identifying trends.\n",
    "> Common window sizes: 7 days (weekly), 30 days (monthly), depending on your data frequency.\n",
    "\n",
    "### Exercise 7 â€” Time series practice\n",
    "Using the `ts` time series created above, compute a cumulative sum of daily revenue and display it.\n",
    "\n",
    "*Try first, then run the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e58a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 7\n",
    "cumulative_revenue = ts.cumsum()\n",
    "\n",
    "ax = cumulative_revenue.plot(marker=\"o\", title=\"Cumulative Daily Revenue\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Cumulative Revenue\")\n",
    "plt.show()\n",
    "\n",
    "cumulative_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d94839",
   "metadata": {},
   "source": [
    "## 12) Exporting processed data\n",
    "\n",
    "After cleaning/analysis, you often export results for:\n",
    "- reporting\n",
    "- dashboards\n",
    "- sharing with other teams\n",
    "\n",
    "Common exports:\n",
    "- CSV (`to_csv`)\n",
    "- Excel (`to_excel`)\n",
    "- Parquet (`to_parquet`) for efficient analytics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a summary table to CSV (writes to the current working directory)\n",
    "output_path = \"chapter04_revenue_by_customer.csv\"\n",
    "by_customer.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ec9bc",
   "metadata": {},
   "source": [
    "> **Warning:** Exporting overwrites files by default if the name already exists.\n",
    ">\n",
    "> If you're experimenting, use unique filenames or export into a dedicated folder.\n",
    "\n",
    "> **Tip:** For large datasets, consider using `to_parquet()` format instead of CSV. Parquet files are:\n",
    "> - Much smaller (compressed)\n",
    "> - Faster to read/write\n",
    "> - Preserve data types automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a39c67",
   "metadata": {},
   "source": [
    "## Miniâ€‘Project: Clean and analyze a real-world dataset\n",
    "\n",
    "In this miniâ€‘project you will:\n",
    "1. Load the seaborn **flights** dataset (monthly airline passengers)\n",
    "2. Clean it (types, missing data check)\n",
    "3. Compute aggregations and summarize by year and month\n",
    "4. Plot a simple time trend\n",
    "5. Export the cleaned dataset and a summary table\n",
    "\n",
    "This is a realistic endâ€‘toâ€‘end workflow for beginner analysts using a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449256e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flights dataset from seaborn\n",
    "raw = sns.load_dataset(\"flights\")\n",
    "\n",
    "# Add some \"messy\" elements for cleaning practice\n",
    "raw_messy = raw.copy()\n",
    "raw_messy.loc[5, \"passengers\"] = np.nan  # introduce missing value\n",
    "raw_messy = pd.concat([raw_messy, raw_messy.iloc[[10]]], ignore_index=True)  # duplicate row\n",
    "\n",
    "raw_messy.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = raw_messy.copy()\n",
    "\n",
    "# 1) Check for missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(clean.isna().sum())\n",
    "\n",
    "# 2) Handle missing values - fill with median\n",
    "clean[\"passengers\"] = clean[\"passengers\"].fillna(clean[\"passengers\"].median())\n",
    "\n",
    "# 3) Remove duplicates\n",
    "clean = clean.drop_duplicates()\n",
    "\n",
    "# 4) Create a proper date column for time series analysis\n",
    "clean[\"date\"] = pd.to_datetime(clean[\"year\"].astype(str) + \"-\" + clean[\"month\"].astype(str) + \"-01\")\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(clean.isna().sum())\n",
    "print(f\"\\nShape: {clean.shape}\")\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34416774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize by year\n",
    "summary_year = (\n",
    "    clean.groupby(\"year\", as_index=False)\n",
    "    .agg(total_passengers=(\"passengers\", \"sum\"), avg_passengers=(\"passengers\", \"mean\"))\n",
    "    .sort_values(by=\"year\")\n",
    ")\n",
    "summary_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86724cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot of passengers\n",
    "ts = clean.set_index(\"date\")[\"passengers\"].sort_index()\n",
    "\n",
    "ax = ts.plot(figsize=(12, 4), title=\"Monthly Airline Passengers Over Time\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Passengers (thousands)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.to_csv(\"chapter04_clean_flights.csv\", index=False)\n",
    "summary_year.to_csv(\"chapter04_flights_summary_by_year.csv\", index=False)\n",
    "\n",
    "[\"chapter04_clean_flights.csv\", \"chapter04_flights_summary_by_year.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d88368",
   "metadata": {},
   "source": [
    "## Optional resources\n",
    "- Pandas User Guide: https://pandas.pydata.org/docs/user_guide/\n",
    "- 10 minutes to pandas: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "- Pandas `merge` docs: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "- Time series / resampling: https://pandas.pydata.org/docs/user_guide/timeseries.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b7e0",
   "metadata": {},
   "source": [
    "## Summary / Key Takeaways\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Series** = one labeled column; **DataFrame** = a table of Series.\n",
    "- The **index** provides row labels and enables powerful alignment during operations.\n",
    "\n",
    "**Workflow Essentials:**\n",
    "- Start every analysis by inspecting: `head`, `info`, `describe`, missing counts.\n",
    "- Use `loc`/`iloc` + boolean masks for safe, readable selection.\n",
    "- Clean data intentionally: measure issues â†’ choose a strategy â†’ re-check.\n",
    "- Convert types early (especially dates) using `pd.to_datetime` / `pd.to_numeric`.\n",
    "\n",
    "**Analysis & Aggregation:**\n",
    "- Summarize with `groupby` + `agg`, and visualize quick insights with simple plots.\n",
    "- Combine tables with `merge`/`join`, stack with `concat`, reshape with `pivot`/`melt`.\n",
    "\n",
    "**Time Series & Export:**\n",
    "- Use `.dt` accessor for datetime operations; use `resample` for time-based aggregation.\n",
    "- Export results with `to_csv` (and other formats) to share or report.\n",
    "\n",
    "**Exercises Completed:**\n",
    "1. Quick inspection (shape, dtypes, value counts)\n",
    "2. Filtering practice (boolean masks with `loc`)\n",
    "3. GroupBy aggregation\n",
    "4. Data type conversion (datetime, weekday extraction)\n",
    "5. Merging DataFrames\n",
    "6. Reshaping with pivot/melt\n",
    "7. Time series cumulative sum\n",
    "\n",
    "**What's Next:** In Chapter 5, you'll learn how to create effective visualizations to communicate your data insights using Matplotlib and Seaborn."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
