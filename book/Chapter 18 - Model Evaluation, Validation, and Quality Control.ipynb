{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73203143",
   "metadata": {},
   "source": [
    "# Chapter 18: Model Evaluation, Validation, and Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ed3a3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A model is only useful if it works on **new (unseen) data**. This chapter teaches you how to validate your work so your results are trustworthy.\n",
    "\n",
    "You will learn how to:\n",
    "- Split data correctly (train/test and train/validation/test)\n",
    "- Choose evaluation metrics for classification and regression\n",
    "- Spot overfitting and underfitting\n",
    "- Understand the bias–variance trade-off\n",
    "- Run simple sensitivity analysis\n",
    "- Perform quality assurance checks (especially avoiding data leakage)\n",
    "\n",
    "**Tools used:** `numpy`, `pandas`, `matplotlib`. Some parts use `scikit-learn` (recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7b19e",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "1. Explain why validation matters.\n",
    "2. Split data into train/test and train/validation/test.\n",
    "3. Use common evaluation metrics and explain what they mean.\n",
    "4. Detect overfitting and underfitting.\n",
    "5. Describe the bias–variance trade-off.\n",
    "6. Perform basic sensitivity analysis.\n",
    "7. Apply quality control checks to reduce mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used throughout the chapter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: scikit-learn (recommended)\n",
    "SKLEARN_AVAILABLE = True\n",
    "try:\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "except Exception as e:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    SKLEARN_IMPORT_ERROR = str(e)\n",
    "\n",
    "print('Ready! scikit-learn available:', SKLEARN_AVAILABLE)\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Install scikit-learn to run all sections: pip install scikit-learn')\n",
    "    print('Import error:', SKLEARN_IMPORT_ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b9f81",
   "metadata": {},
   "source": [
    "## 18.1 Importance of validation\n",
    "Validation answers: **Will my model still perform well on data it has never seen before?**\n",
    "\n",
    "If you only evaluate on training data, you can get a score that looks great but fails in real life.\n",
    "\n",
    "> **Common mistake:** Reporting training accuracy as if it were real-world accuracy.\n",
    "\n",
    "Validation helps you:\n",
    "- Detect overfitting\n",
    "- Compare models fairly\n",
    "- Choose the right metric for the goal\n",
    "- Build confidence and trust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bfed21",
   "metadata": {},
   "source": [
    "## 18.2 Train–test concepts (conceptual level)\n",
    "A common split is:\n",
    "- **Train set**: the model learns from this\n",
    "- **Test set**: the model never sees this until the final evaluation\n",
    "\n",
    "Often we add:\n",
    "- **Validation set**: used to choose model settings (hyperparameters)\n",
    "\n",
    "> **Rule:** Use the test set once at the end. If you keep looking at test results and changing the model, the test set is no longer a fair test.\n",
    "\n",
    "**Important:** Split first. Then fit preprocessing (scaling, imputation) on training data only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fbb4b",
   "metadata": {},
   "source": [
    "### Example dataset (binary classification)\n",
    "We will use a synthetic dataset where the target is 0/1.\n",
    "\n",
    "We purposely make the classes a bit imbalanced to show why accuracy can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset (self-contained)\n",
    "if SKLEARN_AVAILABLE:\n",
    "    X, y = make_classification(\n",
    "        n_samples=1200,\n",
    "        n_features=8,\n",
    "        n_informative=4,\n",
    "        n_redundant=2,\n",
    "        weights=[0.70, 0.30],\n",
    "        class_sep=1.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "else:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X = rng.normal(size=(1200, 8))\n",
    "    w = np.array([1.4, -1.0, 0.9, 0.0, 0.0, 0.6, 0.0, -0.5])\n",
    "    logits = X @ w + rng.normal(scale=0.8, size=1200)\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    # Make about 30% positives\n",
    "    y = (probs > np.quantile(probs, 0.70)).astype(int)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick class balance check\n",
    "counts = df['target'].value_counts().sort_index()\n",
    "props = df['target'].value_counts(normalize=True).sort_index()\n",
    "summary = pd.DataFrame({'count': counts, 'proportion': props})\n",
    "summary.index = ['class 0', 'class 1']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbc4b3",
   "metadata": {},
   "source": [
    "### Train–test split (practice)\n",
    "We split and keep the test set untouched until evaluation.\n",
    "\n",
    "> **Tip:** Use `stratify=y` for classification to keep similar class ratios in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train–test split (code)\n",
    "X_all = df.drop(columns=['target']).values\n",
    "y_all = df['target'].values\n",
    "\n",
    "if SKLEARN_AVAILABLE:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=0.25, random_state=42, stratify=y_all\n",
    "    )\n",
    "else:\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = np.arange(len(y_all))\n",
    "    rng.shuffle(idx)\n",
    "    split = int(0.75 * len(idx))\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "    X_train, X_test = X_all[train_idx], X_all[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "print('Train size:', len(y_train), '| Test size:', len(y_test))\n",
    "print('Train positive rate:', round(float(y_train.mean()), 3))\n",
    "print('Test positive rate:', round(float(y_test.mean()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f4e65",
   "metadata": {},
   "source": [
    "## 18.3 Evaluation metrics\n",
    "Metrics turn predictions into numbers you can compare.\n",
    "\n",
    "### Classification (binary)\n",
    "- **Accuracy**: overall fraction correct\n",
    "- **Precision**: of predicted positives, how many were truly positive?\n",
    "- **Recall**: of true positives, how many did we catch?\n",
    "- **F1-score**: balances precision and recall\n",
    "- **ROC AUC**: measures ranking performance across thresholds\n",
    "\n",
    "### Regression (numeric target)\n",
    "- **MAE**: average absolute error\n",
    "- **RMSE**: penalizes large errors more\n",
    "- **$R^2$**: fraction of variance explained\n",
    "\n",
    "> **Tip:** With imbalanced data, accuracy can hide failures. Always look at precision/recall and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for evaluation (binary classification + regression)\n",
    "\n",
    "def confusion_matrix_2x2(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    return np.array([[tn, fp], [fn, tp]], dtype=int)\n",
    "\n",
    "def metrics_from_cm(cm):\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    accuracy = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    precision = tp / max(tp + fp, 1)\n",
    "    recall = tp / max(tp + fn, 1)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-12)\n",
    "    return {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "    }\n",
    "\n",
    "def plot_cm(cm, title='Confusion matrix'):\n",
    "    fig, ax = plt.subplots(figsize=(4.8, 4))\n",
    "    ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['0', '1'])\n",
    "    ax.set_yticklabels(['0', '1'])\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha='center', va='center', color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2_manual(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return float(1 - ss_res / max(ss_tot, 1e-12))\n",
    "\n",
    "print('Helpers ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad946966",
   "metadata": {},
   "source": [
    "### 18.3.1 Regression metrics in practice\n",
    "Let's see how MAE, RMSE, and R² work on a concrete regression example.\n",
    "\n",
    "| Metric | What it measures | When to use |\n",
    "|--------|------------------|-------------|\n",
    "| **MAE** | Average absolute error | When all errors matter equally |\n",
    "| **RMSE** | Root of average squared error | When large errors are especially bad |\n",
    "| **R²** | Fraction of variance explained | To compare models (0 = baseline, 1 = perfect) |\n",
    "\n",
    "> **Tip:** RMSE will always be ≥ MAE. If RMSE >> MAE, you have some large outlier errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics demonstration\n",
    "# Create a simple regression scenario: predict y from x\n",
    "rng_demo = np.random.default_rng(99)\n",
    "n_demo = 100\n",
    "x_demo = rng_demo.uniform(0, 10, size=n_demo)\n",
    "y_true_demo = 2 * x_demo + 5 + rng_demo.normal(scale=2, size=n_demo)\n",
    "\n",
    "# Simulate predictions from three models:\n",
    "# Model A: Good predictions\n",
    "# Model B: Moderate predictions (more noise)\n",
    "# Model C: Predictions with some large outliers\n",
    "\n",
    "y_pred_A = 2 * x_demo + 5 + rng_demo.normal(scale=1.5, size=n_demo)\n",
    "y_pred_B = 2 * x_demo + 5 + rng_demo.normal(scale=4, size=n_demo)\n",
    "y_pred_C = 2 * x_demo + 5 + rng_demo.normal(scale=1.5, size=n_demo)\n",
    "# Add a few large outliers to Model C\n",
    "outlier_idx = rng_demo.choice(n_demo, size=5, replace=False)\n",
    "y_pred_C[outlier_idx] += rng_demo.choice([-15, 15], size=5)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "models = {\n",
    "    'Model A (good)': y_pred_A,\n",
    "    'Model B (noisy)': y_pred_B,\n",
    "    'Model C (outliers)': y_pred_C,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, y_pred in models.items():\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MAE': round(mae(y_true_demo, y_pred), 3),\n",
    "        'RMSE': round(rmse(y_true_demo, y_pred), 3),\n",
    "        'R²': round(r2_manual(y_true_demo, y_pred), 3),\n",
    "    })\n",
    "\n",
    "reg_metrics_df = pd.DataFrame(results)\n",
    "print('Regression Metrics Comparison:')\n",
    "display(reg_metrics_df)\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "for ax, (name, y_pred) in zip(axes, models.items()):\n",
    "    ax.scatter(y_true_demo, y_pred, alpha=0.6, s=30)\n",
    "    ax.plot([y_true_demo.min(), y_true_demo.max()], \n",
    "            [y_true_demo.min(), y_true_demo.max()], \n",
    "            'r--', label='Perfect prediction')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nNotice: Model C has similar MAE to Model A, but much higher RMSE due to outlier errors.')\n",
    "print('R² tells you what fraction of variance is explained (higher is better, max=1).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a8a60",
   "metadata": {},
   "source": [
    "### Exercise: Interpreting regression metrics\n",
    "Look at the regression metrics table above and answer these questions:\n",
    "\n",
    "1. Which model has the best overall performance?\n",
    "2. Why does Model C have a much higher RMSE than MAE compared to the other models?\n",
    "3. If you could only use one metric to compare models, which would you choose and why?\n",
    "\n",
    "> **Hint:** Think about what each metric penalizes and how outliers affect squared vs absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1603b9",
   "metadata": {},
   "source": [
    "### Start with a baseline (always!)\n",
    "Before training a model, create a simple baseline. If your model cannot beat the baseline, something is wrong.\n",
    "\n",
    "For imbalanced classification, a common baseline is predicting the **most common class** every time.\n",
    "> **Tip:** Baselines prevent you from celebrating meaningless results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: always predict the majority class from the training set\n",
    "majority_class = 0 if y_train.mean() < 0.5 else 1\n",
    "y_pred_base = np.full_like(y_test, fill_value=majority_class)\n",
    "\n",
    "cm_base = confusion_matrix_2x2(y_test, y_pred_base)\n",
    "m_base = metrics_from_cm(cm_base)\n",
    "\n",
    "print('Majority class predicted:', majority_class)\n",
    "print('Baseline metrics:', m_base)\n",
    "plot_cm(cm_base, title='Baseline confusion matrix (majority class)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfd030",
   "metadata": {},
   "source": [
    "## 18.3A Build a simple model (Logistic Regression)\n",
    "Logistic Regression is a strong first model for binary classification because it:\n",
    "- Is fast and often performs well\n",
    "- Outputs probabilities (useful for thresholds)\n",
    "- Is reasonably interpretable\n",
    "\n",
    "We will use a **pipeline** that scales features then trains the model. This is a best practice because it helps prevent leakage (the scaler is fit only on training data).\n",
    "> **Common mistake:** Scaling with the full dataset *before* splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Logistic Regression (requires scikit-learn)\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Skipping Logistic Regression: scikit-learn is not available.')\n",
    "else:\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cm = confusion_matrix_2x2(y_test, y_pred)\n",
    "    m = metrics_from_cm(cm)\n",
    "    auc_value = float(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    print('Metrics at threshold=0.5:', m)\n",
    "    print('ROC AUC:', round(auc_value, 4))\n",
    "    plot_cm(cm, title='Logistic Regression confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe43afe",
   "metadata": {},
   "source": [
    "### Visual examples: ROC and Precision–Recall curves\n",
    "If a model outputs probabilities, you can choose different decision thresholds (not always 0.5).\n",
    "Curves show the trade-offs across thresholds:\n",
    "- **ROC curve**: True Positive Rate vs False Positive Rate\n",
    "- **Precision–Recall curve**: Precision vs Recall (often more informative with imbalanced data)\n",
    "> **Tip:** If the positive class is rare, focus on Precision–Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d33ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Skipping ROC/PR curves: scikit-learn is not available.')\n",
    "else:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.2))\n",
    "\n",
    "    axes[0].plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')\n",
    "    axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    axes[0].set_title('ROC curve')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(rec, prec)\n",
    "    axes[1].set_title('Precision–Recall curve')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f91d5",
   "metadata": {},
   "source": [
    "## Exercise 1: Compute metrics from a confusion matrix\n",
    "Pick a confusion matrix and compute accuracy, precision, recall, and F1 yourself.\n",
    "\n",
    "Why? If you understand TN/FP/FN/TP, you can explain model performance clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use either the baseline or logistic regression confusion matrix (if it exists)\n",
    "cm_ex = cm_base if 'cm_base' in globals() else np.array([[50, 10], [8, 32]])\n",
    "print('Confusion matrix used:\\n', cm_ex)\n",
    "\n",
    "# TODO: compute the metrics manually\n",
    "tn, fp = cm_ex[0]\n",
    "fn, tp = cm_ex[1]\n",
    "accuracy = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "precision = tp / max(tp + fp, 1)\n",
    "recall = tp / max(tp + fn, 1)\n",
    "f1 = 2 * precision * recall / max(precision + recall, 1e-12)\n",
    "\n",
    "print({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "# Check yourself using the helper\n",
    "print('Helper check:', metrics_from_cm(cm_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69688e",
   "metadata": {},
   "source": [
    "## 18.4 Overfitting and underfitting\n",
    "Two common failure modes:\n",
    "- **Underfitting**: model is too simple → high error on both train and test\n",
    "- **Overfitting**: model is too complex → very low train error but worse test error\n",
    "\n",
    "> **Key idea:** Compare training performance vs testing (or validation) performance. Big gaps are a warning sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression example: y = sin(x) + noise\n",
    "rng = np.random.default_rng(42)\n",
    "n = 180\n",
    "x = rng.uniform(-3, 3, size=n)\n",
    "y = np.sin(x) + rng.normal(scale=0.25, size=n)\n",
    "\n",
    "# Train/test split (manual, to keep it simple)\n",
    "idx = np.arange(n)\n",
    "rng.shuffle(idx)\n",
    "split = int(0.75 * n)\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "x_train_r, x_test_r = x[train_idx], x[test_idx]\n",
    "y_train_r, y_test_r = y[train_idx], y[test_idx]\n",
    "\n",
    "plt.figure(figsize=(7.5, 4))\n",
    "plt.scatter(x_train_r, y_train_r, s=18, alpha=0.8, label='train')\n",
    "plt.scatter(x_test_r, y_test_r, s=18, alpha=0.8, label='test')\n",
    "plt.title('Noisy sin(x) regression dataset')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit polynomial regression models with different degrees (numpy only)\n",
    "def fit_poly(x_train, y_train, degree):\n",
    "    X = np.vander(x_train, N=degree + 1, increasing=True)\n",
    "    w, *_ = np.linalg.lstsq(X, y_train, rcond=None)\n",
    "    return w\n",
    "\n",
    "def predict_poly(x_values, w):\n",
    "    degree = len(w) - 1\n",
    "    X = np.vander(x_values, N=degree + 1, increasing=True)\n",
    "    return X @ w\n",
    "\n",
    "degrees = list(range(1, 16))\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for d in degrees:\n",
    "    w = fit_poly(x_train_r, y_train_r, degree=d)\n",
    "    yhat_train = predict_poly(x_train_r, w)\n",
    "    yhat_test = predict_poly(x_test_r, w)\n",
    "    train_scores.append(rmse(y_train_r, yhat_train))\n",
    "    test_scores.append(rmse(y_test_r, yhat_test))\n",
    "\n",
    "best_degree = degrees[int(np.argmin(test_scores))]\n",
    "print('Best degree on test (demo):', best_degree)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(degrees, train_scores, marker='o', label='Train RMSE')\n",
    "plt.plot(degrees, test_scores, marker='o', label='Test RMSE')\n",
    "plt.title('Overfitting vs underfitting (polynomial degree)')\n",
    "plt.xlabel('Polynomial degree (complexity)')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cba962",
   "metadata": {},
   "source": [
    "### Interpreting the overfitting plot\n",
    "In the polynomial plot above:\n",
    "- **Low degree** (left side) often underfits: the curve is too simple, so both train and test errors are relatively high.\n",
    "- **High degree** (right side) often overfits: training error keeps dropping, but test error starts to rise.\n",
    "\n",
    "**What you want:** a complexity level where test (or validation) error is low *and stable*.\n",
    "\n",
    "> **Warning:** In the demo we printed the “best degree on test.” In real projects, you should choose the degree using a **validation** set (or cross‑validation), then evaluate once on the **test** set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aabb5c",
   "metadata": {},
   "source": [
    "## 18.5 Bias and variance trade-off\n",
    "The bias–variance trade-off helps explain why a model can be “too simple” or “too complex.”\n",
    "- **Bias**: error from simplifying assumptions (model can’t capture the true pattern). High bias → underfitting.\n",
    "- **Variance**: error from sensitivity to the training data (model changes a lot if the data changes a little). High variance → overfitting.\n",
    "\n",
    "A useful mental picture:\n",
    "- High bias models are consistently wrong in a similar way.\n",
    "- High variance models are sometimes great, sometimes terrible, depending on the sample.\n",
    "\n",
    "> **Tip:** If your score changes a lot when you change the random split, variance is likely a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27133ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual demo: variance = instability across different train/test splits\n",
    "# We'll see how the \"best\" polynomial degree changes if we resplit the data.\n",
    "def find_best_degree_for_split(seed, degrees=range(1, 16)):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(x))\n",
    "    rng.shuffle(idx)\n",
    "    split = int(0.75 * len(idx))\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "    x_tr, x_te = x[train_idx], x[test_idx]\n",
    "    y_tr, y_te = y[train_idx], y[test_idx]\n",
    "\n",
    "    test_rmse = []\n",
    "    for d in degrees:\n",
    "        w = fit_poly(x_tr, y_tr, degree=d)\n",
    "        yhat_te = predict_poly(x_te, w)\n",
    "        test_rmse.append(rmse(y_te, yhat_te))\n",
    "    best_d = int(np.argmin(test_rmse)) + 1\n",
    "    return best_d, float(min(test_rmse))\n",
    "\n",
    "rows = []\n",
    "for seed in range(10):\n",
    "    best_d, best_rmse = find_best_degree_for_split(seed)\n",
    "    rows.append({'seed': seed, 'best_degree': best_d, 'best_test_rmse': best_rmse})\n",
    "\n",
    "stability_df = pd.DataFrame(rows)\n",
    "stability_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea9f62",
   "metadata": {},
   "source": [
    "## Exercise 2: Train/validation/test (choose complexity without peeking)\n",
    "Update the polynomial regression demo to use **three splits**:\n",
    "1. **Train** (60%): fit the model\n",
    "2. **Validation** (20%): choose the best polynomial degree\n",
    "3. **Test** (20%): final evaluation, used **once** at the very end\n",
    "\n",
    "### Your task:\n",
    "1. Split the `x` and `y` arrays into train/validation/test\n",
    "2. Loop through polynomial degrees 1–15\n",
    "3. For each degree: fit on train, evaluate RMSE on validation\n",
    "4. Pick the degree with the lowest validation RMSE\n",
    "5. Report the test RMSE for that chosen degree (use test set only once!)\n",
    "\n",
    "**Goal:** pick the polynomial degree using validation RMSE, then report test RMSE for that chosen degree.\n",
    "\n",
    "> **Why this matters:** Using the test set to choose settings makes performance look better than it truly is. This is a form of \"data leakage.\"\n",
    "\n",
    "> **Common mistake:** Looking at test set results multiple times and picking the model that does best. This inflates your score!\n",
    "\n",
    "### Starter template (fill in the TODOs)\n",
    "```python\n",
    "# Step 1: Create three-way split\n",
    "rng = np.random.default_rng(123)\n",
    "idx = np.arange(len(x))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "n_total = len(idx)\n",
    "n_train = int(0.60 * n_total)\n",
    "n_val = int(0.20 * n_total)\n",
    "# TODO: Create train_idx, val_idx, test_idx\n",
    "\n",
    "# Step 2: Extract x and y for each split\n",
    "# TODO: x_tr, y_tr = ...\n",
    "# TODO: x_val, y_val = ...\n",
    "# TODO: x_te, y_te = ...\n",
    "\n",
    "# Step 3: Find best degree using validation set\n",
    "degrees = list(range(1, 16))\n",
    "# TODO: Loop, fit on train, evaluate on validation\n",
    "\n",
    "# Step 4: Final evaluation on test (once!)\n",
    "# TODO: Use best degree, fit on train, evaluate on test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d67116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible solution (you can modify it)\n",
    "rng = np.random.default_rng(123)\n",
    "idx = np.arange(len(x))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "n_total = len(idx)\n",
    "n_train = int(0.60 * n_total)\n",
    "n_val = int(0.20 * n_total)\n",
    "train_idx = idx[:n_train]\n",
    "val_idx = idx[n_train:n_train + n_val]\n",
    "test_idx = idx[n_train + n_val:]\n",
    "\n",
    "x_tr, y_tr = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "x_te, y_te = x[test_idx], y[test_idx]\n",
    "\n",
    "degrees = list(range(1, 16))\n",
    "val_rmse = []\n",
    "for d in degrees:\n",
    "    w = fit_poly(x_tr, y_tr, degree=d)\n",
    "    yhat_val = predict_poly(x_val, w)\n",
    "    val_rmse.append(rmse(y_val, yhat_val))\n",
    "\n",
    "best_d = degrees[int(np.argmin(val_rmse))]\n",
    "w_final = fit_poly(x_tr, y_tr, degree=best_d)\n",
    "test_rmse = rmse(y_te, predict_poly(x_te, w_final))\n",
    "\n",
    "print('Best degree (by validation):', best_d)\n",
    "print('Test RMSE (final evaluation):', round(test_rmse, 4))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(degrees, val_rmse, marker='o')\n",
    "plt.title('Validation RMSE by polynomial degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7771f1f",
   "metadata": {},
   "source": [
    "## 18.5.1 Cross-validation (conceptual overview)\n",
    "When you only have limited data, holding out 20–25% for validation can waste valuable training samples. **Cross-validation** solves this by rotating which portion of the data is used for validation.\n",
    "\n",
    "### How K-Fold Cross-Validation Works\n",
    "1. Split the data into **K equal parts** (called \"folds\")\n",
    "2. Train K models, each time using K-1 folds for training and 1 fold for validation\n",
    "3. Average the K validation scores\n",
    "\n",
    "> **Common choice:** K=5 or K=10\n",
    "\n",
    "### Visual diagram (5-fold example)\n",
    "```\n",
    "Fold 1: [VAL] [Train] [Train] [Train] [Train]\n",
    "Fold 2: [Train] [VAL] [Train] [Train] [Train]\n",
    "Fold 3: [Train] [Train] [VAL] [Train] [Train]\n",
    "Fold 4: [Train] [Train] [Train] [VAL] [Train]\n",
    "Fold 5: [Train] [Train] [Train] [Train] [VAL]\n",
    "```\n",
    "\n",
    "Each data point gets used for validation exactly once, and for training K-1 times.\n",
    "\n",
    "### Why use cross-validation?\n",
    "- More reliable performance estimate (uses all data)\n",
    "- Reduces variance in evaluation (less dependent on one lucky/unlucky split)\n",
    "- Especially useful for small datasets\n",
    "\n",
    "> **Tip:** Cross-validation is for choosing hyperparameters and estimating performance. You still need a final held-out test set for the last evaluation.\n",
    "\n",
    "> **Warning:** Cross-validation is slower (trains K models instead of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449815d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation example (requires scikit-learn)\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Skipping cross-validation demo: scikit-learn is not available.')\n",
    "else:\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    # Use the same classification dataset\n",
    "    model_cv = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ])\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model_cv, X_all, y_all, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print('5-Fold Cross-Validation Results:')\n",
    "    print('Individual fold scores:', [round(s, 4) for s in cv_scores])\n",
    "    print('Mean accuracy:', round(cv_scores.mean(), 4))\n",
    "    print('Standard deviation:', round(cv_scores.std(), 4))\n",
    "    \n",
    "    # Visualize fold scores\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.bar(range(1, 6), cv_scores, color='steelblue', edgecolor='black')\n",
    "    plt.axhline(cv_scores.mean(), color='red', linestyle='--', label=f'Mean = {cv_scores.mean():.4f}')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('5-Fold Cross-Validation Scores')\n",
    "    plt.ylim(0.7, 1.0)\n",
    "    plt.xticks(range(1, 6))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n> Low standard deviation = stable model performance across folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e6576",
   "metadata": {},
   "source": [
    "## 18.6 Sensitivity analysis\n",
    "Sensitivity analysis asks: **How sensitive are results to small changes?**\n",
    "\n",
    "We’ll do two beginner-friendly types:\n",
    "1. **Threshold sensitivity (classification):** change the probability cutoff and watch precision/recall change.\n",
    "2. **Input perturbation:** slightly change inputs (noise / shuffle a feature) and see how performance changes.\n",
    "\n",
    "> **Tip:** Sensitivity analysis is a bridge between “model metrics” and “real-world robustness.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c67a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.6A Threshold sensitivity (requires a probability model)\n",
    "if not SKLEARN_AVAILABLE or 'y_proba' not in globals():\n",
    "    print('Run the Logistic Regression section first (and ensure scikit-learn is installed) to generate probabilities.')\n",
    "else:\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_proba >= t).astype(int)\n",
    "        cm_t = confusion_matrix_2x2(y_test, y_pred_t)\n",
    "        m_t = metrics_from_cm(cm_t)\n",
    "        rows.append({'threshold': float(t), **m_t})\n",
    "    sens_df = pd.DataFrame(rows)\n",
    "    display(sens_df.head())\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(sens_df['threshold'], sens_df['precision'], marker='o', label='Precision')\n",
    "    plt.plot(sens_df['threshold'], sens_df['recall'], marker='o', label='Recall')\n",
    "    plt.plot(sens_df['threshold'], sens_df['f1'], marker='o', label='F1')\n",
    "    plt.title('Threshold sensitivity (Logistic Regression)')\n",
    "    plt.xlabel('Decision threshold')\n",
    "    plt.ylabel('Metric value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.6B Input perturbation: add noise and shuffle a feature (robustness check)\n",
    "if not SKLEARN_AVAILABLE or 'model' not in globals():\n",
    "    print('Run the Logistic Regression section first (and ensure scikit-learn is installed).')\n",
    "else:\n",
    "    rng = np.random.default_rng(7)\n",
    "    \n",
    "    # 1) Add small noise to test features\n",
    "    X_test_noisy = X_test + rng.normal(scale=0.15, size=X_test.shape)\n",
    "    y_pred_noisy = model.predict(X_test_noisy)\n",
    "    cm_noisy = confusion_matrix_2x2(y_test, y_pred_noisy)\n",
    "    m_noisy = metrics_from_cm(cm_noisy)\n",
    "    \n",
    "    # 2) Shuffle one feature column (break its relationship to target)\n",
    "    X_test_shuffled = X_test.copy()\n",
    "    col_to_shuffle = 0\n",
    "    X_test_shuffled[:, col_to_shuffle] = rng.permutation(X_test_shuffled[:, col_to_shuffle])\n",
    "    y_pred_shuffled = model.predict(X_test_shuffled)\n",
    "    cm_shuf = confusion_matrix_2x2(y_test, y_pred_shuffled)\n",
    "    m_shuf = metrics_from_cm(cm_shuf)\n",
    "    \n",
    "    print('Original metrics:', m)\n",
    "    print('Noisy-input metrics:', m_noisy)\n",
    "    print(f'Shuffled feature_{col_to_shuffle} metrics:', m_shuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61bc89",
   "metadata": {},
   "source": [
    "## 18.7 Quality assurance checks\n",
    "Quality control is not “extra.” It is how you prevent avoidable mistakes and build trust.\n",
    "\n",
    "Here are practical checks you can apply right away:\n",
    "\n",
    "### A) Reproducibility\n",
    "- Set random seeds (splits, models)\n",
    "- Record library versions\n",
    "- Keep your preprocessing steps consistent (prefer pipelines)\n",
    "\n",
    "### B) Leakage prevention (critical!)\n",
    "- Split before any learned preprocessing (scaling, imputation)\n",
    "- Use pipelines so transforms are fit on training data only\n",
    "- Watch out for “target leakage” features that directly encode the answer\n",
    "\n",
    "### C) Sanity checks\n",
    "- Compare against a baseline\n",
    "- Look at a confusion matrix (not just one metric)\n",
    "- Check for impossible values, missing data, and unexpected duplicates\n",
    "\n",
    "### D) Stability checks\n",
    "- Re-run with different random splits\n",
    "- Check sensitivity to small perturbations\n",
    "\n",
    "> **Common mistake:** Accidentally using information from the future (or the test set) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.7A Leakage demo: scaling BEFORE split (wrong) vs pipeline (right)\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Skipping leakage demo: scikit-learn is not available.')\n",
    "else:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # WRONG: scale using ALL data, then split\n",
    "    scaler_wrong = StandardScaler()\n",
    "    X_scaled_wrong = scaler_wrong.fit_transform(X_all)\n",
    "    X_tr_w, X_te_w, y_tr_w, y_te_w = train_test_split(\n",
    "        X_scaled_wrong, y_all, test_size=0.25, random_state=42, stratify=y_all\n",
    "    )\n",
    "    clf_wrong = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf_wrong.fit(X_tr_w, y_tr_w)\n",
    "    y_pred_w = clf_wrong.predict(X_te_w)\n",
    "    cm_w = confusion_matrix_2x2(y_te_w, y_pred_w)\n",
    "    m_w = metrics_from_cm(cm_w)\n",
    "    \n",
    "    # RIGHT: split first, then scale inside a pipeline\n",
    "    X_tr_r, X_te_r, y_tr_r, y_te_r = train_test_split(\n",
    "        X_all, y_all, test_size=0.25, random_state=42, stratify=y_all\n",
    "    )\n",
    "    clf_right = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ])\n",
    "    clf_right.fit(X_tr_r, y_tr_r)\n",
    "    y_pred_r = clf_right.predict(X_te_r)\n",
    "    cm_r = confusion_matrix_2x2(y_te_r, y_pred_r)\n",
    "    m_r = metrics_from_cm(cm_r)\n",
    "    \n",
    "    print('WRONG (scale before split):', m_w)\n",
    "    print('RIGHT (pipeline):', m_r)\n",
    "    print('\\nNote: leakage can inflate scores more on real datasets than on synthetic demos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.7B Stability check: metrics across different random splits\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Skipping stability check: scikit-learn is not available.')\n",
    "else:\n",
    "    scores = []\n",
    "    for seed in range(6):\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_all, y_all, test_size=0.25, random_state=seed, stratify=y_all\n",
    "        )\n",
    "        m_split = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', LogisticRegression(max_iter=1000, random_state=seed)),\n",
    "        ])\n",
    "        m_split.fit(X_tr, y_tr)\n",
    "        y_pred_split = m_split.predict(X_te)\n",
    "        y_proba_split = m_split.predict_proba(X_te)[:, 1]\n",
    "        cm_split = confusion_matrix_2x2(y_te, y_pred_split)\n",
    "        met = metrics_from_cm(cm_split)\n",
    "        scores.append({\n",
    "            'seed': seed,\n",
    "            **met,\n",
    "            'roc_auc': float(roc_auc_score(y_te, y_proba_split)),\n",
    "        })\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6946e86",
   "metadata": {},
   "source": [
    "## Mini-project: Create a model evaluation report\n",
    "Goal: produce a short, trustworthy “evaluation report” someone else can read and believe.\n",
    "\n",
    "### What to include\n",
    "1. Dataset description (what is predicted? what counts as class 1?)\n",
    "2. Train/test split method (and seed)\n",
    "3. Baseline performance\n",
    "4. Model performance (confusion matrix + metrics + ROC AUC if available)\n",
    "5. One sensitivity analysis (threshold plot or input perturbation)\n",
    "6. One quality check (leakage prevention or stability across splits)\n",
    "\n",
    "> **Deliverable:** A short markdown summary (5–10 lines) stating which metric you prioritize and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project starter template (edit freely)\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print('Install scikit-learn to run this mini-project: pip install scikit-learn')\n",
    "else:\n",
    "    # 1) Split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_all, y_all, test_size=0.25, random_state=101, stratify=y_all\n",
    "    )\n",
    "\n",
    "    # 2) Baseline\n",
    "    maj = 0 if y_tr.mean() < 0.5 else 1\n",
    "    y_pred_base = np.full_like(y_te, maj)\n",
    "    cm_base2 = confusion_matrix_2x2(y_te, y_pred_base)\n",
    "    base_metrics = metrics_from_cm(cm_base2)\n",
    "\n",
    "    # 3) Model (pipeline)\n",
    "    model2 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=101)),\n",
    "    ])\n",
    "    model2.fit(X_tr, y_tr)\n",
    "    y_pred = model2.predict(X_te)\n",
    "    y_proba = model2.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    # 4) Report metrics\n",
    "    cm2 = confusion_matrix_2x2(y_te, y_pred)\n",
    "    metrics2 = metrics_from_cm(cm2)\n",
    "    auc2 = float(roc_auc_score(y_te, y_proba))\n",
    "\n",
    "    print('Baseline metrics:', base_metrics)\n",
    "    print('Model metrics:', metrics2)\n",
    "    print('ROC AUC:', round(auc2, 4))\n",
    "    plot_cm(cm2, title='Mini-project confusion matrix')\n",
    "\n",
    "    # 5) Threshold sensitivity (choose based on your goal)\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best = None\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_proba >= t).astype(int)\n",
    "        m_t = metrics_from_cm(confusion_matrix_2x2(y_te, y_pred_t))\n",
    "        # Example objective: maximize F1 (change this if your goal is different)\n",
    "        if best is None or m_t['f1'] > best['f1']:\n",
    "            best = {'threshold': float(t), **m_t}\n",
    "    print('Best threshold by F1 (demo):', best)\n",
    "    print('Better practice: pick threshold on a validation set, then evaluate once on test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bbb00",
   "metadata": {},
   "source": [
    "### Report template (copy and fill in)\n",
    "Use this template to write your evaluation report. Fill in the blanks based on your analysis above.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation Report\n",
    "\n",
    "**Dataset:** [Describe what the data represents and what class 1 means]\n",
    "\n",
    "**Split method:** [e.g., 75% train / 25% test, stratified, random_state=101]\n",
    "\n",
    "**Baseline performance:**\n",
    "- Accuracy: ___\n",
    "- Precision: ___\n",
    "- Recall: ___\n",
    "\n",
    "**Model performance (Logistic Regression):**\n",
    "- Accuracy: ___\n",
    "- Precision: ___\n",
    "- Recall: ___\n",
    "- F1-score: ___\n",
    "- ROC AUC: ___\n",
    "\n",
    "**Key finding:** [Does the model beat the baseline? By how much?]\n",
    "\n",
    "**Metric prioritization:** [Which metric matters most for this problem and why?]\n",
    "- Example: \"I prioritize Recall because missing a positive case (class 1) is costly.\"\n",
    "\n",
    "**Sensitivity analysis:** [What did you learn from changing the threshold?]\n",
    "\n",
    "**Quality check performed:** [What did you verify? e.g., leakage prevention, stability]\n",
    "\n",
    "**Recommendation:** [Is this model ready for use? What should be done next?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5e1b5",
   "metadata": {},
   "source": [
    "## Common Mistakes to Avoid (Beginner Checklist)\n",
    "Before finalizing your analysis, check that you haven't made these common errors:\n",
    "\n",
    "### ❌ Mistake 1: Using accuracy on imbalanced data\n",
    "If 95% of your data is class 0, predicting \"always 0\" gives 95% accuracy but is useless.\n",
    "**Fix:** Always check precision, recall, F1, and look at the confusion matrix.\n",
    "\n",
    "### ❌ Mistake 2: Scaling/preprocessing before splitting\n",
    "If you scale using the entire dataset, test set statistics \"leak\" into training.\n",
    "**Fix:** Use pipelines, or fit preprocessing on training data only.\n",
    "\n",
    "### ❌ Mistake 3: Peeking at the test set multiple times\n",
    "If you keep checking test performance and adjusting your model, the test set is no longer a fair evaluation.\n",
    "**Fix:** Use a separate validation set for tuning; touch the test set **once** at the very end.\n",
    "\n",
    "### ❌ Mistake 4: Not setting random seeds\n",
    "Without seeds, your results change every run, making debugging hard.\n",
    "**Fix:** Set `random_state` or `np.random.seed()` for reproducibility.\n",
    "\n",
    "### ❌ Mistake 5: Forgetting to compare against a baseline\n",
    "A model that beats \"random guessing\" but loses to \"always predict majority class\" is not useful.\n",
    "**Fix:** Always compute baseline metrics first.\n",
    "\n",
    "### ❌ Mistake 6: Ignoring feature importance and interpretability\n",
    "Knowing *what* the model predicts is not enough—stakeholders want to know *why*.\n",
    "**Fix:** Examine coefficients (for linear models) or use feature importance plots.\n",
    "\n",
    "### ❌ Mistake 7: Not checking for data leakage features\n",
    "Some features directly encode the target (e.g., \"purchase_amount\" when predicting \"did_purchase\").\n",
    "**Fix:** Review feature definitions carefully before modeling.\n",
    "\n",
    "> **Tip:** When in doubt, ask: \"Would I have access to this feature at prediction time in the real world?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e6bb8",
   "metadata": {},
   "source": [
    "## Summary / Key Takeaways\n",
    "\n",
    "### Core concepts covered in this chapter:\n",
    "\n",
    "| Concept | What you learned |\n",
    "|---------|------------------|\n",
    "| **Validation** | Estimates real-world performance on unseen data |\n",
    "| **Train/test split** | Training data for learning, test data for final evaluation |\n",
    "| **Train/validation/test** | Validation set for tuning hyperparameters without touching test |\n",
    "| **Cross-validation** | Rotates validation folds for more reliable estimates |\n",
    "| **Classification metrics** | Accuracy, precision, recall, F1, ROC AUC |\n",
    "| **Regression metrics** | MAE, RMSE, R² |\n",
    "| **Overfitting** | Great training score, worse test score |\n",
    "| **Underfitting** | Bad performance on both train and test |\n",
    "| **Bias-variance trade-off** | Balance between model simplicity and flexibility |\n",
    "| **Sensitivity analysis** | Test robustness to thresholds and input changes |\n",
    "| **Quality assurance** | Leakage prevention, reproducibility, sanity checks |\n",
    "\n",
    "### Remember:\n",
    "- ✅ Always compare against a baseline\n",
    "- ✅ Use metrics that match your business goal\n",
    "- ✅ Split data before any preprocessing\n",
    "- ✅ Use the test set only once at the end\n",
    "- ✅ Check sensitivity to thresholds and perturbations\n",
    "- ✅ Document your process for reproducibility\n",
    "\n",
    "> **Final thought:** A model is only as trustworthy as its evaluation. Take the time to validate properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c1ac4",
   "metadata": {},
   "source": [
    "## Optional references\n",
    "- scikit-learn model evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- Cross-validation (concepts and tools): https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "- Precision/Recall trade-offs: https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics\n",
    "- Data leakage (general explanation): https://scikit-learn.org/stable/common_pitfalls.html#data-leakage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
