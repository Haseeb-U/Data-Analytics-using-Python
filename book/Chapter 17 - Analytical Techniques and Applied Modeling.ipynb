{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab086dc",
   "metadata": {},
   "source": [
    "# Chapter 17: Analytical Techniques and Applied Modeling\n",
    "\n",
    "**Part B – Data Analytics Process and Methodology**\n",
    "\n",
    "---\n",
    "\n",
    "In this chapter, you will learn the core analytical techniques used in real-world data analytics projects. We move beyond exploration and start applying methods to answer business questions, identify patterns, and make predictions.\n",
    "\n",
    "This chapter covers:\n",
    "- **Descriptive analytics** – Summarizing what happened\n",
    "- **Trend analysis** – Understanding direction over time\n",
    "- **Time-series fundamentals** – Working with time-indexed data\n",
    "- **Segmentation and clustering** – Grouping similar items\n",
    "- **Forecasting basics** – Predicting what comes next\n",
    "- **Model selection** – Choosing the right approach\n",
    "\n",
    "These techniques form the foundation of applied analytics. Whether you're analyzing sales, customer behavior, or operational data, you'll use these methods repeatedly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483251fd",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "1. Use descriptive analytics to summarize data and answer common business questions.\n",
    "2. Detect and visualize trends over time.\n",
    "3. Work with time-indexed data and compute rolling metrics.\n",
    "4. Understand segmentation vs clustering and build a simple clustering model (if scikit-learn is available).\n",
    "5. Build and compare basic forecasts (naïve, moving average, exponential smoothing).\n",
    "6. Choose a modeling approach using clear criteria (goal, data, interpretability, cost of errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used throughout the chapter\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Ready! numpy:', np.__version__, '| pandas:', pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11731a",
   "metadata": {},
   "source": [
    "## Loading our dataset\n",
    "\n",
    "We'll use the **flights** dataset from seaborn — a classic time-series dataset containing monthly airline passenger counts from 1949-1960. This is perfect for:\n",
    "- Trend analysis\n",
    "- Time-series fundamentals\n",
    "- Forecasting basics\n",
    "\n",
    "We'll also use the **iris** dataset for segmentation/clustering examples.\n",
    "\n",
    "Why use real datasets?\n",
    "- You can run the notebook without downloading files\n",
    "- The patterns are real and well-documented\n",
    "- You learn to work with actual data quirks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flights dataset from seaborn\n",
    "flights = sns.load_dataset(\"flights\")\n",
    "\n",
    "# Create a proper datetime index\n",
    "flights[\"date\"] = pd.to_datetime(flights[\"year\"].astype(str) + \"-\" + flights[\"month\"].astype(str) + \"-01\")\n",
    "\n",
    "# Create our time-series dataframe\n",
    "ts = pd.DataFrame({\n",
    "    \"date\": flights[\"date\"],\n",
    "    \"orders\": flights[\"passengers\"],  # Rename to match our examples\n",
    "    \"month\": flights[\"date\"].dt.to_period(\"M\").astype(str),\n",
    "    \"dow\": flights[\"date\"].dt.day_name()\n",
    "})\n",
    "\n",
    "# Sort by date\n",
    "ts = ts.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {ts.shape}\")\n",
    "print(f\"Date range: {ts['date'].min()} to {ts['date'].max()}\")\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2c05b",
   "metadata": {},
   "source": [
    "# 17.1 Descriptive analytics techniques\n",
    "Descriptive analytics answers: **“What happened?”**\n",
    "It focuses on:\n",
    "- Summaries (mean, median, min/max, counts)\n",
    "- Comparisons (this month vs last month)\n",
    "- Breakdown by categories (day-of-week, region, product)\n",
    "- Basic KPIs (totals, growth rates, conversion rates)\n",
    "\n",
    "### Why it matters\n",
    "Before you build any model, you should know what your data looks like. Many real projects stop here because the descriptive insights are already actionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba35e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive stats for daily orders\n",
    "ts['orders'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84482017",
   "metadata": {},
   "source": [
    "### Grouping and aggregation\n",
    "A common descriptive task is: **summarize by time bucket** (month) or category (day of week).\n",
    "We use `groupby` because it makes the “split → apply → combine” workflow easy:\n",
    "- Split data into groups (e.g., each month)\n",
    "- Apply an aggregation (sum, mean, count)\n",
    "- Combine results into a clean table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37583d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = (\n",
    "    ts.groupby('month', as_index=False)\n",
    "      .agg(total_orders=('orders', 'sum'), avg_daily_orders=('orders', 'mean'))\n",
    ")\n",
    "monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders by month name (descriptive insight: which months are strongest?)\n",
    "# Note: The flights dataset is monthly, so we analyze by month name (seasonality)\n",
    "# rather than day of week\n",
    "\n",
    "# Extract month name for seasonality analysis\n",
    "ts['month_name'] = ts['date'].dt.month_name()\n",
    "\n",
    "month_summary = (\n",
    "    ts.groupby('month_name', as_index=False)\n",
    "      .agg(avg_orders=('orders', 'mean'), total_orders=('orders', 'sum'), observations=('orders', 'size'))\n",
    ")\n",
    "\n",
    "# Put months in calendar order for readability\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "month_summary['month_name'] = pd.Categorical(month_summary['month_name'], categories=month_order, ordered=True)\n",
    "month_summary.sort_values('month_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5a9a9",
   "metadata": {},
   "source": [
    "### Visual descriptive analytics (quick plots)\n",
    "Plotting is often the fastest way to spot patterns.\n",
    "Below, we plot:\n",
    "- Daily orders (lots of noise)\n",
    "- A smoother rolling average (easier to see trend)\n",
    "\n",
    "Tip: Rolling averages are a *descriptive* technique. They do not “predict” the future by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0febafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot monthly orders with a rolling average to smooth out variations\n",
    "ts_sorted = ts.sort_values('date').copy()\n",
    "\n",
    "# For monthly data, use a 12-month rolling average to show the trend\n",
    "ts_sorted['orders_rolling_12'] = ts_sorted['orders'].rolling(window=12).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(ts_sorted['date'], ts_sorted['orders'], alpha=0.5, marker='o', markersize=3, label='Monthly orders')\n",
    "ax.plot(ts_sorted['date'], ts_sorted['orders_rolling_12'], linewidth=2, label='12-month rolling mean')\n",
    "ax.set_title('Monthly Orders with Rolling Average')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Orders (passengers)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac68b85",
   "metadata": {},
   "source": [
    "> **Common mistakes (descriptive analytics)**\n",
    "> - Mixing totals and averages: a month with more days can have a higher total but a similar daily average.\n",
    "> - Ignoring missing dates: time series often has gaps; you must check date continuity.\n",
    "> - Using mean only: the median is often more “typical” when data has outliers.\n",
    "> - Forgetting units: are we counting orders, dollars, or customers?\n",
    "\n",
    "### Exercise 17.1 (quick practice)\n",
    "1. Compute the **median** orders per month.\n",
    "2. Find the month with the highest **average orders**.\n",
    "3. Create a bar chart of average orders by **month name** (seasonality analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e36fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17.1 - Starter code (fill in the TODOs)\n",
    "exercise_monthly = (\n",
    "    ts.groupby('month', as_index=False)\n",
    "      .agg(\n",
    "          median_orders=('orders', 'median'),\n",
    "          avg_orders=('orders', 'mean')\n",
    "      )\n",
    ")\n",
    "# TODO: find the month with highest avg_orders\n",
    "best_month_row = exercise_monthly.sort_values('avg_orders', ascending=False).head(1)\n",
    "display(best_month_row)\n",
    "\n",
    "# TODO: bar chart of avg orders by month name (seasonality)\n",
    "month_plot = month_summary.sort_values('month_name')\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(month_plot['month_name'].astype(str), month_plot['avg_orders'])\n",
    "ax.set_title('Average Orders by Month (Seasonality)')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Orders')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b818e",
   "metadata": {},
   "source": [
    "# 17.2 Trend analysis\n",
    "Trend analysis asks: **“Is the overall level increasing, decreasing, or staying stable?”**\n",
    "Trends are important because they affect planning (inventory, staffing, budgeting).\n",
    "We will look at trends using:\n",
    "- A line chart over time\n",
    "- Rolling averages (smoothing)\n",
    "- Simple linear trend estimation (a very basic model)\n",
    "\n",
    "Important idea: a trend is usually easier to see at a **higher time level** (weekly/monthly) than daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8011074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to weekly to reduce noise\n",
    "weekly_ts = (\n",
    "    ts.set_index('date')\n",
    "      .resample('W')\n",
    "      .agg(orders=('orders', 'sum'))\n",
    "      .reset_index()\n",
    ")\n",
    "weekly_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weekly totals and a rolling mean\n",
    "weekly_ts['orders_roll_8'] = weekly_ts['orders'].rolling(8).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(weekly_ts['date'], weekly_ts['orders'], alpha=0.4, label='Weekly total orders')\n",
    "ax.plot(weekly_ts['date'], weekly_ts['orders_roll_8'], linewidth=2, label='8-week rolling mean')\n",
    "ax.set_title('Weekly Orders (Trend View)')\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44693648",
   "metadata": {},
   "source": [
    "### A simple linear trend line (conceptual)\n",
    "A linear trend is the simplest “model” for trend:\n",
    "$$\text{orders} pprox a + b dot t$$\n",
    "- $t$ is time as 0, 1, 2, ...\n",
    "- $b$ is the trend slope (positive = increasing)\n",
    "We use `np.polyfit` for a beginner-friendly approach.\n",
    "Warning: A linear trend is often **too simple** for real seasonality. It is mainly a baseline and a way to quantify direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear trend on weekly data\n",
    "t = np.arange(len(weekly_ts))\n",
    "y = weekly_ts['orders'].to_numpy()\n",
    "slope, intercept = np.polyfit(t, y, deg=1)\n",
    "weekly_ts['trend_line'] = intercept + slope * t\n",
    "print(f'Trend slope (orders per week): {slope:.2f}')\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(weekly_ts['date'], weekly_ts['orders'], alpha=0.4, label='Weekly orders')\n",
    "ax.plot(weekly_ts['date'], weekly_ts['trend_line'], linewidth=2, label='Linear trend')\n",
    "ax.set_title('Weekly Orders with Linear Trend')\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b28875",
   "metadata": {},
   "source": [
    "### Exercise 17.2\n",
    "1. Recompute the trend line using **monthly totals** instead of weekly totals.\n",
    "2. Compare the slope magnitude and explain (in a sentence) why the unit changes.\n",
    "3. Optional: try a different smoothing window (e.g., 4-week rolling mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17.2 - Starter code\n",
    "monthly_ts = (\n",
    "    ts.set_index('date')\n",
    "      .resample('MS')\n",
    "      .agg(orders=('orders', 'sum'))\n",
    "      .reset_index()\n",
    ")\n",
    "t_m = np.arange(len(monthly_ts))\n",
    "y_m = monthly_ts['orders'].to_numpy()\n",
    "slope_m, intercept_m = np.polyfit(t_m, y_m, deg=1)\n",
    "monthly_ts['trend_line'] = intercept_m + slope_m * t_m\n",
    "print(f'Monthly trend slope (orders per month): {slope_m:.2f}')\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(monthly_ts['date'], monthly_ts['orders'], marker='o', label='Monthly orders')\n",
    "ax.plot(monthly_ts['date'], monthly_ts['trend_line'], linewidth=2, label='Linear trend')\n",
    "ax.set_title('Monthly Orders with Linear Trend')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f57ed",
   "metadata": {},
   "source": [
    "# 17.3 Time-series analysis fundamentals\n",
    "A **time series** is data where each observation is associated with a timestamp.\n",
    "Time series analysis often includes:\n",
    "- Sorting and indexing by time\n",
    "- Resampling (daily → weekly/monthly)\n",
    "- Rolling statistics (rolling mean, rolling std)\n",
    "- Lag features (yesterday’s value) and differencing (change over time)\n",
    "- Thinking about seasonality (weekly/monthly/yearly cycles)\n",
    "Tip: Always check the **frequency** and whether dates are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ed221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the time series into a DateTimeIndex for time-series operations\n",
    "ts_idx = ts.set_index('date').sort_index()\n",
    "# Check for missing dates\n",
    "expected = pd.date_range(ts_idx.index.min(), ts_idx.index.max(), freq='D')\n",
    "missing_dates = expected.difference(ts_idx.index)\n",
    "print('Missing dates:', len(missing_dates))\n",
    "ts_idx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47132e6b",
   "metadata": {},
   "source": [
    "### Rolling statistics: mean and volatility\n",
    "Rolling mean shows the “local average” around each date.\n",
    "Rolling standard deviation (std) is a basic way to see **volatility** (how noisy/variable values are).\n",
    "We use rolling windows because:\n",
    "- Real systems change over time (what was true last year might not be true now)\n",
    "- Rolling metrics help detect shifts and instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_idx['roll_mean_30'] = ts_idx['orders'].rolling(30).mean()\n",
    "ts_idx['roll_std_30'] = ts_idx['orders'].rolling(30).std()\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(ts_idx.index, ts_idx['orders'], alpha=0.25, label='Daily orders')\n",
    "ax.plot(ts_idx.index, ts_idx['roll_mean_30'], linewidth=2, label='30-day rolling mean')\n",
    "ax.set_title('Rolling Mean (Smoothing)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49594555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(ts_idx.index, ts_idx['roll_std_30'], linewidth=2, color='tab:orange', label='30-day rolling std')\n",
    "ax.set_title('Rolling Standard Deviation (Volatility)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Std of orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c196b",
   "metadata": {},
   "source": [
    "### Lag features and differencing\n",
    "Two very common time-series transformations are:\n",
    "- **Lag**: use the previous value(s) as features (e.g., yesterday’s orders)\n",
    "- **Difference**: look at change (today - yesterday)\n",
    "Why do we do this?\n",
    "- Lags often carry useful information (recent history matters)\n",
    "- Differencing can reduce trend and make patterns easier to model\n",
    "Warning: lag features create missing values at the top (because there is no “previous day” for the first row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feats = ts_idx[['orders']].copy()\n",
    "ts_feats['lag_1'] = ts_feats['orders'].shift(1)\n",
    "ts_feats['lag_7'] = ts_feats['orders'].shift(7)\n",
    "ts_feats['diff_1'] = ts_feats['orders'].diff(1)\n",
    "ts_feats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07b935",
   "metadata": {},
   "source": [
    "### Exercise 17.3\n",
    "1. Create a `lag_14` feature.\n",
    "2. Create a `diff_7` feature (today - value 7 days ago).\n",
    "3. Plot `diff_1` over time and describe (in words) what it shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17.3 - Starter code\n",
    "ts_feats_ex = ts_idx[['orders']].copy()\n",
    "ts_feats_ex['lag_14'] = ts_feats_ex['orders'].shift(14)\n",
    "ts_feats_ex['diff_7'] = ts_feats_ex['orders'].diff(7)\n",
    "ts_feats_ex['diff_1'] = ts_feats_ex['orders'].diff(1)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(ts_feats_ex.index, ts_feats_ex['diff_1'], alpha=0.6)\n",
    "ax.axhline(0, color='black', linewidth=1)\n",
    "ax.set_title('Daily Change in Orders (diff_1)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Change vs previous day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8798bf",
   "metadata": {},
   "source": [
    "# 17.4 Segmentation and clustering concepts\n",
    "Segmentation is the idea of grouping similar things (customers, products, stores) so you can treat each group differently.\n",
    "There are two common approaches:\n",
    "\n",
    "## A) Rule-based segmentation (easy and interpretable)\n",
    "You define rules like:\n",
    "- “High value” customers = spend > $500/year\n",
    "- “New customers” = first purchase within 30 days\n",
    "This is great for beginners and business communication.\n",
    "## B) Clustering (data-driven grouping)\n",
    "The algorithm forms groups based on similarity.\n",
    "Common example: **K-Means** clustering.\n",
    "Warning: Clustering does **not** know your business meaning. After clustering, you must interpret the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c53056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple customer dataset for segmentation/clustering\n",
    "n_customers = 600\n",
    "customer_id = [f'C{str(i).zfill(4)}' for i in range(1, n_customers + 1)]\n",
    "# Create three 'behavior types' to make clusters visible\n",
    "segment = np.random.choice(['Budget', 'Regular', 'Premium'], size=n_customers, p=[0.45, 0.4, 0.15])\n",
    "orders_per_year = np.where(\n",
    "    segment == 'Budget', np.random.poisson(4, n_customers),\n",
    "    np.where(segment == 'Regular', np.random.poisson(10, n_customers), np.random.poisson(18, n_customers))\n",
    ")\n",
    "avg_order_value = np.where(\n",
    "    segment == 'Budget', np.random.normal(25, 8, n_customers),\n",
    "    np.where(segment == 'Regular', np.random.normal(45, 10, n_customers), np.random.normal(85, 15, n_customers))\n",
    ")\n",
    "avg_order_value = np.clip(avg_order_value, 5, None)\n",
    "days_since_last_purchase = np.where(\n",
    "    segment == 'Budget', np.random.gamma(3, 18, n_customers),\n",
    "    np.where(segment == 'Regular', np.random.gamma(3, 10, n_customers), np.random.gamma(3, 6, n_customers))\n",
    ")\n",
    "days_since_last_purchase = np.clip(days_since_last_purchase, 0, None)\n",
    "annual_spend = orders_per_year * avg_order_value\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': customer_id,\n",
    "    'orders_per_year': orders_per_year,\n",
    "    'avg_order_value': avg_order_value,\n",
    "    'days_since_last_purchase': days_since_last_purchase,\n",
    "    'annual_spend': annual_spend,\n",
    "})\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dd492",
   "metadata": {},
   "source": [
    "## 17.4A Rule-based segmentation (RFM-style idea)\n",
    "A popular segmentation idea is **RFM**:\n",
    "- **R**ecency: how recently did the customer buy?\n",
    "- **F**requency: how often do they buy?\n",
    "- **M**onetary: how much do they spend?\n",
    "We will do a simplified version using:\n",
    "- Recency = `days_since_last_purchase`\n",
    "- Frequency = `orders_per_year`\n",
    "- Monetary = `annual_spend`\n",
    "Why this is great for beginners:\n",
    "- Very interpretable\n",
    "- Easy to explain to stakeholders\n",
    "- No special libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple segments using quantiles (beginner-friendly)\n",
    "seg = customers.copy()\n",
    "# Lower recency is better (more recent)\n",
    "seg['recency_bucket'] = pd.qcut(seg['days_since_last_purchase'], q=3, labels=['Recent', 'Warm', 'Cold'])\n",
    "seg['frequency_bucket'] = pd.qcut(seg['orders_per_year'], q=3, labels=['Low Freq', 'Mid Freq', 'High Freq'])\n",
    "seg['monetary_bucket'] = pd.qcut(seg['annual_spend'], q=3, labels=['Low Spend', 'Mid Spend', 'High Spend'])\n",
    "seg['segment_label'] = (\n",
    "    seg['recency_bucket'].astype(str)\n",
    "    + ' | ' + seg['frequency_bucket'].astype(str)\n",
    "    + ' | ' + seg['monetary_bucket'].astype(str)\n",
    ")\n",
    "seg['segment_label'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d0371",
   "metadata": {},
   "source": [
    "### Tip: standardize names and keep it simple\n",
    "Segments are only useful if people can understand and use them.\n",
    "A helpful pattern is to **rename** common segment combinations into short names like:\n",
    "- “Champions”\n",
    "- “At Risk”\n",
    "- “New / Promising”\n",
    "This naming step is a business decision, not a math decision.\n",
    "### Exercise 17.4A\n",
    "1. Compute average `annual_spend` by `recency_bucket`.\n",
    "2. Which bucket has the highest average spend?\n",
    "3. Create a pivot table: rows = `recency_bucket`, columns = `frequency_bucket`, values = average `annual_spend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17.4A - Starter code\n",
    "avg_by_recency = seg.groupby('recency_bucket', as_index=False).agg(avg_spend=('annual_spend', 'mean'))\n",
    "display(avg_by_recency)\n",
    "pivot = pd.pivot_table(\n",
    "    seg,\n",
    "    index='recency_bucket',\n",
    "    columns='frequency_bucket',\n",
    "    values='annual_spend',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0a741",
   "metadata": {},
   "source": [
    "## 17.4B Clustering with K-Means (optional: requires scikit-learn)\n",
    "K-Means tries to put points into $k$ groups by minimizing distances inside each group.\n",
    "Two important beginner points:\n",
    "1. **Scaling matters**: features with larger numbers can dominate distance. We usually standardize features.\n",
    "2. **Choosing k** is not automatic: you test a few values and use business judgement.\n",
    "If `scikit-learn` is not installed, the code will show you how to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf646a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing scikit-learn. If it's missing, we'll provide a helpful message.\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "SKLEARN_AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"\"\"\n",
    "    ⚠️ scikit-learn is not installed.\n",
    "    \n",
    "    To install it, run one of the following commands:\n",
    "    \n",
    "    Using pip:\n",
    "        pip install scikit-learn\n",
    "    \n",
    "    Using conda:\n",
    "        conda install scikit-learn\n",
    "    \n",
    "    After installation, restart your kernel and re-run this notebook.\n",
    "    \n",
    "    Note: The clustering examples in this section require scikit-learn.\n",
    "    You can still follow along with the rule-based segmentation approach.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKLEARN_AVAILABLE:\n",
    "    features = customers[['orders_per_year', 'avg_order_value', 'days_since_last_purchase']].copy()\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(features)\n",
    "    # Choose k=3 for demonstration (we generated 3 behavior types)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    clustered = customers.copy()\n",
    "    clustered['cluster'] = clusters\n",
    "    display(clustered.groupby('cluster').mean(numeric_only=True))\n",
    "    # Visualize in 2D (orders_per_year vs avg_order_value)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    scatter = ax.scatter(\n",
    "        clustered['orders_per_year'],\n",
    "        clustered['avg_order_value'],\n",
    "        c=clustered['cluster'],\n",
    "        cmap='tab10',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    ax.set_title('Customer Clusters (K-Means)')\n",
    "    ax.set_xlabel('Orders per year')\n",
    "    ax.set_ylabel('Average order value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407b4e4",
   "metadata": {},
   "source": [
    "> **Common mistakes (clustering)**\n",
    "> - Not scaling features (distance gets dominated by one column)\n",
    "> - Treating cluster IDs as “ranked” (cluster 2 is not automatically “better” than cluster 1)\n",
    "> - Expecting perfect clusters in messy real-world data\n",
    "> - Using clustering when you actually need a supervised model (when you have labels)\n",
    "\n",
    "### Exercise 17.4B\n",
    "If scikit-learn is available:\n",
    "1. Try `k=2`, `k=4`, `k=5`.\n",
    "2. Compare the cluster summaries. Which k produces the most interpretable groups?\n",
    "3. (Optional) Create a simple label for each cluster based on its averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e96575",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKLEARN_AVAILABLE:\n",
    "    def fit_kmeans(customers_df: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "        feats = customers_df[['orders_per_year', 'avg_order_value', 'days_since_last_purchase']].copy()\n",
    "        X = StandardScaler().fit_transform(feats)\n",
    "        model = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        labels = model.fit_predict(X)\n",
    "        out = customers_df.copy()\n",
    "        out['cluster'] = labels\n",
    "        return out\n",
    "\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        tmp = fit_kmeans(customers, k)\n",
    "        summary = tmp.groupby('cluster').mean(numeric_only=True)\n",
    "        print(f'\\nK={k} cluster means')\n",
    "        display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c969c",
   "metadata": {},
   "source": [
    "# 17.5 Forecasting basics\n",
    "Forecasting answers: **“What might happen next?”**\n",
    "As a beginner, you should start with **simple forecasting baselines**. They are:\n",
    "- Easy to implement\n",
    "- Easy to explain\n",
    "- Often surprisingly strong\n",
    "We will build and compare:\n",
    "- Naïve forecast (tomorrow = today)\n",
    "- Moving average forecast\n",
    "- Simple exponential smoothing (implemented manually)\n",
    "Important: forecasting should be evaluated on a **holdout period** (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a 1D series for forecasting\n",
    "series = ts_idx['orders'].asfreq('D')\n",
    "# Train-test split: last 90 days for testing\n",
    "test_days = 90\n",
    "train = series.iloc[:-test_days]\n",
    "test = series.iloc[-test_days:]\n",
    "train.index.min(), train.index.max(), test.index.min(), test.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    # Mean Absolute Error: easy to interpret (average absolute mistake)\n",
    "    return float(np.mean(np.abs(y_true.to_numpy() - y_pred.to_numpy())))\n",
    "def rmse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    return float(np.sqrt(np.mean((y_true.to_numpy() - y_pred.to_numpy()) ** 2)))\n",
    "\n",
    "# 1) Naïve forecast: predict today's value as yesterday's value\n",
    "naive_pred = test.copy()\n",
    "naive_pred[:] = series.shift(1).loc[test.index]\n",
    "\n",
    "print('Naïve MAE:', mae(test, naive_pred))\n",
    "print('Naïve RMSE:', rmse(test, naive_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Moving average forecast: use average of last N days from training/history\n",
    "def moving_average_forecast(full_series: pd.Series, forecast_index: pd.DatetimeIndex, window: int) -> pd.Series:\n",
    "    # For each date in forecast_index, average the previous `window` days\n",
    "    preds = []\n",
    "    for dt in forecast_index:\n",
    "        history = full_series.loc[:dt - pd.Timedelta(days=1)]\n",
    "        preds.append(float(history.tail(window).mean()))\n",
    "    return pd.Series(preds, index=forecast_index)\n",
    "\n",
    "ma7_pred = moving_average_forecast(series, test.index, window=7)\n",
    "ma28_pred = moving_average_forecast(series, test.index, window=28)\n",
    "print('MA(7) MAE:', mae(test, ma7_pred))\n",
    "print('MA(28) MAE:', mae(test, ma28_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b058f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Simple Exponential Smoothing (SES)\n",
    "# Level-only model:\n",
    "#   level_t = alpha * y_t + (1-alpha) * level_{t-1}\n",
    "# Forecast = last level\n",
    "def simple_exponential_smoothing_forecast(train_series: pd.Series, forecast_index: pd.DatetimeIndex, alpha: float) -> pd.Series:\n",
    "    if not (0 < alpha <= 1):\n",
    "        raise ValueError('alpha must be in (0, 1]')\n",
    "    # Initialize level using the first value\n",
    "    level = float(train_series.iloc[0])\n",
    "    for y in train_series.iloc[1:]:\n",
    "        level = alpha * float(y) + (1 - alpha) * level\n",
    "    # Forecast constant level into the future\n",
    "    return pd.Series([level] * len(forecast_index), index=forecast_index)\n",
    "\n",
    "ses_02 = simple_exponential_smoothing_forecast(train, test.index, alpha=0.2)\n",
    "ses_06 = simple_exponential_smoothing_forecast(train, test.index, alpha=0.6)\n",
    "print('SES(alpha=0.2) MAE:', mae(test, ses_02))\n",
    "print('SES(alpha=0.6) MAE:', mae(test, ses_06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare forecasts visually\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(train.index[-120:], train.iloc[-120:], label='Train (last 120 days)', alpha=0.6)\n",
    "ax.plot(test.index, test, label='Test (actual)', linewidth=2)\n",
    "ax.plot(test.index, naive_pred, label='Naïve', alpha=0.9)\n",
    "ax.plot(test.index, ma7_pred, label='MA(7)', alpha=0.9)\n",
    "ax.plot(test.index, ses_06, label='SES alpha=0.6', alpha=0.9)\n",
    "ax.set_title('Forecast Comparison on Test Period')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f5cd0",
   "metadata": {},
   "source": [
    "> **Beginner tip (forecasting)**: Always beat a baseline.\n",
    "> If your “advanced” model is not better than naïve or moving average on test data, it is probably not ready.\n",
    "\n",
    "### Exercise 17.5\n",
    "1. Compute MAE and RMSE for all methods (Naïve, MA(7), MA(28), SES alpha=0.2, SES alpha=0.6).\n",
    "2. Which method performs best on this dataset?\n",
    "3. Change the test period to 180 days and re-evaluate. Do results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 17.5 - Starter code\n",
    "methods = {\n",
    "    'Naïve': naive_pred,\n",
    "    'MA(7)': ma7_pred,\n",
    "    'MA(28)': ma28_pred,\n",
    "    'SES(0.2)': ses_02,\n",
    "    'SES(0.6)': ses_06,\n",
    "}\n",
    "rows = []\n",
    "for name, pred in methods.items():\n",
    "    rows.append({\n",
    "        'method': name,\n",
    "        'MAE': mae(test, pred),\n",
    "        'RMSE': rmse(test, pred),\n",
    "    })\n",
    "results = pd.DataFrame(rows).sort_values('MAE')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d962f64",
   "metadata": {},
   "source": [
    "# 17.6 Model selection considerations\n",
    "Model selection is the skill of choosing a method that fits your goal, data, and constraints.\n",
    "\n",
    "## A Beginner's Checklist for Model Selection\n",
    "\n",
    "| Question | Why It Matters |\n",
    "|----------|----------------|\n",
    "| 1. **What is your goal?** | Describing, segmenting, forecasting, or predicting a label require different approaches |\n",
    "| 2. **What data do you have?** | Do you have enough history? Do you have labels? What's the quality? |\n",
    "| 3. **Who needs to understand it?** | Stakeholders often need simple, explainable methods |\n",
    "| 4. **What's the cost of errors?** | Is it worse to over-predict or under-predict? |\n",
    "| 5. **Who will maintain it?** | Will this model be updated regularly? Who will own it? |\n",
    "| 6. **What's the simplest approach?** | Always start with a baseline you can beat |\n",
    "\n",
    "## Decision Guide: Which Technique to Use?\n",
    "\n",
    "```\n",
    "START HERE\n",
    "    │\n",
    "    ▼\n",
    "What do you want to know?\n",
    "    │\n",
    "    ├── \"What happened?\" ──────────────────► DESCRIPTIVE ANALYTICS\n",
    "    │                                         (groupby, summary stats, KPIs)\n",
    "    │\n",
    "    ├── \"Is there an upward/downward trend?\" ► TREND ANALYSIS\n",
    "    │                                         (rolling averages, linear fit)\n",
    "    │\n",
    "    ├── \"What will happen next?\" ──────────► FORECASTING\n",
    "    │   │                                     (naïve, MA, exponential smoothing)\n",
    "    │   └── Do you have time-indexed data? \n",
    "    │       └── Yes ► TIME SERIES ANALYSIS\n",
    "    │\n",
    "    └── \"How can I group similar items?\" ───► SEGMENTATION / CLUSTERING\n",
    "        │\n",
    "        ├── Do you know the rules? ─► Yes ──► Rule-based segmentation (RFM)\n",
    "        │\n",
    "        └── No ────────────────────────────► K-Means clustering\n",
    "```\n",
    "\n",
    "### Picking techniques (practical guidance)\n",
    "- **Descriptive analytics**: start here always.\n",
    "- **Trend analysis**: use rolling averages; quantify with a trend line if helpful.\n",
    "- **Time series forecasting**: start with naïve and moving average, then add complexity.\n",
    "- **Segmentation**: start with rule-based (RFM-like), then consider clustering.\n",
    "- **Clustering**: use when you *don't* have labels and want natural groupings.\n",
    "\n",
    "> **Tip:** The best model is often NOT the most complex one. It's the one that:\n",
    "> - Answers the business question clearly\n",
    "> - Can be explained to stakeholders\n",
    "> - Is maintainable over time\n",
    "> - Beats a simple baseline on your test data\n",
    "\n",
    "### Optional Resources\n",
    "- [scikit-learn Model Selection Overview](https://scikit-learn.org/stable/model_selection.html)\n",
    "- [Forecasting: Principles and Practice (Free Online Book)](https://otexts.com/fpp3/)\n",
    "- [Towards Data Science: Model Selection Guide](https://towardsdatascience.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443e243",
   "metadata": {},
   "source": [
    "## Mini-project (Chapter 17)\n",
    "### Scenario\n",
    "You are an analyst for a small online store. You need to:\n",
    "1. Provide a descriptive summary of order volume.\n",
    "2. Identify whether orders are trending up.\n",
    "3. Create a simple forecast for the next 30 days.\n",
    "4. Propose a customer segmentation approach.\n",
    "### Deliverables\n",
    "- A table of monthly totals and average daily orders\n",
    "- A trend plot (weekly or monthly)\n",
    "- Forecast plot + error metrics on a test set\n",
    "- A segmentation table (rule-based) and (optional) a clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project starter: 1) monthly summary\n",
    "monthly_summary = (\n",
    "    ts.set_index('date')\n",
    "      .resample('MS')\n",
    "      .agg(total_orders=('orders', 'sum'), avg_daily_orders=('orders', 'mean'))\n",
    "      .reset_index()\n",
    ")\n",
    "monthly_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project starter: 2) trend plot using weekly totals\n",
    "weekly = (\n",
    "    ts.set_index('date')\n",
    "      .resample('W')\n",
    "      .agg(orders=('orders', 'sum'))\n",
    ")\n",
    "weekly['roll_8'] = weekly['orders'].rolling(8).mean()\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(weekly.index, weekly['orders'], alpha=0.35, label='Weekly orders')\n",
    "ax.plot(weekly.index, weekly['roll_8'], linewidth=2, label='8-week rolling mean')\n",
    "ax.set_title('Weekly Orders Trend')\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project starter: 3) 30-day forecast using your chosen method\n",
    "forecast_horizon = 30\n",
    "future_index = pd.date_range(series.index.max() + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')\n",
    "# Choose a method (example: MA(7))\n",
    "future_forecast = moving_average_forecast(series, future_index, window=7)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(series.index[-120:], series.iloc[-120:], label='History (last 120 days)')\n",
    "ax.plot(future_forecast.index, future_forecast, label='Forecast (next 30 days)', linewidth=2)\n",
    "ax.set_title('30-Day Forecast (Moving Average)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Orders')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd353af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-project starter: 4) rule-based segmentation summary\n",
    "seg_summary = (\n",
    "    seg.groupby(['recency_bucket', 'frequency_bucket'], as_index=False)\n",
    "      .agg(customers=('customer_id', 'count'), avg_spend=('annual_spend', 'mean'))\n",
    ")\n",
    "seg_summary.sort_values(['recency_bucket', 'frequency_bucket']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b954c5",
   "metadata": {},
   "source": [
    "# Summary / Key Takeaways\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "| Technique | Purpose | Key Methods |\n",
    "|-----------|---------|-------------|\n",
    "| **Descriptive Analytics** | Answer \"What happened?\" | `groupby`, `agg`, summary stats, KPIs |\n",
    "| **Trend Analysis** | Identify direction over time | Rolling averages, linear trend lines |\n",
    "| **Time Series Fundamentals** | Work with time-indexed data | Resampling, lags, differencing, rolling stats |\n",
    "| **Segmentation** | Group similar items (rule-based) | Quantile cuts, RFM approach |\n",
    "| **Clustering** | Group similar items (data-driven) | K-Means, StandardScaler |\n",
    "| **Forecasting** | Predict future values | Naïve, Moving Average, Exponential Smoothing |\n",
    "| **Model Selection** | Choose the right approach | Match goal, data, interpretability, constraints |\n",
    "\n",
    "## Key Principles to Remember\n",
    "\n",
    "1. **Descriptive analytics** (summaries, groupby, plots) is the foundation of everything else.\n",
    "2. **Trend analysis** is easier when you aggregate and smooth (weekly/monthly + rolling mean).\n",
    "3. **Time series** work best when you use time indexes, rolling stats, lags, and differences.\n",
    "4. **Segmentation** can be rule-based (highly interpretable) or data-driven (clustering).\n",
    "5. **Forecasting** should start with strong baselines (naïve, moving average, simple smoothing).\n",
    "6. **Model selection** is about matching method to goal + constraints, not maximizing complexity.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- [Pandas Time Series Documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html) – Official guide for time-indexed data\n",
    "- [scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html) – Comprehensive clustering documentation\n",
    "- [scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html) – Guide for choosing and evaluating models\n",
    "\n",
    "### Learning Resources\n",
    "- [Forecasting: Principles and Practice (Free Online Book)](https://otexts.com/fpp3/) – Excellent introduction to forecasting\n",
    "- [Kaggle Time Series Course](https://www.kaggle.com/learn/time-series) – Hands-on tutorials with real datasets\n",
    "- [RFM Segmentation Tutorial](https://clevertap.com/blog/rfm-analysis/) – Business-oriented explanation of RFM\n",
    "\n",
    "### Practice Datasets\n",
    "- [Kaggle Retail Data](https://www.kaggle.com/datasets) – Search for \"retail sales\" or \"e-commerce\"\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) – Classic datasets for practice\n",
    "\n",
    "---\n",
    "\n",
    "**Next Chapter Preview:** In Chapter 18, you will learn how to properly validate and evaluate your models using train-test splits, evaluation metrics, and techniques to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
