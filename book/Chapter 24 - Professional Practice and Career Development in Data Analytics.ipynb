{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d22782",
   "metadata": {},
   "source": [
    "# Chapter 24: Professional Practice and Career Development in Data Analytics\n",
    "\n",
    "This chapter covers the **non-technical skills and professional habits** that separate great analysts from good ones. While technical skills get you the interview, professional practices help you succeed on the job and advance your career.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Roles and responsibilities of data analysts in industry\n",
    "- Industry tools and best practices for professional work\n",
    "- Coding standards and documentation practices\n",
    "- Building an effective portfolio\n",
    "- Resume writing and interview preparation\n",
    "- Continuous learning and career development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015aed9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Being a great analyst is not only about knowing Python or statistics. Itâ€™s also about:\n",
    "- Asking the right questions\n",
    "- Communicating clearly\n",
    "- Producing **reproducible** results (someone else can run your work and get the same output)\n",
    "- Keeping your work organized and trustworthy\n",
    "\n",
    "In this notebook, youâ€™ll learn professional practices that companies expectâ€”and youâ€™ll practice them step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6737ab",
   "metadata": {},
   "source": [
    "## What youâ€™ll learn\n",
    "\n",
    "By the end of Chapter 24, you should be able to:\n",
    "- Describe the common **roles and responsibilities** of data analysts\n",
    "- Use a practical toolkit (Python environment + Git + notebooks) in a professional workflow\n",
    "- Apply **coding standards** (readable code, functions, naming) and write helpful documentation\n",
    "- Build a portfolio project structure and produce a clean analysis notebook\n",
    "- Improve a resume with measurable impact statements and prepare for interviews\n",
    "- Create a continuous learning plan without getting overwhelmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup used throughout this notebook\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Platform:', platform.platform())\n",
    "print('Timestamp:', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e22ef",
   "metadata": {},
   "source": [
    "## 24.1 Roles and responsibilities of data analysts\n",
    "\n",
    "A **data analyst** helps an organization make decisions using data. The exact work varies by company, but common responsibilities include:\n",
    "\n",
    "### Typical responsibilities\n",
    "- **Define the question:** What decision are we trying to make? What does success mean?\n",
    "- **Collect data:** Pull data from databases, spreadsheets, APIs, or logs.\n",
    "- **Clean and validate:** Fix missing values, inconsistent formats, duplicates, and confirm the data makes sense.\n",
    "- **Analyze:** Summarize, compare groups, test hypotheses, and find patterns.\n",
    "- **Communicate:** Create charts/tables and tell a clear story with recommendations.\n",
    "- **Deliver assets:** Notebooks, dashboards, reports, and well-documented code.\n",
    "\n",
    "### What companies often expect\n",
    "- You document assumptions and decisions\n",
    "- You can explain your work to non-technical people\n",
    "- You are careful with privacy and confidential data\n",
    "- You avoid â€˜black boxâ€™ work: results should be reproducible\n",
    "\n",
    "**Tip:** Your value is not just chartsâ€”itâ€™s reducing uncertainty in decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple checklist you can reuse for most analytics tasks\n",
    "workflow_checklist = [\n",
    "    {'stage': 'Define problem', 'what_good_looks_like': 'Clear question, stakeholders, metrics, and constraints'},\n",
    "    {'stage': 'Get data', 'what_good_looks_like': 'Reliable sources, permission to use, documented extraction query'},\n",
    "    {'stage': 'Clean + validate', 'what_good_looks_like': 'Handled missing/duplicates, sanity checks pass, assumptions logged'},\n",
    "    {'stage': 'Analyze', 'what_good_looks_like': 'Correct methods, sensitivity checks, no cherry-picking'},\n",
    "    {'stage': 'Communicate', 'what_good_looks_like': 'Simple visuals, clear narrative, actionable recommendation'},\n",
    "    {'stage': 'Deliver + reproduce', 'what_good_looks_like': 'Version-controlled code, requirements captured, runnable notebook'}\n",
    "]\n",
    "pd.DataFrame(workflow_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546f14a",
   "metadata": {},
   "source": [
    "### Exercise 1 (quick reflection)\n",
    "Write 2â€“3 sentences answering:\n",
    "1. What type of decisions do you want to help with (marketing, finance, operations, etc.)?\n",
    "2. What data sources do you currently have access to (public datasets, your own projects, etc.)?\n",
    "\n",
    "You can type the answer directly in a Markdown cell below, or store it in a Python variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: store your reflection in variables (edit the strings)\n",
    "career_focus = 'e.g., marketing analytics'\n",
    "data_sources = 'e.g., public datasets, Kaggle, synthetic data'\n",
    "career_focus, data_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97fa4e",
   "metadata": {},
   "source": [
    "## 24.2 Industry tools and best practices\n",
    "\n",
    "Professional analytics work is usually a mix of **tools** and **habits**. Tools help you do the work; habits make the work reliable.\n",
    "\n",
    "### Common tools (you donâ€™t need *all* of these to start)\n",
    "- **Python:** pandas, NumPy, matplotlib/seaborn, SciPy/statsmodels\n",
    "- **SQL:** querying databases (almost always needed in jobs)\n",
    "- **Notebooks:** Jupyter / VS Code notebooks for exploration and teaching\n",
    "- **Version control:** Git + GitHub/GitLab\n",
    "- **Spreadsheets:** Excel/Google Sheets (still common in many teams)\n",
    "- **BI tools:** Power BI, Tableau, Looker (varies by company)\n",
    "- **Workflow tools (later):** Airflow, dbt, Prefect\n",
    "- **Cloud (later):** AWS/GCP/Azure services\n",
    "\n",
    "### Best practices that beginners should adopt early\n",
    "1. **Reproducibility:** someone else (or future you) can rerun your notebook.\n",
    "2. **Version control:** treat analysis like softwareâ€”track changes and write clear commit messages.\n",
    "3. **Clear folder structure:** data, notebooks, scripts, reports.\n",
    "4. **Donâ€™t hardcode secrets:** never put passwords/tokens in notebooks.\n",
    "5. **Document assumptions:** record choices like filters, date ranges, and definitions.\n",
    "\n",
    "**Common mistake:** doing â€˜analysis by accidentâ€™ (copy/paste cells until it looks right) instead of having a clear question and checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing environment information helps reproducibility.\n",
    "# This is a Python-only alternative to running `pip freeze`.\n",
    "from importlib import metadata\n",
    "\n",
    "def list_installed_packages(limit=25):\n",
    "    dists = sorted(metadata.distributions(), key=lambda d: d.metadata['Name'].lower())\n",
    "    rows = []\n",
    "    for dist in dists[:limit]:\n",
    "        name = dist.metadata['Name']\n",
    "        version = dist.version\n",
    "        rows.append({'package': name, 'version': version})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "list_installed_packages(limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494fdfa",
   "metadata": {},
   "source": [
    "### Tips: Git and notebooks\n",
    "- Commit frequently with messages like: `Clean missing values in sales dataset`\n",
    "- Avoid committing large raw datasets to Git (use small samples or documentation)\n",
    "- Notebooks can create noisy diffs; consider clearing outputs before committing\n",
    "\n",
    "**Warning (secrets):** Never paste API keys into notebooks. Use environment variables or config files excluded from Git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4dd6f",
   "metadata": {},
   "source": [
    "### Exercise 2 (tool awareness)\n",
    "Pick 3 tools from the list above and write:\n",
    "- What problem the tool solves\n",
    "- One task you can do this week using it\n",
    "\n",
    "Example:\n",
    "- Git: track changes â†’ make 3 commits while learning pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: write your tool plan as a small table\n",
    "tool_plan = pd.DataFrame(\n",
    "    [\n",
    "        {'tool': 'Git', 'problem_it_solves': 'Version control / history', 'this_week_task': 'Make 3 commits while learning'},\n",
    "        {'tool': 'SQL', 'problem_it_solves': 'Query data from databases', 'this_week_task': 'Practice SELECT + JOIN on sample data'},\n",
    "        {'tool': 'Power BI / Tableau', 'problem_it_solves': 'Dashboards for stakeholders', 'this_week_task': 'Build a simple sales dashboard from CSV'}\n",
    "    ]\n",
    ")\n",
    "tool_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538debb",
   "metadata": {},
   "source": [
    "## 24.3 Coding standards and documentation\n",
    "\n",
    "In a job, your code is read by other people (and by *future you*). â€˜Workingâ€™ code is not enoughâ€”**readable** code saves time and prevents mistakes.\n",
    "\n",
    "### Beginner-friendly coding standards\n",
    "- Use **descriptive names**: `total_revenue` is clearer than `tr`\n",
    "- Keep functions small and focused (one job per function)\n",
    "- Prefer **pure functions** when possible (inputs â†’ outputs)\n",
    "- Avoid magic numbers; use constants like `TAX_RATE = 0.15`\n",
    "- Add docstrings to explain what a function expects and returns\n",
    "\n",
    "### Documentation you should practice\n",
    "- **README.md:** what the project is, how to run it\n",
    "- **Data dictionary:** what each column means\n",
    "- **Assumptions/decisions:** filters, rules, exclusions\n",
    "\n",
    "**Common mistake:** writing long notebooks with no headings and no explanation of *why* steps are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ccf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: refactor messy code into clean, testable functions\n",
    "\n",
    "def clean_transactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean a transactions table.\n",
    "\n",
    "    Steps:\n",
    "    1) Normalize column names\n",
    "    2) Convert dates and numeric types\n",
    "    3) Drop exact duplicates\n",
    "    4) Create a total column\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Must include columns: date, quantity, unit_price\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Cleaned dataframe with an added 'total' column.\n",
    "    \"\"\"\n",
    "    cleaned = df.copy()\n",
    "\n",
    "    # 1) Consistent column names\n",
    "    cleaned.columns = [c.strip().lower().replace(' ', '_') for c in cleaned.columns]\n",
    "\n",
    "    # 2) Types\n",
    "    cleaned['date'] = pd.to_datetime(cleaned['date'], errors='coerce')\n",
    "    cleaned['quantity'] = pd.to_numeric(cleaned['quantity'], errors='coerce')\n",
    "    cleaned['unit_price'] = pd.to_numeric(cleaned['unit_price'], errors='coerce')\n",
    "\n",
    "    # 3) Remove duplicates\n",
    "    cleaned = cleaned.drop_duplicates()\n",
    "\n",
    "    # 4) Feature: total\n",
    "    cleaned['total'] = cleaned['quantity'] * cleaned['unit_price']\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "raw = pd.DataFrame({\n",
    "    'Date': ['2025-01-01', '2025-01-01', 'bad_date'],\n",
    "    'Quantity': ['2', '2', '3'],\n",
    "    'Unit Price': ['10.00', '10.00', '7.50'],\n",
    "})\n",
    "clean_transactions(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf92877",
   "metadata": {},
   "source": [
    "### Why this refactor is â€˜professionalâ€™\n",
    "- The function clearly states inputs and outputs (docstring + type hints).\n",
    "- Steps are separated and explained.\n",
    "- Using `errors='coerce'` prevents crashing and helps you detect bad data later.\n",
    "\n",
    "**Tip:** In real projects, youâ€™d also add validation checks (e.g., quantity must be non-negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ceee7c",
   "metadata": {},
   "source": [
    "### Exercise 3 (refactoring practice)\n",
    "Below is intentionally messy code. Your task: rewrite it into a function with a docstring and clear variable names.\n",
    "\n",
    "Goals:\n",
    "- Convert `d` to a datetime column\n",
    "- Compute revenue = `q * p`\n",
    "- Return only valid rows (no missing date, q, or p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28471e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter data (keep this)\n",
    "df_messy = pd.DataFrame({\n",
    "    'd': ['2025-01-01', '2025-01-02', None],\n",
    "    'q': ['1', 'bad', '3'],\n",
    "    'p': [10, 20, None]\n",
    "})\n",
    "df_messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your refactored function here\n",
    "def tidy_revenue_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace this docstring with your own explanation.\n",
    "    \"\"\"\n",
    "    tidy = df.copy()\n",
    "\n",
    "    # Your steps go here\n",
    "    tidy['d'] = pd.to_datetime(tidy['d'], errors='coerce')\n",
    "    tidy['q'] = pd.to_numeric(tidy['q'], errors='coerce')\n",
    "    tidy['p'] = pd.to_numeric(tidy['p'], errors='coerce')\n",
    "    tidy['revenue'] = tidy['q'] * tidy['p']\n",
    "\n",
    "    tidy = tidy.dropna(subset=['d', 'q', 'p'])\n",
    "    return tidy\n",
    "\n",
    "tidy_revenue_table(df_messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4412f1",
   "metadata": {},
   "source": [
    "## 24.4 Portfolio development\n",
    "\n",
    "A **portfolio** is proof that you can do real work. A good beginner portfolio:\n",
    "- Uses a small dataset (or a clean sample)\n",
    "- Asks a clear question\n",
    "- Shows cleaning + analysis + visualization\n",
    "- Ends with conclusions and next steps\n",
    "- Is easy to run (instructions + requirements)\n",
    "\n",
    "### A simple portfolio project structure\n",
    "A common structure looks like this:\n",
    "- `README.md` (project overview + how to run)\n",
    "- `data/` (small sample data or links)\n",
    "- `notebooks/` (exploration)\n",
    "- `src/` (reusable functions)\n",
    "- `reports/` (exported charts or summary tables)\n",
    "\n",
    "**Tip:** Employers like projects that resemble real work: clear question, clean code, and clear communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7df600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a portfolio-style folder structure (safe to run: creates folders if missing)\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path('portfolio_project_demo')\n",
    "for folder in ['data', 'notebooks', 'src', 'reports']:\n",
    "    (project_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(project_root / 'README.md').write_text(\n",
    "    '# Portfolio Project Demo\\n'\n",
    "    '\\n'\n",
    "    '## Question\\n'\n",
    "    '- What question are we answering?\\n'\n",
    "    '\\n'\n",
    "    '## Data\\n'\n",
    "    '- Where does it come from? What are its columns?\\n'\n",
    "    '\\n'\n",
    "    '## How to run\\n'\n",
    "    '- Open the notebook in `notebooks/` and run cells top-to-bottom.\\n'\n",
    "    '\\n'\n",
    "    '## Results\\n'\n",
    "    '- Key findings and recommendations.\\n',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "project_root.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a481069",
   "metadata": {},
   "source": [
    "### Mini-project: a portfolio-ready analysis (end-to-end)\n",
    "\n",
    "To practice professional habits, weâ€™ll do a small end-to-end project on a **synthetic dataset** (so you donâ€™t need to download anything).\n",
    "\n",
    "**Scenario:** You work for an online store. You want to understand:\n",
    "- Which customer segments spend more\n",
    "- Whether discounts reduce revenue\n",
    "- What the monthly trend looks like\n",
    "\n",
    "We will:\n",
    "1. Generate data\n",
    "2. Clean and validate\n",
    "3. Analyze and visualize\n",
    "4. Write conclusions\n",
    "\n",
    "**Professional tip:** Even for synthetic data, practice the same process youâ€™d use at work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ea05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate a synthetic dataset (reproducible with a fixed random seed)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n = 800\n",
    "dates = pd.to_datetime('2025-01-01') + pd.to_timedelta(rng.integers(0, 365, size=n), unit='D')\n",
    "segment = rng.choice(['New', 'Returning', 'VIP'], size=n, p=[0.55, 0.35, 0.10])\n",
    "quantity = rng.integers(1, 6, size=n)\n",
    "base_price = rng.normal(loc=25, scale=8, size=n).clip(5, 80)\n",
    "discount = rng.choice([0.0, 0.05, 0.10, 0.20], size=n, p=[0.60, 0.20, 0.15, 0.05])\n",
    "\n",
    "# Segment effect: VIP tends to buy slightly higher-priced items\n",
    "segment_multiplier = np.where(segment == 'VIP', 1.25, np.where(segment == 'Returning', 1.05, 1.00))\n",
    "unit_price = (base_price * segment_multiplier).round(2)\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_date': dates,\n",
    "    'segment': segment,\n",
    "    'quantity': quantity,\n",
    "    'unit_price': unit_price,\n",
    "    'discount_rate': discount\n",
    "})\n",
    "\n",
    "# Introduce a few data quality issues intentionally\n",
    "orders.loc[rng.choice(orders.index, size=6, replace=False), 'unit_price'] = None\n",
    "orders.loc[rng.choice(orders.index, size=6, replace=False), 'segment'] = 'vip'  # inconsistent casing\n",
    "orders.loc[rng.choice(orders.index, size=4, replace=False), 'quantity'] = -1   # invalid quantity\n",
    "\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed7d4f",
   "metadata": {},
   "source": [
    "### Step 2: Clean and validate (with â€˜whyâ€™)\n",
    "\n",
    "In professional work, cleaning is not random. You clean data to make it:\n",
    "- **Consistent** (same meanings, same categories)\n",
    "- **Valid** (no impossible values like negative quantities)\n",
    "- **Usable** for analysis (correct types and no critical missing fields)\n",
    "\n",
    "Weâ€™ll do these actions:\n",
    "1. Standardize categories (VIP vs vip)\n",
    "2. Convert to numeric and handle missing values\n",
    "3. Remove invalid rows (quantity <= 0)\n",
    "4. Create business metrics (gross, net revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = orders.copy()\n",
    "\n",
    "# 1) Standardize segment values\n",
    "clean['segment'] = clean['segment'].str.strip().str.title()\n",
    "\n",
    "# 2) Ensure numeric types\n",
    "clean['quantity'] = pd.to_numeric(clean['quantity'], errors='coerce')\n",
    "clean['unit_price'] = pd.to_numeric(clean['unit_price'], errors='coerce')\n",
    "clean['discount_rate'] = pd.to_numeric(clean['discount_rate'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# 3) Remove invalid quantities\n",
    "clean = clean[clean['quantity'] > 0].copy()\n",
    "\n",
    "# 4) Create metrics\n",
    "clean['gross_revenue'] = clean['quantity'] * clean['unit_price']\n",
    "clean['net_revenue'] = clean['gross_revenue'] * (1 - clean['discount_rate'])\n",
    "\n",
    "# Important: missing unit_price makes revenue missing; decide what to do.\n",
    "missing_price = clean['unit_price'].isna().mean()\n",
    "missing_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f6c4f",
   "metadata": {},
   "source": [
    "#### Handling missing values: a professional decision\n",
    "We found some missing prices. There is no single â€˜correctâ€™ approachâ€”your choice depends on context. Common options:\n",
    "- **Drop** rows (safe if itâ€™s a tiny fraction and missingness is random)\n",
    "- **Impute** (fill) with a reasonable value (needs justification)\n",
    "- **Investigate** upstream (best option at work if possible)\n",
    "\n",
    "For this mini-project, weâ€™ll **drop rows with missing revenue fields** because they are few and we canâ€™t compute revenue without price.\n",
    "\n",
    "**Warning:** Dropping rows can bias results if missingness is not random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f12450",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean.dropna(subset=['unit_price', 'gross_revenue', 'net_revenue']).copy()\n",
    "clean.shape, orders.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55568578",
   "metadata": {},
   "source": [
    "### Step 3: Analyze\n",
    "Weâ€™ll answer three simple questions:\n",
    "1. Which segment has the highest average net revenue per order?\n",
    "2. Do higher discounts correlate with lower net revenue?\n",
    "3. What is the monthly net revenue trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Segment comparison\n",
    "segment_summary = (\n",
    "    clean.groupby('segment', as_index=False)\n",
    "    .agg(orders=('net_revenue', 'size'), avg_net_revenue=('net_revenue', 'mean'), total_net_revenue=('net_revenue', 'sum'))\n",
    ")\n",
    "segment_summary.sort_values('avg_net_revenue', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eccfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: average net revenue by segment\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(segment_summary['segment'], segment_summary['avg_net_revenue'])\n",
    "plt.title('Average Net Revenue per Order by Segment')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Average Net Revenue')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Discount vs net revenue (simple scatter)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(clean['discount_rate'], clean['net_revenue'], alpha=0.3)\n",
    "plt.title('Discount Rate vs Net Revenue')\n",
    "plt.xlabel('Discount Rate')\n",
    "plt.ylabel('Net Revenue')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "corr = clean[['discount_rate', 'net_revenue']].corr().iloc[0, 1]\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5d2d4",
   "metadata": {},
   "source": [
    "**Interpretation tip:** Correlation does not prove causation. Discounts might be used on low-value items, or during slow periods, etc. Professional analysts mention these caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Monthly trend\n",
    "monthly = (\n",
    "    clean.assign(month=clean['order_date'].dt.to_period('M').dt.to_timestamp())\n",
    "    .groupby('month', as_index=False)\n",
    "    .agg(total_net_revenue=('net_revenue', 'sum'), orders=('net_revenue', 'size'))\n",
    ")\n",
    "monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa895ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(monthly['month'], monthly['total_net_revenue'], marker='o')\n",
    "plt.title('Monthly Total Net Revenue')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Net Revenue')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cd9f5",
   "metadata": {},
   "source": [
    "### Step 4: Write conclusions (how analysts communicate)\n",
    "A professional conclusion usually includes:\n",
    "- What you found (1â€“3 bullets)\n",
    "- Why it matters\n",
    "- What you recommend\n",
    "- What you would check next\n",
    "\n",
    "Below we auto-generate a simple text summary (you can edit it to sound more human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be75082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a professional summary statement\n",
    "best_segment = segment_summary.sort_values('avg_net_revenue', ascending=False).iloc[0]['segment']\n",
    "best_value = segment_summary.sort_values('avg_net_revenue', ascending=False).iloc[0]['avg_net_revenue']\n",
    "\n",
    "summary = f\"\"\"\n",
    "=== ANALYSIS SUMMARY ===\n",
    "\n",
    "Key Findings:\n",
    "1. {best_segment} customers have the highest average net revenue per order (${best_value:.2f}).\n",
    "2. Discount rate shows a weak negative correlation with net revenue (r = {corr:.3f}).\n",
    "3. Monthly revenue appears relatively stable with some seasonal variation.\n",
    "\n",
    "Recommendations:\n",
    "- Focus retention efforts on {best_segment} customersâ€”they drive higher value per transaction.\n",
    "- Review the discount strategy: consider whether discounts are attracting price-sensitive customers\n",
    "  who may not return without promotions.\n",
    "- Investigate months with lower revenue to understand seasonality or external factors.\n",
    "\n",
    "Next Steps:\n",
    "- Segment analysis by acquisition channel (if available).\n",
    "- A/B test discount levels to measure causal impact.\n",
    "- Build a simple forecasting model for monthly revenue.\n",
    "\n",
    "Caveats:\n",
    "- This is synthetic data for demonstration purposes.\n",
    "- Correlation does not imply causation.\n",
    "- Missing values were dropped, which could introduce bias if missingness is not random.\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63e71e",
   "metadata": {},
   "source": [
    "## 24.5 Resume and interview preparation\n",
    "\n",
    "Getting your first data analytics job requires more than technical skills. You need to **communicate your value** clearly through your resume and in interviews.\n",
    "\n",
    "### Building a strong data analyst resume\n",
    "\n",
    "Your resume should answer one question: **\"What can you do for us?\"**\n",
    "\n",
    "#### Key sections for a data analyst resume\n",
    "1. **Summary/Objective** (2-3 sentences): Who you are, what you do, and what you're looking for\n",
    "2. **Skills**: Technical skills (Python, SQL, Excel, visualization tools) and soft skills\n",
    "3. **Experience**: Use the **STAR format** (Situation, Task, Action, Result)\n",
    "4. **Projects**: Portfolio work that demonstrates your abilities\n",
    "5. **Education**: Degrees, certifications, relevant coursework\n",
    "\n",
    "#### The power of quantified impact statements\n",
    "Instead of: *\"Analyzed sales data\"*\n",
    "Write: *\"Analyzed 2 years of sales data (50K+ records) to identify seasonal trends, leading to a 15% improvement in inventory planning accuracy\"*\n",
    "\n",
    "**Formula:** Action Verb + What You Did + Tools Used + Quantified Impact\n",
    "\n",
    "### Common mistakes to avoid\n",
    "- âŒ Listing every tool you've touched (focus on what you're proficient in)\n",
    "- âŒ Using vague language (\"helped with data analysis\")\n",
    "- âŒ Including irrelevant experience without connecting it to analytics\n",
    "- âŒ Typos and formatting inconsistencies (shows lack of attention to detail)\n",
    "- âŒ Long paragraphs instead of bullet points\n",
    "\n",
    "**Tip:** Your resume is a data productâ€”it should be clean, well-organized, and tell a clear story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aded7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create impact statements\n",
    "def create_impact_statement(action: str, what: str, tools: str, impact: str) -> str:\n",
    "    \"\"\"Generate a well-formatted resume bullet point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    action : str\n",
    "        Strong action verb (e.g., 'Analyzed', 'Developed', 'Automated')\n",
    "    what : str\n",
    "        What you did or worked with\n",
    "    tools : str\n",
    "        Technologies or methods used\n",
    "    impact : str\n",
    "        Quantified result or outcome\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted impact statement for your resume\n",
    "    \"\"\"\n",
    "    return f\"{action} {what} using {tools}, resulting in {impact}\"\n",
    "\n",
    "# Example impact statements\n",
    "examples = [\n",
    "    create_impact_statement(\n",
    "        \"Analyzed\", \"customer churn data (10K+ records)\",\n",
    "        \"Python and SQL\", \"identification of 3 key risk factors and 12% reduction in churn\"\n",
    "    ),\n",
    "    create_impact_statement(\n",
    "        \"Developed\", \"an automated reporting dashboard\",\n",
    "        \"Power BI and Python\", \"saving 8 hours/week of manual report generation\"\n",
    "    ),\n",
    "    create_impact_statement(\n",
    "        \"Cleaned and transformed\", \"messy sales data from 5 regional sources\",\n",
    "        \"pandas\", \"a unified dataset enabling cross-regional analysis for the first time\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Example Resume Bullet Points:\\n\")\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"{i}. {ex}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d8315",
   "metadata": {},
   "source": [
    "### Preparing for data analyst interviews\n",
    "\n",
    "Data analyst interviews typically include several components:\n",
    "\n",
    "| Interview Type | What to Expect | How to Prepare |\n",
    "|---------------|----------------|----------------|\n",
    "| **Phone Screen** | Basic fit, salary expectations, background | Practice your \"tell me about yourself\" story |\n",
    "| **Technical Screen** | SQL queries, Python basics, statistics | Practice on LeetCode, StrataScratch, or similar |\n",
    "| **Case Study** | Analyze a dataset and present findings | Practice end-to-end analysis with time limits |\n",
    "| **Behavioral** | Past experiences, teamwork, problem-solving | Prepare STAR stories for common questions |\n",
    "| **Take-Home** | Complete an analysis project (1-3 days) | Treat it like real work: clean code, clear narrative |\n",
    "\n",
    "### Common technical interview questions\n",
    "\n",
    "**SQL:**\n",
    "- Write a query to find the top 5 customers by total purchase amount\n",
    "- How do you handle NULL values in aggregations?\n",
    "- Explain the difference between INNER JOIN and LEFT JOIN\n",
    "\n",
    "**Python/pandas:**\n",
    "- How do you handle missing data?\n",
    "- What's the difference between `loc` and `iloc`?\n",
    "- Write a function to clean and aggregate a dataset\n",
    "\n",
    "**Statistics:**\n",
    "- Explain the difference between correlation and causation\n",
    "- When would you use a t-test vs a chi-square test?\n",
    "- What is p-value and how do you interpret it?\n",
    "\n",
    "**Business/Case:**\n",
    "- How would you measure the success of a marketing campaign?\n",
    "- Sales dropped 20% last monthâ€”how would you investigate?\n",
    "- What metrics would you track for a subscription business?\n",
    "\n",
    "### Tips for interview success\n",
    "1. **Think out loud**: Interviewers want to see your thought process\n",
    "2. **Ask clarifying questions**: Don't assumeâ€”verify understanding\n",
    "3. **Start simple, then refine**: Get a basic answer first, then optimize\n",
    "4. **Practice under time pressure**: Real interviews have time limits\n",
    "5. **Prepare questions to ask**: Shows genuine interest and engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: SQL-style questions using pandas\n",
    "# This simulates the kind of data manipulation questions you might face\n",
    "\n",
    "# Sample data: customer orders\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'signup_date': pd.to_datetime(['2024-01-15', '2024-02-20', '2024-03-10', '2024-01-05', '2024-04-01'])\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'customer_id': [1, 1, 2, 3, 3, 3, 5],\n",
    "    'amount': [150.00, 75.50, 200.00, 50.00, 125.00, 300.00, 80.00],\n",
    "    'order_date': pd.to_datetime(['2024-02-01', '2024-03-15', '2024-03-01', '2024-04-01', '2024-04-15', '2024-05-01', '2024-05-10'])\n",
    "})\n",
    "\n",
    "print(\"Customers Table:\")\n",
    "display(customers)\n",
    "print(\"\\nOrders Table:\")\n",
    "display(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bbdae",
   "metadata": {},
   "source": [
    "### Exercise 4 (Interview Practice)\n",
    "\n",
    "Using the customers and orders tables above, answer these common interview questions:\n",
    "\n",
    "1. **Find the top 3 customers by total order amount**\n",
    "2. **Calculate the average order value per customer**\n",
    "3. **Find customers who have never placed an order** (hint: LEFT JOIN equivalent)\n",
    "4. **Calculate the number of orders per month**\n",
    "\n",
    "Try to write the solutions yourself before looking at the answers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Top 3 customers by total order amount\n",
    "top_customers = (\n",
    "    orders.groupby('customer_id')['amount']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .reset_index()\n",
    "    .rename(columns={'amount': 'total_amount'})\n",
    ")\n",
    "# Add customer names\n",
    "top_customers = top_customers.merge(customers[['customer_id', 'name']], on='customer_id')\n",
    "print(\"Top 3 Customers by Total Orders:\")\n",
    "display(top_customers[['name', 'total_amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Average order value per customer\n",
    "avg_order_value = (\n",
    "    orders.groupby('customer_id')['amount']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'amount': 'avg_order_value'})\n",
    "    .merge(customers[['customer_id', 'name']], on='customer_id')\n",
    ")\n",
    "print(\"Average Order Value per Customer:\")\n",
    "display(avg_order_value[['name', 'avg_order_value']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f65e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Customers who never placed an order (LEFT JOIN equivalent)\n",
    "# Merge customers with orders using left join\n",
    "customer_orders = customers.merge(orders, on='customer_id', how='left')\n",
    "# Find customers with no orders (NaN in order_id)\n",
    "no_orders = customer_orders[customer_orders['order_id'].isna()][['customer_id', 'name']]\n",
    "print(\"Customers with No Orders:\")\n",
    "display(no_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Number of orders per month\n",
    "orders_by_month = (\n",
    "    orders.assign(month=orders['order_date'].dt.to_period('M'))\n",
    "    .groupby('month')\n",
    "    .size()\n",
    "    .reset_index(name='order_count')\n",
    ")\n",
    "print(\"Orders per Month:\")\n",
    "display(orders_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa3952",
   "metadata": {},
   "source": [
    "## 24.6 Continuous learning pathways\n",
    "\n",
    "Data analytics is a rapidly evolving field. The tools and techniques you learn today may change, but **learning how to learn** will always be valuable.\n",
    "\n",
    "### Why continuous learning matters\n",
    "- New tools and libraries emerge regularly (e.g., Polars, DuckDB)\n",
    "- Business problems evolveâ€”new domains require new knowledge\n",
    "- Career advancement often requires expanding your skillset\n",
    "- Staying current makes you more valuable and employable\n",
    "\n",
    "### A sustainable approach to learning\n",
    "\n",
    "**Warning:** Don't try to learn everything at once. This leads to burnout and shallow knowledge.\n",
    "\n",
    "Instead, follow these principles:\n",
    "\n",
    "1. **Master the fundamentals first**\n",
    "   - Strong Python/pandas skills â†’ then explore new libraries\n",
    "   - Solid SQL foundation â†’ then learn advanced techniques\n",
    "   - Basic statistics â†’ then tackle machine learning\n",
    "\n",
    "2. **Learn in layers**\n",
    "   - Week 1-4: Core concepts and syntax\n",
    "   - Month 2-3: Apply to real projects\n",
    "   - Month 4+: Deepen with edge cases and optimization\n",
    "\n",
    "3. **Balance breadth and depth**\n",
    "   - **T-shaped skills**: Deep expertise in one area + working knowledge of many\n",
    "   - Example: Expert in Python analytics + familiar with SQL, visualization, cloud basics\n",
    "\n",
    "4. **Learn by doing**\n",
    "   - Tutorials teach concepts; projects teach skills\n",
    "   - Every 2-3 hours of learning â†’ 1 hour of practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d7d2f",
   "metadata": {},
   "source": [
    "### Learning resources for data analysts\n",
    "\n",
    "| Category | Free Resources | Paid/Premium |\n",
    "|----------|---------------|--------------|\n",
    "| **Python** | Python.org docs, Real Python | DataCamp, Coursera |\n",
    "| **SQL** | SQLZoo, Mode Analytics tutorials | StrataScratch, LeetCode Premium |\n",
    "| **Statistics** | Khan Academy, Stat Quest (YouTube) | Coursera Statistics courses |\n",
    "| **Visualization** | Matplotlib/Seaborn docs | Storytelling with Data (book) |\n",
    "| **Practice** | Kaggle, HackerRank | Interview Query, DataLemur |\n",
    "| **Career** | LinkedIn Learning (often free via library) | Career coaching services |\n",
    "\n",
    "### Certifications worth considering\n",
    "\n",
    "Certifications can help, especially early in your career, but they are **not a substitute for practical skills**.\n",
    "\n",
    "**Beginner-friendly certifications:**\n",
    "- Google Data Analytics Professional Certificate (Coursera)\n",
    "- IBM Data Analyst Professional Certificate (Coursera)\n",
    "- Microsoft Power BI Data Analyst (PL-300)\n",
    "\n",
    "**Intermediate/Advanced:**\n",
    "- AWS Certified Data Analytics\n",
    "- Databricks Certified Data Analyst\n",
    "- Tableau Desktop Specialist\n",
    "\n",
    "**Tip:** A strong portfolio often matters more than certifications. Prioritize building real projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a personal learning plan template\n",
    "def create_learning_plan(current_level: str, goals: list, weekly_hours: int) -> pd.DataFrame:\n",
    "    \"\"\"Generate a structured learning plan.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_level : str\n",
    "        Your current skill level (beginner/intermediate/advanced)\n",
    "    goals : list\n",
    "        List of learning goals\n",
    "    weekly_hours : int\n",
    "        Hours available per week for learning\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A structured learning plan\n",
    "    \"\"\"\n",
    "    plan_items = []\n",
    "    for i, goal in enumerate(goals, 1):\n",
    "        plan_items.append({\n",
    "            'priority': i,\n",
    "            'goal': goal,\n",
    "            'estimated_weeks': max(2, 8 // len(goals)),  # Spread across available time\n",
    "            'hours_per_week': weekly_hours // len(goals),\n",
    "            'status': 'Not Started'\n",
    "        })\n",
    "    return pd.DataFrame(plan_items)\n",
    "\n",
    "# Example: Create a 3-month learning plan\n",
    "my_goals = [\n",
    "    'Master pandas for data manipulation',\n",
    "    'Learn SQL for database queries',\n",
    "    'Build 2 portfolio projects',\n",
    "    'Practice interview questions weekly'\n",
    "]\n",
    "\n",
    "my_plan = create_learning_plan(\n",
    "    current_level='beginner',\n",
    "    goals=my_goals,\n",
    "    weekly_hours=10\n",
    ")\n",
    "\n",
    "print(\"ðŸ“š My 3-Month Learning Plan:\")\n",
    "display(my_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b6201",
   "metadata": {},
   "source": [
    "### Exercise 5 (Your Learning Plan)\n",
    "\n",
    "Create your own personalized learning plan by modifying the code above:\n",
    "\n",
    "1. Replace `my_goals` with YOUR actual learning goals\n",
    "2. Adjust `weekly_hours` to match YOUR available time\n",
    "3. Run the cell to generate your plan\n",
    "4. **Commit to reviewing and updating your plan monthly**\n",
    "\n",
    "**Pro tip:** Schedule learning time in your calendar like you would a meeting. Consistency beats intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc46db",
   "metadata": {},
   "source": [
    "### Staying current without getting overwhelmed\n",
    "\n",
    "The data world moves fast. Here's how to stay informed without drowning in information:\n",
    "\n",
    "**Weekly (15-30 min):**\n",
    "- Skim 1-2 data newsletters (e.g., Data Elixir, The Analytics Engineering Roundup)\n",
    "- Check LinkedIn for trending topics in your network\n",
    "\n",
    "**Monthly (1-2 hours):**\n",
    "- Read 1 in-depth blog post or tutorial on a new topic\n",
    "- Watch 1 conference talk or webinar\n",
    "\n",
    "**Quarterly:**\n",
    "- Revisit your learning plan and adjust priorities\n",
    "- Start a small project using something new you've learned\n",
    "- Update your portfolio with recent work\n",
    "\n",
    "**Yearly:**\n",
    "- Assess your career trajectory\n",
    "- Consider whether certifications align with your goals\n",
    "- Set 2-3 major learning objectives for the year\n",
    "\n",
    "**Communities to join:**\n",
    "- Reddit: r/dataanalysis, r/learnpython\n",
    "- Discord: Various data science servers\n",
    "- Meetups: Local data analytics or Python groups\n",
    "- LinkedIn: Follow data leaders and engage with content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c872a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "Congratulations on completing Chapter 24â€”and the entire book! Here's what we covered in this final chapter:\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways\n",
    "\n",
    "**24.1 Roles and Responsibilities**\n",
    "- Data analysts reduce uncertainty in business decisions\n",
    "- Your value is in the clarity and reliability of your analysis, not just the charts\n",
    "- Always document assumptions, validate data, and make work reproducible\n",
    "\n",
    "**24.2 Industry Tools and Best Practices**\n",
    "- Master the essentials first: Python, SQL, visualization, version control\n",
    "- Adopt good habits early: reproducibility, documentation, clear folder structure\n",
    "- Never hardcode secrets; use environment variables\n",
    "\n",
    "**24.3 Coding Standards and Documentation**\n",
    "- Write code for humans first, computers second\n",
    "- Use descriptive names, docstrings, and consistent formatting\n",
    "- Document not just *what* you did, but *why*\n",
    "\n",
    "**24.4 Portfolio Development**\n",
    "- A portfolio is proof you can do real work\n",
    "- Structure projects clearly: README, data, notebooks, reports\n",
    "- Focus on end-to-end projects that answer real questions\n",
    "\n",
    "**24.5 Resume and Interview Preparation**\n",
    "- Quantify your impact using the Action + What + Tools + Impact formula\n",
    "- Practice SQL and Python under time pressure\n",
    "- Think out loud and ask clarifying questions in interviews\n",
    "\n",
    "**24.6 Continuous Learning**\n",
    "- Master fundamentals before chasing new tools\n",
    "- Learn by doingâ€”projects teach more than tutorials\n",
    "- Build sustainable habits: consistent learning beats intensive cramming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0d29c",
   "metadata": {},
   "source": [
    "### ðŸ“‹ Professional Practices Checklist\n",
    "\n",
    "Use this checklist for every analytics project:\n",
    "\n",
    "- [ ] **Problem Definition:** Clear question with measurable success criteria\n",
    "- [ ] **Data Collection:** Sources documented, permissions verified\n",
    "- [ ] **Data Cleaning:** Missing values handled, validation checks passed\n",
    "- [ ] **Analysis:** Methods appropriate, assumptions stated, sensitivity checked\n",
    "- [ ] **Communication:** Clear visuals, narrative structure, actionable recommendations\n",
    "- [ ] **Reproducibility:** Code version-controlled, environment documented, notebook runs top-to-bottom\n",
    "- [ ] **Documentation:** README complete, data dictionary included, assumptions logged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e19797",
   "metadata": {},
   "source": [
    "### ðŸš€ What's Next?\n",
    "\n",
    "You've completed this book! Here are your next steps:\n",
    "\n",
    "**Immediate (This Week):**\n",
    "1. Review your notes from all chapters\n",
    "2. Identify 2-3 concepts you want to practice more\n",
    "3. Start building a portfolio project\n",
    "\n",
    "**Short-term (This Month):**\n",
    "1. Complete at least one end-to-end portfolio project\n",
    "2. Set up a GitHub profile with your work\n",
    "3. Update your resume with your new skills\n",
    "\n",
    "**Medium-term (3-6 Months):**\n",
    "1. Apply for data analyst positions or internships\n",
    "2. Contribute to open-source projects or Kaggle competitions\n",
    "3. Network with other data professionals\n",
    "\n",
    "**Remember:** The best analysts are not those who know everythingâ€”they're the ones who:\n",
    "- Ask good questions\n",
    "- Communicate clearly\n",
    "- Never stop learning\n",
    "\n",
    "**Good luck on your data analytics journey!** ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720d6ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- *Storytelling with Data* by Cole Nussbaumer Knaflic (visualization and communication)\n",
    "- *Python for Data Analysis* by Wes McKinney (pandas creator)\n",
    "- *Naked Statistics* by Charles Wheelan (statistics for non-statisticians)\n",
    "\n",
    "### Online Platforms\n",
    "- **Kaggle** (https://kaggle.com) - Datasets, competitions, and notebooks\n",
    "- **StrataScratch** (https://stratascratch.com) - SQL and Python interview practice\n",
    "- **Mode Analytics SQL Tutorial** (https://mode.com/sql-tutorial/) - Free SQL learning\n",
    "- **Real Python** (https://realpython.com) - High-quality Python tutorials\n",
    "\n",
    "### Communities\n",
    "- **r/dataanalysis** and **r/learnpython** on Reddit\n",
    "- **Data Science Discord servers**\n",
    "- **Local meetups** - Search Meetup.com for data analytics groups\n",
    "\n",
    "### Career Resources\n",
    "- **LinkedIn Learning** - Often free through public libraries\n",
    "- **Glassdoor** - Research company salaries and interview questions\n",
    "- **Levels.fyi** - Salary data for tech companies\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for learning with this book. Your journey as a data analyst is just beginning!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
